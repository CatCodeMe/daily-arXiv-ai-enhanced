<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DS](#cs.DS) [Total: 10]
- [cs.SE](#cs.SE) [Total: 16]
- [cs.NI](#cs.NI) [Total: 8]
- [cs.LG](#cs.LG) [Total: 64]
- [q-fin.GN](#q-fin.GN) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 6]
- [math.PR](#math.PR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [stat.ML](#stat.ML) [Total: 5]
- [cs.IR](#cs.IR) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.SD](#cs.SD) [Total: 4]
- [cs.PL](#cs.PL) [Total: 1]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [cs.CL](#cs.CL) [Total: 11]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.CV](#cs.CV) [Total: 10]
- [cs.AI](#cs.AI) [Total: 8]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.GT](#cs.GT) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Template-Based Schema Matching of Multi-Layout Tenancy Schedules:A Comparative Study of a Template-Based Hybrid Matcher and the ALITE Full Disjunction Model](https://arxiv.org/abs/2507.02020)
*Tim Uilkema,Yao Ma,Seyed Sahand Mohammadi Ziabari,Joep van Vliet*

Main category: cs.DB

TL;DR: 提出了一种基于模板的混合模式匹配器，用于解决房地产租赁表格数据集成中的低效问题，显著提升了匹配效果。


<details>
  <summary>Details</summary>
Motivation: 房地产公司租赁表格格式不统一导致数据集成效率低下，现有方法（如ALITE）虽完整但存在模式膨胀、稀疏属性和业务可用性差的问题。

Method: 结合模式（Jaccard、Levenshtein）和实例（数据类型、分布）指标，通过匈牙利算法确定全局最优匹配。

Result: 优化后F1-score达0.881，空值率45.7%，优于ALITE的0.712和75.6%空值率。

Conclusion: 结合业务知识的混合匹配方法能生成更实用且业务对齐的模式映射，未来可扩展至复杂表格。

Abstract: The lack of standardized tabular formats for tenancy schedules across real
estate firms creates significant inefficiencies in data integration. Existing
automated integration methods, such as Full Disjunction (FD)-based models like
ALITE, prioritize completeness but result in schema bloat, sparse attributes
and limited business usability. We propose a novel hybrid, template-based
schema matcher that aligns multi-layout tenancy schedules to a predefined
target schema. The matcher combines schema (Jaccard, Levenshtein) and
instance-based metrics (data types, distributions) with globally optimal
assignments determined via the Hungarian Algorithm. Evaluation against a
manually labeled ground truth demonstrates substantial improvements, with grid
search optimization yielding a peak F1-score of 0.881 and an overall null
percentage of 45.7%. On a separate ground truth of 20 semantically similar
column sets, ALITE achieves an F1-score of 0.712 and 75.6% nulls. These results
suggest that combining structured business knowledge with hybrid matching can
yield more usable and business-aligned schema mappings. The approach assumes
cleanly extracted tabular input, future work could explore extending the
matcher to support complex, composite tables.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [SAKURAONE: Empowering Transparent and Open AI Platforms through Private-Sector HPC Investment in Japan](https://arxiv.org/abs/2507.02124)
*Fumikazu Konishi*

Main category: cs.DC

TL;DR: SAKURAONE是一个高性能计算集群，采用开放式网络技术，在全球TOP500中排名第49位，展示了其在AI和大规模HPC任务中的竞争力。


<details>
  <summary>Details</summary>
Motivation: 开发SAKURAONE旨在为高级工作负载（如大型语言模型训练）提供优化的集群计算资源，并验证开放式网络技术在大规模HPC基础设施中的可行性。

Method: SAKURAONE由100个计算节点组成，每个节点配备8个NVIDIA H100 GPU，支持800 GbE和SONiC操作系统，采用Rail-Optimized拓扑结构实现高速互联。

Result: 在HPL基准测试中达到33.95 PFLOP/s，HPCG测试中达到396.295 TFLOP/s，FP8精度的HPL-MxP测试中达到339.86 PFLOP/s。

Conclusion: SAKURAONE证明了开放式网络技术在大规模HPC中的可行性，并在全球高性能计算领域展现了竞争力。

Abstract: SAKURAONE is a managed high performance computing (HPC) cluster developed and
operated by the SAKURA Internet Research Center. It reinforces the ``KOKARYOKU
PHY'' configuration of bare-metal GPU servers and is designed as a cluster
computing resource optimized for advanced workloads, including large language
model (LLM) training.
  In the ISC 2025 edition of the TOP500 list, SAKURAONE was ranked
\textbf{49th} in the world based on its High Performance Linpack (HPL) score,
demonstrating its global competitiveness. In particular, it is the \textbf{only
system within the top 100} that employs a fully open networking stack based on
\textbf{800~GbE (Gigabit Ethernet)} and the \textbf{SONiC (Software for Open
Networking in the Cloud)} operating system, highlighting the viability of open
and vendor-neutral technologies in large-scale HPC infrastructure.
  SAKURAONE achieved a sustained performance of 33.95~PFLOP/s on the HPL
benchmark (Rmax), and 396.295~TFLOP/s on the High Performance Conjugate
Gradient (HPCG) benchmark. For the HPL-MxP benchmark, which targets
low-precision workloads representative of AI applications, SAKURAONE delivered
an impressive 339.86~PFLOP/s using FP8 precision.
  The system comprises 100 compute nodes, each equipped with eight NVIDIA H100
GPUs. It is supported by an all-flash Lustre storage subsystem with a total
physical capacity of 2~petabytes, providing high-throughput and low-latency
data access. Internode communication is enabled by a full-bisection bandwidth
interconnect based on a Rail-Optimized topology, where the Leaf and Spine
layers are interconnected via 800~GbE links. This topology, in combination with
RoCEv2 (RDMA over Converged Ethernet version 2), enables high-speed, lossless
data transfers and mitigates communication bottlenecks in large-scale parallel
workloads.

</details>


### [3] [Signalling Health for Improved Kubernetes Microservice Availability](https://arxiv.org/abs/2507.02158)
*Jacob Roberts,Blair Archibald,Phil Trinder*

Main category: cs.DC

TL;DR: 论文比较了基于轮询（PCM）和基于信号（SCM）的容器监控方法，发现SCM在故障检测速度和准确性上优于PCM，且无需调优。


<details>
  <summary>Details</summary>
Motivation: 探讨如何提高容器监控的效率，避免PCM的调优需求和误检测问题。

Method: 设计并实现了基于Kubernetes的SCM方法，并通过数学模型和实验（使用SockShop基准）与PCM对比。

Result: SCM检测故障速度快86%，误检测少，服务可用性提高4%。

Conclusion: 建议容器编排器支持SCM功能，以实现更快的故障检测且无需调优。

Abstract: Microservices are often deployed and managed by a container orchestrator that
can detect and fix failures to maintain the service availability critical in
many applications. In Poll-based Container Monitoring (PCM), the orchestrator
periodically checks container health. While a common approach, PCM requires
careful tuning, may degrade service availability, and can be slow to detect
container health changes. An alternative is Signal-based Container Monitoring
(SCM), where the container signals the orchestrator when its status changes. We
present the design, implementation, and evaluation of an SCM approach for
Kubernetes and empirically show that it has benefits over PCM, as predicted by
a new mathematical model. We compare the service availability of SCM and PCM
over six experiments using the SockShop benchmark. SCM does not require that
polling intervals are tuned, and yet detects container failure 86\% faster than
PCM and container readiness in a comparable time with limited resource
overheads. We find PCM can erroneously detect failures, and this reduces
service availability by 4\%. We propose that orchestrators offer SCM features
for faster failure detection than PCM without erroneous detections or careful
tuning.

</details>


### [4] [Domain-Adversarial Transfer Learning for Fault Root Cause Identification in Cloud Computing Systems](https://arxiv.org/abs/2507.02233)
*Bruce Fang,Danyi Gao*

Main category: cs.DC

TL;DR: 提出了一种基于迁移学习的智能算法，用于解决云计算环境中故障根因识别的挑战，通过共享特征提取和域对抗机制提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 云计算环境中的故障根因识别因系统结构复杂、服务耦合度高和故障信息有限而困难。

Method: 采用迁移学习方法，结合共享特征提取模块和域对抗机制，并引入伪标签选择策略以增强少数类识别能力。

Result: 实验表明，该方法在准确性、F1分数和AUC等关键指标上优于主流方法，且在极端类别不平衡和异构节点环境下仍保持高性能。

Conclusion: 验证了所提机制在复杂云计算系统中的有效性和实用价值。

Abstract: This paper addresses the challenge of fault root cause identification in
cloud computing environments. The difficulty arises from complex system
structures, dense service coupling, and limited fault information. To solve
this problem, an intelligent identification algorithm based on transfer
learning is proposed. The method introduces a shared feature extraction module
and a domain adversarial mechanism to enable effective knowledge transfer from
the source domain to the target domain. This improves the model's
discriminative ability and generalization performance in the target domain. The
model incorporates a pseudo-label selection strategy. When labeled samples are
lacking in the target domain, high-confidence predictions are used in training.
This enhances the model's ability to recognize minority classes. To evaluate
the stability and adaptability of the method in real-world scenarios,
experiments are designed under three conditions: label scarcity, class
imbalance, and heterogeneous node environments. Experimental results show that
the proposed method outperforms existing mainstream approaches in several key
metrics, including accuracy, F1-Score, and AUC. The model demonstrates stronger
discriminative power and robustness. Notably, under extreme class imbalance and
significant structural differences in the target domain, the model still
maintains high performance. This validates the effectiveness and practical
value of the proposed mechanisms in complex cloud computing systems.

</details>


### [5] [Flotilla: A scalable, modular and resilient federated learning framework for heterogeneous resources](https://arxiv.org/abs/2507.02295)
*Roopkatha Banerjee,Prince Modi,Jinal Vyas,Chunduru Sri Abhijit,Tejus Chandrashekar,Harsha Varun Marisetty,Manik Gupta,Yogesh Simmhan*

Main category: cs.DC

TL;DR: Flotilla是一个轻量级、可扩展的联邦学习框架，支持同步和异步策略，具有模块化设计和故障恢复能力，适用于边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习框架多关注伪分布式模拟，缺乏对真实边缘硬件部署的支持，且不支持异步聚合和故障恢复。

Method: Flotilla采用模块化设计，支持无状态客户端和分离会话状态的服务器设计，支持同步和异步策略。

Result: Flotilla在200+客户端上展示了故障恢复能力，资源使用优于现有框架，且支持1000+客户端的扩展。

Conclusion: Flotilla是一个高效、灵活的联邦学习框架，适用于策略开发、部署和系统优化。

Abstract: With the recent improvements in mobile and edge computing and rising concerns
of data privacy, Federated Learning(FL) has rapidly gained popularity as a
privacy-preserving, distributed machine learning methodology. Several FL
frameworks have been built for testing novel FL strategies. However, most focus
on validating the learning aspects of FL through pseudo-distributed simulation
but not for deploying on real edge hardware in a distributed manner to
meaningfully evaluate the federated aspects from a systems perspective. Current
frameworks are also inherently not designed to support asynchronous
aggregation, which is gaining popularity, and have limited resilience to client
and server failures. We introduce Flotilla, a scalable and lightweight FL
framework. It adopts a ``user-first'' modular design to help rapidly compose
various synchronous and asynchronous FL strategies while being agnostic to the
DNN architecture. It uses stateless clients and a server design that separates
out the session state, which are periodically or incrementally checkpointed. We
demonstrate the modularity of Flotilla by evaluating five different FL
strategies for training five DNN models. We also evaluate the client and
server-side fault tolerance on 200+ clients, and showcase its ability to
rapidly failover within seconds. Finally, we show that Flotilla's resource
usage on Raspberry Pis and Nvidia Jetson edge accelerators are comparable to or
better than three state-of-the-art FL frameworks, Flower, OpenFL and FedML. It
also scales significantly better compared to Flower for 1000+ clients. This
positions Flotilla as a competitive candidate to build novel FL strategies on,
compare them uniformly, rapidly deploy them, and perform systems research and
optimizations.

</details>


### [6] [Alps, a versatile research infrastructure](https://arxiv.org/abs/2507.02404)
*Maxime Martinasso,Mark Klein,Thomas C. Schulthess*

Main category: cs.DC

TL;DR: CSCS开发了Alps，一种新型HPC基础设施，通过独立端点和高速网络设计，满足多样化的科学需求。


<details>
  <summary>Details</summary>
Motivation: 传统垂直集成的HPC架构缺乏灵活性和可组合性，无法满足日益多样化的科学需求。

Method: Alps采用异构硬件（CPU和GPU）和高性能Slingshot网络，结合模块化存储系统和软件定义的vCluster技术。

Result: Alps支持多种科学领域，如数值天气预报和AI研究，提供定制化平台。

Conclusion: Alps通过创新架构和技术，成功解决了传统HPC的局限性，为多样化科学需求提供了灵活解决方案。

Abstract: The Swiss National Supercomputing Centre (CSCS) has a long-standing tradition
of delivering top-tier high-performance computing systems, exemplified by the
Piz Daint supercomputer. However, the increasing diversity of scientific needs
has exposed limitations in traditional vertically integrated HPC architectures,
which often lack flexibility and composability. To address these challenges,
CSCS developed Alps, a next-generation HPC infrastructure designed with a
transformative principle: resources operate as independent endpoints within a
high-speed network. This architecture enables the creation of independent
tenant-specific and platform-specific services, tailored to diverse scientific
requirements.
  Alps incorporates heterogeneous hardware, including CPUs and GPUs,
interconnected by a high-performance Slingshot network, and offers a modular
storage system. A key innovation is the versatile software-defined cluster
(vCluster) technology, which bridges cloud and HPC paradigms. By abstracting
infrastructure, service management, and user environments into distinct layers,
vClusters allow for customized platforms that support diverse workloads.
Current platforms on Alps serve various scientific domains, including numerical
weather prediction, and AI research.

</details>


### [7] [FlowSpec: Continuous Pipelined Speculative Decoding for Efficient Distributed LLM Inference](https://arxiv.org/abs/2507.02620)
*Xing Liu,Lizhuo Luo,Ming Tang,Chao Huang*

Main category: cs.DC

TL;DR: FlowSpec是一种基于管道并行和树状推测解码的框架，旨在提高边缘网络中大型语言模型（LLM）的分布式推理效率。


<details>
  <summary>Details</summary>
Motivation: 边缘网络中稀疏的推理请求导致现有管道并行方法利用率低，影响推理延迟。

Method: FlowSpec通过三种机制优化解码效率：基于评分的逐步验证、高效的草稿管理和动态草稿扩展策略。

Result: 实验表明，FlowSpec在多种模型和配置下显著提升推理速度，比基线快1.36-1.77倍。

Conclusion: FlowSpec通过提升管道利用率和推测效率，为边缘网络中的LLM推理提供了高效解决方案。

Abstract: Distributed inference serves as a promising approach to enabling the
inference of large language models (LLMs) at the network edge. It distributes
the inference process to multiple devices to ensure that the LLMs can fit into
the device memory. Recent pipeline-based approaches have the potential to
parallelize communication and computation, which helps reduce inference
latency. However, the benefit diminishes when the inference request at the
network edge is sparse, where pipeline is typically at low utilization. To
enable efficient distributed LLM inference at the edge, we propose
\textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding
framework. FlowSpec incorporates three key mechanisms to improve decoding
efficiency: 1) score-based step-wise verification prioritizes more important
draft tokens to bring earlier accpeted tokens; 2) efficient draft management to
prune invalid tokens while maintaining correct causal relationship during
verification; 3) dynamic draft expansion strategies to supply high-quality
speculative inputs. These techniques work in concert to enhance both pipeline
utilization and speculative efficiency. We evaluate FlowSpec on a real-world
testbed with other baselines. Experimental results demonstrate that our
proposed framework significantly improves inference speed across diverse models
and configurations, achieving speedup ratios 1.36$\times$-1.77$\times$ compared
to baselines. Our code is publicly available at
\href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\#}

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [8] [New algorithms for girth and cycle detection](https://arxiv.org/abs/2507.02061)
*Liam Roditty,Plia Trabelsi*

Main category: cs.DS

TL;DR: 论文提出了一种随机算法，用于在无权重无向图中找到长度受限的短周期，并推广了现有算法，提供了更大的参数选择灵活性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是改进现有算法，使其能够更灵活地选择参数，从而在运行时间和周期长度之间实现更广泛的权衡。

Method: 方法包括开发新的技术，引入混合周期检测算法的形式化定义，并推广现有算法的参数选择。

Result: 结果显示，新算法在运行时间和周期长度之间提供了更好的权衡，尤其是在稀疏图中。

Conclusion: 结论是提出的算法不仅推广了现有工作，还通过引入更灵活的参数选择，为周期检测问题提供了更优的解决方案。

Abstract: Let $G=(V,E)$ be an unweighted undirected graph with $n$ vertices and $m$
edges. Let $g$ be the girth of $G$, that is, the length of a shortest cycle in
$G$. We present a randomized algorithm with a running time of
$\tilde{O}\big(\ell \cdot n^{1 + \frac{1}{\ell - \varepsilon}}\big)$ that
returns a cycle of length at most $ 2\ell \left\lceil \frac{g}{2} \right\rceil
- 2 \left\lfloor \varepsilon \left\lceil \frac{g}{2} \right\rceil
\right\rfloor, $ where $\ell \geq 2$ is an integer and $\varepsilon \in [0,1]$,
for every graph with $g = polylog(n)$.
  Our algorithm generalizes an algorithm of Kadria \etal{} [SODA'22] that
computes a cycle of length at most $4\left\lceil \frac{g}{2} \right\rceil -
2\left\lfloor \varepsilon \left\lceil \frac{g}{2} \right\rceil \right\rfloor $
in $\tilde{O}\big(n^{1 + \frac{1}{2 - \varepsilon}}\big)$ time. Kadria \etal{}
presented also an algorithm that finds a cycle of length at most $ 2\ell
\left\lceil \frac{g}{2} \right\rceil $ in $\tilde{O}\big(n^{1 +
\frac{1}{\ell}}\big)$ time, where $\ell$ must be an integer. Our algorithm
generalizes this algorithm, as well, by replacing the integer parameter $\ell$
in the running time exponent with a real-valued parameter $\ell - \varepsilon$,
thereby offering greater flexibility in parameter selection and enabling a
broader spectrum of combinations between running times and cycle lengths.
  We also show that for sparse graphs a better tradeoff is possible, by
presenting an $\tilde{O}(\ell\cdot m^{1+1/(\ell-\varepsilon)})$ time randomized
algorithm that returns a cycle of length at most $2\ell(\lfloor
\frac{g-1}{2}\rfloor) - 2(\lfloor \varepsilon \lfloor \frac{g-1}{2}\rfloor
\rfloor+1)$, where $\ell\geq 3$ is an integer and $\varepsilon\in [0,1)$, for
every graph with $g=polylog(n)$.
  To obtain our algorithms we develop several techniques and introduce a formal
definition of hybrid cycle detection algorithms. [...]

</details>


### [9] [A Computational Proof of the Highest-Scoring Boggle Board](https://arxiv.org/abs/2507.02117)
*Dan Vanderkam*

Main category: cs.DS

TL;DR: 论文提出了一种通过分支定界和决策图数据结构的方法，首次实现了对Boggle棋盘全局最优解的搜索，并验证了局部优化方法找到的解确实是全局最优。


<details>
  <summary>Details</summary>
Motivation: 解决Boggle棋盘问题的全局最优解搜索难题，填补了传统局部优化方法无法覆盖的空白。

Method: 采用分支定界算法和决策图数据结构，对Boggle棋盘进行全局搜索。

Result: 发现通过局部优化方法（如爬山法和模拟退火）找到的高分棋盘实际上是全局最优解。

Conclusion: 论文验证了局部优化方法的有效性，并首次实现了对Boggle棋盘全局最优解的搜索。

Abstract: Finding all the words on a Boggle board is a classic computer programming
problem. With a fast Boggle solver, local optimization techniques such as
hillclimbing and simulated annealing can be used to find particularly
high-scoring boards. The sheer number of possible Boggle boards has
historically prevented an exhaustive search for the global optimum board. We
apply Branch and Bound and a decision diagram-like data structure to perform
the first such search. We find that the highest-scoring boards found via
hillclimbing are, in fact, the global optima.

</details>


### [10] [On the Adversarial Robustness of Online Importance Sampling](https://arxiv.org/abs/2507.02394)
*Yotam Kenneth-Mordoch,Shay Sapir*

Main category: cs.DS

TL;DR: 本文研究了重要性采样（即敏感性采样）的对抗鲁棒性，提出了一种在线重要性采样方法，能在自适应流中保持近似和，并应用于超图割稀疏化和ℓp子空间嵌入问题。


<details>
  <summary>Details</summary>
Motivation: 重要性采样是一种有用的算法技术，但在自适应输入流中，其分析通常失效。本文旨在探索在线重要性采样在对抗性环境中的鲁棒性。

Method: 提出在线重要性采样方法，处理自适应流输入，保持近似和，并匹配非自适应情况下的存储需求。

Result: 在线重要性采样在自适应流中能保持(1±ϵ)-近似和，存储需求与非自适应情况相近。

Conclusion: 在线重要性采样在对抗性环境中具有鲁棒性，可应用于超图割稀疏化和ℓp子空间嵌入等基础问题。

Abstract: This paper studies the adversarial-robustness of importance-sampling (aka
sensitivity sampling); a useful algorithmic technique that samples elements
with probabilities proportional to some measure of their importance. A
streaming or online algorithm is called adversarially-robust if it succeeds
with high probability on input streams that may change adaptively depending on
previous algorithm outputs. Unfortunately, the dependence between stream
elements breaks the analysis of most randomized algorithms, and in particular
that of importance-sampling algorithms. Previously, Braverman et al. [NeurIPS
2021] suggested that streaming algorithms based on importance-sampling may be
adversarially-robust; however, they proved it only for well-behaved inputs.
  We focus on the adversarial-robustness of online importance-sampling, a
natural variant where sampling decisions are irrevocable and made as data
arrives. Our main technical result shows that, given as input an adaptive
stream of elements $x_1,\ldots,x_T\in \mathbb{R}_+$, online importance-sampling
maintains a $(1\pm\epsilon)$-approximation of their sum while matching (up to
lower order terms) the storage guarantees of the oblivious (non-adaptive) case.
We then apply this result to develop adversarially-robust online algorithms for
two fundamental problems: hypergraph cut sparsification and $\ell_p$ subspace
embedding.

</details>


### [11] [Numerical Linear Algebra in Linear Space](https://arxiv.org/abs/2507.02433)
*Yiping Liu,Hoai-An Nguyen,Junzhao Yang*

Main category: cs.DS

TL;DR: 提出了一种随机化的线性空间求解器，用于解决一般线性系统，无需假设条件数，适用于大范围输入，并在多项时间内运行。


<details>
  <summary>Details</summary>
Motivation: 解决传统线性系统求解器在空间和时间效率上的限制，特别是在处理大矩阵时。

Method: 使用随机化算法，通过线性空间和多项式时间逼近解，适用于整数矩阵和有理数解。

Result: 实现了线性空间和多项式时间的求解器，适用于多种数值线性代数问题。

Conclusion: 该求解器在空间和时间效率上具有显著优势，适用于广泛的数值计算问题。

Abstract: We present a randomized linear-space solver for general linear systems
$\mathbf{A} \mathbf{x} = \mathbf{b}$ with $\mathbf{A} \in \mathbb{Z}^{n \times
n}$ and $\mathbf{b} \in \mathbb{Z}^n$, without any assumption on the condition
number of $\mathbf{A}$. For matrices whose entries are bounded by
$\mathrm{poly}(n)$, the solver returns a $(1+\epsilon)$-multiplicative
entry-wise approximation to vector $\mathbf{x} \in \mathbb{Q}^{n}$ using
$\widetilde{O}(n^2 \cdot \mathrm{nnz}(\mathbf{A}))$ bit operations and $O(n
\log n)$ bits of working space (i.e., linear in the size of a vector), where
$\mathrm{nnz}$ denotes the number of nonzero entries. Our solver works for
right-hand vector $\mathbf{b}$ with entries up to $n^{O(n)}$. To our knowledge,
this is the first linear-space linear system solver over the rationals that
runs in $\widetilde{O}(n^2 \cdot \mathrm{nnz}(\mathbf{A}))$ time.
  We also present several applications of our solver to numerical linear
algebra problems, for which we provide algorithms with efficient polynomial
running time and near-linear space. In particular, we present results for
linear regression, linear programming, eigenvalues and eigenvectors, and
singular value decomposition.

</details>


### [12] [Bounded Weighted Edit Distance: Dynamic Algorithms and Matching Lower Bounds](https://arxiv.org/abs/2507.02548)
*Itai Boneh,Egor Gorbachev,Tomasz Kociumaka*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The edit distance $ed(X,Y)$ of two strings $X,Y\in \Sigma^*$ is the minimum
number of character edits (insertions, deletions, and substitutions) needed to
transform $X$ into $Y$. Its weighted counterpart $ed^w(X,Y)$ minimizes the
total cost of edits, which are specified using a function $w$, normalized so
that each edit costs at least one. The textbook dynamic-programming procedure,
given strings $X,Y\in \Sigma^{\le n}$ and oracle access to $w$, computes
$ed^w(X,Y)$ in $O(n^2)$ time. Nevertheless, one can achieve better running
times if the computed distance, denoted $k$, is small: $O(n+k^2)$ for unit
weights [Landau and Vishkin; JCSS'88] and $\tilde{O}(n+\sqrt{nk^3})$ for
arbitrary weights [Cassis, Kociumaka, Wellnitz; FOCS'23].
  In this paper, we study the dynamic version of the weighted edit distance
problem, where the goal is to maintain $ed^w(X,Y)$ for strings $X,Y\in
\Sigma^{\le n}$ that change over time, with each update specified as an edit in
$X$ or $Y$. Very recently, Gorbachev and Kociumaka [STOC'25] showed that the
unweighted distance $ed(X,Y)$ can be maintained in $\tilde{O}(k)$ time per
update after $\tilde{O}(n+k^2)$-time preprocessing; here, $k$ denotes the
current value of $ed(X,Y)$. Their algorithm generalizes to small integer
weights, but the underlying approach is incompatible with large weights.
  Our main result is a dynamic algorithm that maintains $ed^w(X,Y)$ in
$\tilde{O}(k^{3-\gamma})$ time per update after $\tilde{O}(nk^\gamma)$-time
preprocessing. Here, $\gamma\in [0,1]$ is a real trade-off parameter and $k\ge
1$ is an integer threshold fixed at preprocessing time, with $\infty$ returned
whenever $ed^w(X,Y)>k$. We complement our algorithm with conditional lower
bounds showing fine-grained optimality of our trade-off for $\gamma \in
[0.5,1)$ and justifying our choice to fix $k$.

</details>


### [13] [On the Complexity of Knapsack under Explorable Uncertainty: Hardness and Algorithms](https://arxiv.org/abs/2507.02657)
*Jens Schlöter*

Main category: cs.DS

TL;DR: 论文研究了背包问题在可探索不确定性下的离线变体，证明了其计算复杂性为Σ₂ᵖ完全，并探讨了资源增强变体的算法可能性。


<details>
  <summary>Details</summary>
Motivation: 研究在不确定性区间下如何最小化查询次数以解决背包问题，特别是在离线情况下计算最小查询集的复杂性。

Method: 通过分析离线变体的计算复杂性（Σ₂ᵖ完全性），并引入资源增强变体以放宽算法要求，从而探索非平凡算法结果。

Result: 离线变体是Σ₂ᵖ完全的，且无法非平凡近似；资源增强变体允许非平凡算法结果。

Conclusion: 离线变体的复杂性极高，但资源增强变体为算法设计提供了新的可能性。

Abstract: In the knapsack problem under explorable uncertainty, we are given a knapsack
instance with uncertain item profits. Instead of having access to the precise
profits, we are only given uncertainty intervals that are guaranteed to contain
the corresponding profits. The actual item profit can be obtained via a query.
The goal of the problem is to adaptively query item profits until the revealed
information suffices to compute an optimal (or approximate) solution to the
underlying knapsack instance. Since queries are costly, the objective is to
minimize the number of queries.
  In the offline variant of this problem, we assume knowledge of the precise
profits and the task is to compute a query set of minimum cardinality that a
third party without access to the profits could use to identify an optimal (or
approximate) knapsack solution. We show that this offline variant is complete
for the second-level of the polynomial hierarchy, i.e., $\Sigma_2^p$-complete,
and cannot be approximated within a non-trivial factor unless $\Sigma_2^p =
\Delta_2^p$. Motivated by these strong hardness results, we consider a
resource-augmented variant of the problem where the requirements on the query
set computed by an algorithm are less strict than the requirements on the
optimal solution we compare against. More precisely, a query set computed by
the algorithm must reveal sufficient information to identify an approximate
knapsack solution, while the optimal query set we compare against has to reveal
sufficient information to identify an optimal solution. We show that this
resource-augmented setting allows interesting non-trivial algorithmic results.

</details>


### [14] [Faster Algorithm for Bounded Tree Edit Distance in the Low-Distance Regime](https://arxiv.org/abs/2507.02701)
*Tomasz Kociumaka,Ali Shahali*

Main category: cs.DS

TL;DR: 论文提出了一种新的算法，用于计算加权和非加权树编辑距离的边界版本，运行时间为O(n + k^6 log k)。


<details>
  <summary>Details</summary>
Motivation: 现有的树编辑距离算法在大型但非常相似的树上运行时间过长，边界版本问题通过参数化距离k来优化运行时间。

Method: 提出了一种基于周期性结构的优化方法，并改进了现有的通用核组件。

Result: 新算法在加权和非加权设置下均实现了O(n + k^6 log k)的运行时间。

Conclusion: 该算法显著提高了计算树编辑距离的效率，尤其是在k远小于n的情况下。

Abstract: The tree edit distance is a natural dissimilarity measure between rooted
ordered trees whose nodes are labeled over an alphabet $\Sigma$. It is defined
as the minimum number of node edits (insertions, deletions, and relabelings)
required to transform one tree into the other. In the weighted variant, the
edits have associated costs (depending on the involved node labels) normalized
so that each cost is at least one, and the goal is to minimize the total cost
of edits.
  The unweighted tree edit distance between two trees of total size $n$ can be
computed in $O(n^{2.6857})$ time; in contrast, determining the weighted tree
edit distance is fine-grained equivalent to the All-Pairs Shortest Paths
problem and requires $n^3/2^{\Omega(\sqrt{\log n})}$ time [Nogler et al.;
STOC'25]. These super-quadratic running times are unattractive for large but
very similar trees, which motivates the bounded version of the problem, where
the runtime is parameterized by the computed distance $k$, potentially yielding
faster algorithms for $k\ll n$.
  Previous best algorithms for the bounded unweighted setting run in
$O(nk^2\log n)$ time [Akmal & Jin; ICALP'21] and $O(n + k^7\log k)$ time [Das
et al.; STOC'23]. For the weighted variant, the only known running time has
been $O(n + k^{15})$.
  We present an $O(n + k^6\log k)$-time algorithm for computing the bounded
tree edit distance in both the weighted and unweighted settings. Our approach
begins with an alternative $O(nk^2\log n)$-time algorithm that handles weights
and is significantly easier to analyze than the existing counterpart. We then
introduce a novel optimization that leverages periodic structures within the
input trees. To utilize it, we modify the $O(k^5)$-size $O(n)$-time universal
kernel, the central component of the prior $O(n + k^{O(1)})$-time algorithms,
so that it produces instances containing these periodic structures.

</details>


### [15] [Indexing Tries within Entropy-Bounded Space](https://arxiv.org/abs/2507.02728)
*Lorenzo Carfagna,Carlo Tosoni*

Main category: cs.DS

TL;DR: 论文研究了基于BWT的trie索引和压缩方法，提出了一种类似于FM-index的XBWT表示，用于高效计数匹配字符串模式的节点数。通过组合计数问题分析了空间复杂度，定义了trie的最坏情况熵和k阶经验熵，并证明XBWT可以在k阶经验熵加o(n)的空间内编码。最后与r-index进行了空间复杂度比较。


<details>
  <summary>Details</summary>
Motivation: 解决trie索引和压缩问题，提出更高效的表示方法以优化空间和时间性能。

Method: 基于BWT的XBWT表示，结合组合计数和熵分析，定义并比较空间复杂度。

Result: XBWT可以在k阶经验熵加o(n)的空间内编码，且在某些情况下比r-index更优。

Conclusion: 提出的方法在空间效率上优于现有技术，适用于trie的高效索引和压缩。

Abstract: We study the problem of indexing and compressing tries using a BWT-based
approach. Specifically, we consider a succinct and compressed representation of
the XBWT of Ferragina et al.\ [FOCS '05, JACM '09] corresponding to the
analogous of the FM-index [FOCS '00, JACM '05] for tries. This representation
allows to efficiently count the number of nodes reached by a given string
pattern. To analyze the space complexity of the above trie index, we propose a
proof for the combinatorial problem of counting the number of tries with a
given symbol distribution. We use this formula to define a worst-case entropy
measure for tries, as well as a notion of k-th order empirical entropy. In
particular, we show that the relationships between these two entropy measures
are similar to those between the corresponding well-known measures for strings.
We use these measures to prove that the XBWT of a trie can be encoded within a
space bounded by our k-th order empirical entropy plus a o(n) term, with n
being the number of nodes in the trie. Notably, as happens for strings, this
space bound can be reached for every sufficiently small k simultaneously.
Finally, we compare the space complexity of the above index with that of the
r-index for tries proposed by Prezza [SODA '21] and we prove that in some cases
the FM-index for tries is asymptotically smaller.

</details>


### [16] [Connected k-Median with Disjoint and Non-disjoint Clusters](https://arxiv.org/abs/2507.02774)
*Jan Eube,Kelin Luo,Dorian Reineccius,Heiko Röglin,Melanie Schmidt*

Main category: cs.DS

TL;DR: 论文研究了连通k-中值问题，提出了一种允许重叠簇的近似算法，并分析了其计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 该问题在多个领域（如地理学、社交网络分析和生物信息学）有广泛应用，尤其是社区检测场景。

Method: 提出了一种允许簇重叠的算法，并针对不同情况（如树状连通图）设计了精确算法。

Result: 证明了问题的近似复杂度下界，并提出了一个O(k² log n)的近似算法。

Conclusion: 研究为连通k-中值问题提供了理论支持，并展示了其在不同场景下的适用性。

Abstract: The connected $k$-median problem is a constrained clustering problem that
combines distance-based $k$-clustering with connectivity information. The
problem allows to input a metric space and an unweighted undirected
connectivity graph that is completely unrelated to the metric space. The goal
is to compute $k$ centers and corresponding clusters such that each cluster
forms a connected subgraph of $G$, and such that the $k$-median cost is
minimized.
  The problem has applications in very different fields like geodesy
(particularly districting), social network analysis (especially community
detection), or bioinformatics. We study a version with overlapping clusters
where points can be part of multiple clusters which is natural for the use case
of community detection. This problem variant is $\Omega(\log n)$-hard to
approximate, and our main result is an $\mathcal{O}(k^2 \log n)$-approximation
algorithm for the problem. We complement it with an
$\Omega(n^{1-\epsilon})$-hardness result for the case of disjoint clusters
without overlap with general connectivity graphs, as well as an exact algorithm
in this setting if the connectivity graph is a tree.

</details>


### [17] [On the Structure of Replicable Hypothesis Testers](https://arxiv.org/abs/2507.02842)
*Anders Aamand,Maryam Aliakbarpour,Justin Y. Chen,Shyam Narayanan,Sandeep Silwal*

Main category: cs.DS

TL;DR: 论文研究了可复现假设检验算法的样本复杂度，提出了通用工具来改进现有结果，并解决了对称性问题。


<details>
  <summary>Details</summary>
Motivation: 提高假设检验算法的可复现性，增强对测试过程的信任，并与稳定性、泛化性和隐私性建立联系。

Method: 构建通用工具证明可复现检验器的样本复杂度上下界，提出规范化属性并改进算法设计策略。

Result: 改进了均匀性、同一性和接近性检验的下界，并在高斯均值检验中取得最新成果。

Conclusion: 可复现算法设计框架显著提升了效率，并在多项测试中实现最优或接近最优的性能。

Abstract: A hypothesis testing algorithm is replicable if, when run on two different
samples from the same distribution, it produces the same output with high
probability. This notion, defined by by Impagliazzo, Lei, Pitassi, and Sorell
[STOC'22], can increase trust in testing procedures and is deeply related to
algorithmic stability, generalization, and privacy. We build general tools to
prove lower and upper bounds on the sample complexity of replicable testers,
unifying and quantitatively improving upon existing results.
  We identify a set of canonical properties, and prove that any replicable
testing algorithm can be modified to satisfy these properties without worsening
accuracy or sample complexity. A canonical replicable algorithm computes a
deterministic function of its input (i.e., a test statistic) and thresholds
against a uniformly random value in $[0,1]$. It is invariant to the order in
which the samples are received, and, if the testing problem is ``symmetric,''
then the algorithm is also invariant to the labeling of the domain elements,
resolving an open question by Liu and Ye [NeurIPS'24]. We prove new lower
bounds for uniformity, identity, and closeness testing by reducing to the case
where the replicable algorithm satisfies these canonical properties.
  We systematize and improve upon a common strategy for replicable algorithm
design based on test statistics with known expectation and bounded variance.
Our framework allow testers which have been extensively analyzed in the
non-replicable setting to be made replicable with minimal overhead. As direct
applications of our framework, we obtain constant-factor optimal bounds for
coin testing and closeness testing and get replicability for free in a large
parameter regime for uniformity testing.
  We also give state-of-the-art bounds for replicable Gaussian mean testing,
and, unlike prior work, our algorithm runs in polynomial time.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [18] [How do Software Engineering Candidates Prepare for Technical Interviews?](https://arxiv.org/abs/2507.02068)
*Brian Bell,Teresa Thomas,Sang Won Lee,Chris Brown*

Main category: cs.SE

TL;DR: 研究探讨了软件工程师技术面试准备的现状，发现候选人缺乏真实场景训练且教育支持不足，导致压力和准备不足。


<details>
  <summary>Details</summary>
Motivation: 技术面试是软件工程师求职的关键环节，但复杂且难以准备，现有教育体系支持不足，因此研究候选人准备方式及其效果。

Method: 通过向131名正在准备技术面试的候选人分发调查问卷，收集数据并分析准备方法和教育的作用。

Result: 结果显示候选人很少在真实场景中训练，课程未能有效支持准备，导致压力和准备不足。

Conclusion: 研究为利益相关者提供了改进技术面试准备的建议，以帮助候选人更好地应对软件工程师角色的求职挑战。

Abstract: To obtain employment, aspiring software engineers must complete technical
interviews -- a hiring process which involves candidates writing code while
communicating to an audience. However, the complexities of tech interviews are
difficult to prepare for and seldom faced in computing curricula. To this end,
we seek to understand how candidates prepare for technical interviews,
investigating the effects of preparation methods and the role of education. We
distributed a survey to candidates (n = 131) actively preparing for technical
interviews. Our results suggest candidates rarely train in authentic settings
and courses fail to support preparation efforts -- leading to stress and
unpreparedness. Based on our findings, we provide implications for stakeholders
to enhance tech interview preparation for candidates pursuing software
engineering roles.

</details>


### [19] [Structural Code Search using Natural Language Queries](https://arxiv.org/abs/2507.02107)
*Ben Limpanukorn,Yanjun Wang,Zach Patterson,Pranav Garg,Murali Krishna Ramanathan,Xiaofei Ma,Anoop Deoras,Miryung Kim*

Main category: cs.SE

TL;DR: 提出了一种利用自然语言进行结构化代码搜索的新方法，结合LLM和结构化搜索工具，显著提高了搜索效果。


<details>
  <summary>Details</summary>
Motivation: 开发者通常使用关键词和正则表达式搜索代码，但结构化代码搜索工具需要学习复杂的DSL，限制了其使用。自然语言搜索可以降低门槛。

Method: 结合LLM的自然语言理解能力和结构化搜索工具，将自然语言查询翻译为DSL查询，并在Semgrep和GQL上实现。

Result: 在400个查询的基准测试中，方法表现出色，精确率和召回率在55%-70%之间，显著优于基线方法。

Conclusion: 自然语言驱动的结构化代码搜索是可行且高效的，为开发者提供了更直观的搜索方式。

Abstract: Searching code is a common task that developers perform to understand APIs,
learn common code patterns, and navigate code. Currently, developers most
commonly search using keywords and regular expressions that are easy to use and
widely available. Beyond keywords and regular expressions, structural code
search tools allow developers to search for code based on its syntactic
structure. This has numerous applications ranging from bug finding to
systematically refactoring code. However, these structural code search tools
operate on queries expressed in domain-specific languages (DSL) that can be
difficult to learn and write. We propose to allow developers to use natural
language to search for code structurally. Expressing queries in natural
language provides an intuitive way to search for code and lowers the barrier to
entry.
  In this work, we develop a novel general approach that combines the reasoning
capabilities of an LLM to interpret natural language search queries with the
power of structural search tools to efficiently and accurately retrieve
relevant code. We then instantiate this approach for two structural code search
DSLs: Semgrep and GQL. In our evaluation, we construct a new benchmark for
structural code search consisting of 400 queries over 10 Java projects. We show
that our approach for structural code search based on translating NL queries to
DSL queries using an LLM is effective and robust, achieving a high precision
and recall ranging from 55% - 70%. Further, our approach significantly
outperforms baselines based on semantic code search and LLM retrievals by up to
57% and 14% on F1 scores.

</details>


### [20] [Can Internal Software Metrics Predict App Popularity at Launch? Yeas! and Nays!](https://arxiv.org/abs/2507.02110)
*Md Nahidul Islam Opu,Fatima Islam Mouri,Rick Kazman,Yuanfang Cai,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 研究探讨了通过源代码内部指标预测移动应用发布前受欢迎程度的可行性，发现分类模型表现优于回归模型，多层感知器在特定特征集下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 预测应用发布前的受欢迎程度可为开发者提供竞争优势，但现有方法效果有限。

Method: 使用446个开源Android应用的数据集，提取代码指标、代码异味和元数据，评估回归和分类模型。

Result: 回归模型表现不佳，但分类模型（尤其是多层感知器）在特定特征集下F1分数达0.72。

Conclusion: 内部代码指标可作为应用受欢迎程度的有限但有用指标，挑战了此前认为其无效的观点。

Abstract: Predicting mobile app popularity before release can provide developers with a
strategic advantage in a competitive marketplace, yet it remains a challenging
problem. This study explores whether internal software metrics, measurable from
source code before deployment, can predict an app's popularity, defined by user
ratings (calculated from user reviews) and DownloadsPerYear (yearly downloads).
Using a dataset of 446 open-source Android apps from F-Droid, we extract a wide
array of features, including system-, class-, and method-level code metrics,
code smells, and app metadata. Additional information, such as user reviews,
download counts, and uses-permission, was collected from the Google Play Store.
We evaluate regression and classification models across three feature sets: a
minimal Size-only baseline, a domain-informed Handpicked set, and a Voting set
derived via feature selection algorithms. Regression models perform poorly due
to skewed data, with low $R^2$ scores. However, when reframed as binary
classification (Popular vs. Unpopular), results improve significantly. The best
model, a Multilayer Perceptron using the Voting set, achieves F1-scores of
0.72. These results suggest that internal code metrics, although limited in
their explanatory power, can serve as useful indicators of app popularity. This
challenges earlier findings that dismissed internal metrics as predictors of
software quality.

</details>


### [21] [A Multimodal Approach Combining Biometrics and Self-Report Instruments for Monitoring Stress in Programming: Methodological Insights](https://arxiv.org/abs/2507.02118)
*Cristina Martinez Montes,Daniela Grassi,Nicole Novielli,Birgit Penzenstadle*

Main category: cs.SE

TL;DR: 研究比较了心理测量压力指标与生物特征指标，发现时间限制未能有效诱导压力，仅EDA阶段性峰值显示显著差异。


<details>
  <summary>Details</summary>
Motivation: 传统自我报告工具可能存在偏差，研究探索结合生物特征指标的替代方法。

Method: 实验包括预调查、编程任务（佩戴生物传感器）、后调查和退出访谈。

Result: 心理测量工具未显示压力，访谈反馈混合，仅EDA阶段性峰值有显著差异。

Conclusion: 时间限制不足以诱导压力，为未来研究提供了方法学启示。

Abstract: The study of well-being, stress and other human factors has traditionally
relied on self-report instruments to assess key variables. However, concerns
about potential biases in these instruments, even when thoroughly validated and
standardised, have driven growing interest in alternatives in combining these
measures with more objective methods, such as physiological measures.
  We aimed to (i) compare psychometric stress measures and biometric indicators
and (ii) identify stress-related patterns in biometric data during software
engineering tasks.
  We conducted an experiment where participants completed a pre-survey, then
programmed two tasks wearing biometric sensors, answered brief post-surveys for
each, and finally went through a short exit interview.
  Our results showed diverse outcomes; we found no stress in the psychometric
instruments. Participants in the interviews reported a mix of feeling no stress
and experiencing time pressure. Finally, the biometrics showed a significant
difference only in EDA phasic peaks.
  We conclude that our chosen way of inducing stress by imposing a stricter
time limit was insufficient. We offer methodological insights for future
studies working with stress, biometrics, and psychometric instruments.

</details>


### [22] [Towards Trustworthy Sentiment Analysis in Software Engineering: Dataset Characteristics and Tool Selection](https://arxiv.org/abs/2507.02137)
*Martin Obaidi,Marc Herrmann,Jil Klünder,Kurt Schneider*

Main category: cs.SE

TL;DR: 研究分析了10个开发者通信数据集的语言和统计特征，评估了14种情感分析工具的性能，并提出了一种基于数据集特征的工具推荐方法。


<details>
  <summary>Details</summary>
Motivation: 现有情感分析工具在不同平台数据集上表现不一致，影响团队动态分析和可信AI驱动的需求工程。

Method: 分析5个平台的10个数据集的语言和统计特征，评估14种工具，提出基于特征的映射方法和问卷。

Result: 数据集特征可改进工具选择，Transformer模型表现稳定但效果仍依赖上下文。

Conclusion: 方法支持选择可信工具，强调需持续评估以适应通信环境变化。

Abstract: Software development relies heavily on text-based communication, making
sentiment analysis a valuable tool for understanding team dynamics and
supporting trustworthy AI-driven analytics in requirements engineering.
However, existing sentiment analysis tools often perform inconsistently across
datasets from different platforms, due to variations in communication style and
content.
  In this study, we analyze linguistic and statistical features of 10 developer
communication datasets from five platforms and evaluate the performance of 14
sentiment analysis tools. Based on these results, we propose a mapping approach
and questionnaire that recommends suitable sentiment analysis tools for new
datasets, using their characteristic features as input.
  Our results show that dataset characteristics can be leveraged to improve
tool selection, as platforms differ substantially in both linguistic and
statistical properties. While transformer-based models such as SetFit and
RoBERTa consistently achieve strong results, tool effectiveness remains
context-dependent. Our approach supports researchers and practitioners in
selecting trustworthy tools for sentiment analysis in software engineering,
while highlighting the need for ongoing evaluation as communication contexts
evolve.

</details>


### [23] [Enhancing COBOL Code Explanations: A Multi-Agents Approach Using Large Language Models](https://arxiv.org/abs/2507.02182)
*Fangjian Lei,Jiawen Liu,Shayan Noei,Ying Zou,Derek Truong,William Alexander*

Main category: cs.SE

TL;DR: 提出了一种多智能体方法，利用两个基于LLM的智能体协作生成COBOL代码的解释，解决了传统LLM因COBOL代码过长而无法处理的问题。


<details>
  <summary>Details</summary>
Motivation: COBOL代码维护困难，缺乏文档，且现有LLM方法因COBOL的独特语法和长度限制效果不佳。

Method: 采用多智能体协作方法，结合代码库上下文信息生成代码解释。

Result: 在14个开源COBOL项目上测试，方法在功能、文件和项目级别的解释效果均显著优于基线。

Conclusion: 多智能体方法有效解决了COBOL代码解释的挑战，显著提升了生成解释的质量。

Abstract: Common Business Oriented Language (COBOL) is a programming language used to
develop business applications that are widely adopted by financial, business,
and government agencies. Due to its age, complexity, and declining number of
COBOL developers, maintaining COBOL codebases is becoming increasingly
challenging. In particular, the lack of documentation makes it difficult for
new developers to effectively understand and maintain COBOL systems. Existing
research utilizes large language models (LLMs) to explain the functionality of
code snippets. However, COBOL presents unique challenges due to its
architectural and syntactical differences, which often cause its code to exceed
the token window size of LLMs. In this work, we propose a multi-agent approach
that leverages two LLM-based agents working collaboratively to generate
explanations for functions, files, and the overall project. These agents
incorporate together by utilizing contextual information from the codebase into
the code explanation prompts. We evaluate the effectiveness of our approach
using 14 open-source, real-world COBOL projects. Our results indicate that our
approach performs significantly better than the baseline in function code
explanation, with improvements of 12.67%, 18.59%, and 0.62% in terms of METEOR,
chrF, and SentenceBERT scores, respectively. At the file level, our approach
effectively explains both short and long COBOL files that exceed the token
window size of LLMs and surpass the baseline by 4.21%, 10.72%, and 14.68% in
explaining the purpose, functionality, and clarity of the generated
explanation. At the project level, our approach generates explanations that
convey the functionality and purpose of 82% of the selected projects.

</details>


### [24] [VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative Software](https://arxiv.org/abs/2507.02376)
*Chung-ju Huang,Ziqi Zhang,Yinggui Wang,Binghui Wang,Tao Wei,Leye Wang*

Main category: cs.SE

TL;DR: VeFIA框架为垂直联邦学习（VFL）提供了一种审计机制，确保数据方的推理软件执行正确性，同时保护数据隐私且不增加延迟。


<details>
  <summary>Details</summary>
Motivation: 现有VFL工作缺乏对数据方推理软件执行正确性的审计机制，可能导致潜在风险。

Method: 设计VeFIA框架，利用可信执行环境（TEE）和协调器验证数据方的计算结果。

Result: VeFIA在异常推理超过5.4%时能以99.99%概率检测异常，且随机采样验证的准确率达100%。

Conclusion: VeFIA是首个解决VFL中推理软件执行正确性问题的框架，具有高效性和隐私保护性。

Abstract: Vertical Federated Learning (VFL) is a distributed AI software deployment
mechanism for cross-silo collaboration without accessing participants' data.
However, existing VFL work lacks a mechanism to audit the execution correctness
of the inference software of the data party. To address this problem, we design
a Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task
party to audit whether the data party's inference software is executed as
expected during large-scale inference without leaking the data privacy of the
data party or introducing additional latency to the inference system. The core
of VeFIA is that the task party can use the inference results from a framework
with Trusted Execution Environments (TEE) and the coordinator to validate the
correctness of the data party's computation results. VeFIA guarantees that, as
long as the abnormal inference exceeds 5.4%, the task party can detect
execution anomalies in the inference software with a probability of 99.99%,
without incurring any additional online inference latency. VeFIA's random
sampling validation achieves 100% positive predictive value, negative
predictive value, and true positive rate in detecting abnormal inference. To
the best of our knowledge, this is the first paper to discuss the correctness
of inference software execution in VFL.

</details>


### [25] [Precisely Detecting Python Type Errors via LLM-based Unit Test Generation](https://arxiv.org/abs/2507.02318)
*Chen Yang,Ziqi Wang,Yanjie Jiang,Lin Yang,Yuteng Zheng,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: RTED是一种新型的类型感知测试生成技术，用于自动检测Python类型错误，结合类型约束分析和反射验证，显著减少误报并提高检测能力。


<details>
  <summary>Details</summary>
Motivation: Python中的类型错误常导致运行时故障，现有静态分析工具误报率高，单元测试生成技术缺乏针对性指导。

Method: RTED结合逐步类型约束分析和反射验证，指导测试生成过程。

Result: 在BugsInPy和TypeBugs基准测试中，RTED比现有技术多检测22-29个类型错误，误报率降低173.9%-245.9%，并发现12个未知错误。

Conclusion: RTED在检测Python类型错误方面表现出色，显著优于现有技术。

Abstract: Type errors in Python often lead to runtime failures, posing significant
challenges to software reliability and developer productivity. Existing static
analysis tools aim to detect such errors without execution but frequently
suffer from high false positive rates. Recently, unit test generation
techniques offer great promise in achieving high test coverage, but they often
struggle to produce bug-revealing tests without tailored guidance. To address
these limitations, we present RTED, a novel type-aware test generation
technique for automatically detecting Python type errors. Specifically, RTED
combines step-by-step type constraint analysis with reflective validation to
guide the test generation process and effectively suppress false positives. We
evaluated RTED on two widely-used benchmarks, BugsInPy and TypeBugs.
Experimental results show that RTED can detect 22-29 more benchmarked type
errors than four state-of-the-art techniques. RTED is also capable of producing
fewer false positives, achieving an improvement of 173.9%-245.9% in precision.
Furthermore, RTED successfully discovered 12 previously unknown type errors
from six real-world open-source Python projects.

</details>


### [26] [Meta-Fair: AI-Assisted Fairness Testing of Large Language Models](https://arxiv.org/abs/2507.02533)
*Miguel Romero-Arjona,José A. Parejo,Juan C. Alonso,Ana B. Sánchez,Aitor Arrieta,Sergio Segura*

Main category: cs.SE

TL;DR: 提出了一种自动化测试LLM公平性的新方法Meta-Fair，结合了蜕变测试和LLM的能力，减少了资源依赖并提高了可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM公平性测试方法依赖人工和固定模板，资源密集且难以扩展，需要一种更自动化的解决方案。

Method: 采用蜕变测试（MRs）和LLM生成测试用例及评估输出，开发了三个开源工具支持测试流程。

Result: Meta-Fair在12个LLM上测试，平均精度92%，发现29%的执行存在偏见，LLM评估表现良好（F1-score达0.79）。

Conclusion: Meta-Fair展示了自动化测试LLM公平性的潜力，尽管仍需改进，但为未来研究提供了方向。

Abstract: Fairness--the absence of unjustified bias--is a core principle in the
development of Artificial Intelligence (AI) systems, yet it remains difficult
to assess and enforce. Current approaches to fairness testing in large language
models (LLMs) often rely on manual evaluation, fixed templates, deterministic
heuristics, and curated datasets, making them resource-intensive and difficult
to scale. This work aims to lay the groundwork for a novel, automated method
for testing fairness in LLMs, reducing the dependence on domain-specific
resources and broadening the applicability of current approaches. Our approach,
Meta-Fair, is based on two key ideas. First, we adopt metamorphic testing to
uncover bias by examining how model outputs vary in response to controlled
modifications of input prompts, defined by metamorphic relations (MRs). Second,
we propose exploiting the potential of LLMs for both test case generation and
output evaluation, leveraging their capability to generate diverse inputs and
classify outputs effectively. The proposal is complemented by three open-source
tools supporting LLM-driven generation, execution, and evaluation of test
cases. We report the findings of several experiments involving 12 pre-trained
LLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases.
The results show that Meta-Fair is effective in uncovering bias in LLMs,
achieving an average precision of 92% and revealing biased behaviour in 29% of
executions. Additionally, LLMs prove to be reliable and consistent evaluators,
with the best-performing models achieving F1-scores of up to 0.79. Although
non-determinism affects consistency, these effects can be mitigated through
careful MR design. While challenges remain to ensure broader applicability, the
results indicate a promising path towards an unprecedented level of automation
in LLM testing.

</details>


### [27] [LLMREI: Automating Requirements Elicitation Interviews with LLMs](https://arxiv.org/abs/2507.02564)
*Alexander Korn,Samuel Gorsch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: LLMREI是一种基于大型语言模型的聊天机器人，旨在自动化需求获取访谈，减少人为错误并提高可扩展性。研究评估了其性能，发现其在减少错误、提取需求和生成上下文相关问题上表现良好。


<details>
  <summary>Details</summary>
Motivation: 需求获取访谈依赖分析师技能，资源密集且易受人为偏见和沟通问题影响。利用大型语言模型自动化部分流程可提高效率和准确性。

Method: 研究设计了LLMREI，采用零样本提示和最少到最多提示两种方法优化需求获取，并在33次模拟访谈中评估其性能。

Result: LLMREI在减少错误、提取需求和生成上下文相关问题上表现与人类访谈者相当，尤其适合大规模利益相关者访谈。

Conclusion: LLMREI在自动化需求获取访谈中具有潜力，特别适用于大规模场景，未来可进一步优化其性能。

Abstract: Requirements elicitation interviews are crucial for gathering system
requirements but heavily depend on skilled analysts, making them
resource-intensive, susceptible to human biases, and prone to miscommunication.
Recent advancements in Large Language Models present new opportunities for
automating parts of this process. This study introduces LLMREI, a chat bot
designed to conduct requirements elicitation interviews with minimal human
intervention, aiming to reduce common interviewer errors and improve the
scalability of requirements elicitation. We explored two main approaches,
zero-shot prompting and least-to-most prompting, to optimize LLMREI for
requirements elicitation and evaluated its performance in 33 simulated
stakeholder interviews. A third approach, fine-tuning, was initially considered
but abandoned due to poor performance in preliminary trials. Our study assesses
the chat bot's effectiveness in three key areas: minimizing common interview
errors, extracting relevant requirements, and adapting its questioning based on
interview context and user responses. Our findings indicate that LLMREI makes a
similar number of errors compared to human interviewers, is capable of
extracting a large portion of requirements, and demonstrates a notable ability
to generate highly context-dependent questions. We envision the greatest
benefit of LLMREI in automating interviews with a large number of stakeholders.

</details>


### [28] [Human-Machine Collaboration and Ethical Considerations in Adaptive Cyber-Physical Systems](https://arxiv.org/abs/2507.02578)
*Zoe Pfister*

Main category: cs.SE

TL;DR: 论文探讨了如何在自适应信息物理系统（CPS）中实现有效的人机协作（HMT），并解决隐私和伦理问题。


<details>
  <summary>Details</summary>
Motivation: 自适应CPS需要整合人机协作，但现有反馈循环（如MAPE-K）难以适应人与机器的不同操作节奏，且涉及隐私和伦理问题。

Method: 开发新方法将HMT融入自适应CPS，并构建框架以在系统生命周期中验证伦理和人类价值观。

Result: 提出了整合HMT的方法和伦理框架，以提升人机协作的效率和伦理合规性。

Conclusion: 通过新方法和框架，研究为自适应CPS中的人机协作提供了解决方案，兼顾效率和伦理。

Abstract: Adaptive Cyber-Physical Systems (CPS) are systems that integrate both
physical and computational capabilities, which can adjust in response to
changing parameters. Furthermore, they increasingly incorporate human-machine
collaboration, allowing them to benefit from the individual strengths of humans
and machines. Human-Machine Teaming (HMT) represents the most advanced paradigm
of human-machine collaboration, envisioning seamless teamwork between humans
and machines. However, achieving effective and seamless HMT in adaptive CPS is
challenging. While adaptive CPS already benefit from feedback loops such as
MAPE-K, there is still a gap in integrating humans into these feedback loops
due to different operational cadences of humans and machines. Further, HMT
requires constant monitoring of human operators, collecting potentially
sensitive information about their actions and behavior. Respecting the privacy
and human values of the actors of the CPS is crucial for the success of
human-machine teams. This research addresses these challenges by: (1)
developing novel methods and processes for integrating HMT into adaptive CPS,
focusing on human-machine interaction principles and their incorporation into
adaptive feedback loops found in CPS, and (2) creating frameworks for
integrating, verifying, and validating ethics and human values throughout the
system lifecycle, starting from requirements engineering.

</details>


### [29] [Do Research Software Engineers and Software Engineering Researchers Speak the Same Language?](https://arxiv.org/abs/2507.02665)
*Timo Kehrer,Robert Haines,Guido Juckeland,Shurui Zhou,David E. Bernholdt*

Main category: cs.SE

TL;DR: 研究发现RSEs和SERs在术语使用上存在差异，初步揭示了合作潜力，并提出了一种系统化的术语映射方法。


<details>
  <summary>Details</summary>
Motivation: 解决RSEs和SERs因术语差异导致的沟通障碍。

Method: 通过调查SE基础概念在RSE社区中的解释，识别一致概念、知识差距及适应领域。

Result: 初步发现合作机会，并提出术语映射方法。

Conclusion: 术语映射方法为未来众包扩展和验证奠定了基础。

Abstract: Anecdotal evidence suggests that Research Software Engineers (RSEs) and
Software Engineering Researchers (SERs) often use different terminologies for
similar concepts, creating communication challenges. To better understand these
divergences, we have started investigating how SE fundamentals from the SER
community are interpreted within the RSE community, identifying aligned
concepts, knowledge gaps, and areas for potential adaptation. Our preliminary
findings reveal opportunities for mutual learning and collaboration, and our
systematic methodology for terminology mapping provides a foundation for a
crowd-sourced extension and validation in the future.

</details>


### [30] [RLHGNN: Reinforcement Learning-driven Heterogeneous Graph Neural Network for Next Activity Prediction in Business Processes](https://arxiv.org/abs/2507.02690)
*Jiaxing Wang,Yifeng Yu,Jiahan Song,Bin Cao,Jing Fan,Ji Zhang*

Main category: cs.SE

TL;DR: RLHGNN是一个新颖的框架，通过将事件日志转换为异构过程图，并结合强化学习，有效预测业务流程中的下一活动，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有序列和基于图的方法在捕捉非顺序关系和适应不同流程复杂性方面的不足。

Method: 将事件日志转换为异构过程图，结合强化学习自动选择最优图结构，并使用异构图卷积进行预测。

Result: 在六个真实数据集上表现优于现有方法，推理延迟约为1毫秒。

Conclusion: RLHGNN是一种高效且实用的解决方案，适用于实时业务流程监控。

Abstract: Next activity prediction represents a fundamental challenge for optimizing
business processes in service-oriented architectures such as microservices
environments, distributed enterprise systems, and cloud-native platforms, which
enables proactive resource allocation and dynamic service composition. Despite
the prevalence of sequence-based methods, these approaches fail to capture
non-sequential relationships that arise from parallel executions and
conditional dependencies. Even though graph-based approaches address structural
preservation, they suffer from homogeneous representations and static
structures that apply uniform modeling strategies regardless of individual
process complexity characteristics. To address these limitations, we introduce
RLHGNN, a novel framework that transforms event logs into heterogeneous process
graphs with three distinct edge types grounded in established process mining
theory. Our approach creates four flexible graph structures by selectively
combining these edges to accommodate different process complexities, and
employs reinforcement learning formulated as a Markov Decision Process to
automatically determine the optimal graph structure for each specific process
instance. RLHGNN then applies heterogeneous graph convolution with
relation-specific aggregation strategies to effectively predict the next
activity. This adaptive methodology enables precise modeling of both sequential
and non-sequential relationships in service interactions. Comprehensive
evaluation on six real-world datasets demonstrates that RLHGNN consistently
outperforms state-of-the-art approaches. Furthermore, it maintains an inference
latency of approximately 1 ms per prediction, representing a highly practical
solution suitable for real-time business process monitoring applications. The
source code is available at https://github.com/Joker3993/RLHGNN.

</details>


### [31] [Sustainability Flags for the Identification of Sustainability Posts in Q&A Platforms](https://arxiv.org/abs/2507.02695)
*Sahar Ahmadisakha,Lech Bialek,Mohamed Soliman,Vasilios Andrikopoulos*

Main category: cs.SE

TL;DR: 论文提出了一种通过可持续性标志识别云架构讨论中可持续性概念的方法，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着云计算的兴起，可持续性在软件架构中的重要性增加，但缺乏明确的识别指南。

Method: 通过主题分析开发可持续性标志，并通过对照实验评估其有效性。

Result: 使用标志分类的帖子较少但确定性更高，性能显著提升，标志比定义更易理解。

Conclusion: 可持续性标志是识别云架构讨论中可持续性概念的有效工具。

Abstract: In recent years, sustainability in software systems has gained significant
attention, especially with the rise of cloud computing and the shift towards
cloud-based architectures. This shift has intensified the need to identify
sustainability in architectural discussions to take informed architectural
decisions. One source to see these decisions is in online Q&A forums among
practitioners' discussions. However, recognizing sustainability concepts within
software practitioners' discussions remains challenging due to the lack of
clear and distinct guidelines for this task. To address this issue, we
introduce the notion of sustainability flags as pointers in relevant
discussions, developed through thematic analysis of multiple sustainability
best practices from cloud providers. This study further evaluates the
effectiveness of these flags in identifying sustainability within cloud
architecture posts, using a controlled experiment. Preliminary results suggest
that the use of flags results in classifying fewer posts as
sustainability-related compared to a control group, with moderately higher
certainty and significantly improved performance. Moreover, sustainability
flags are perceived as more useful and understandable than relying solely on
definitions for identifying sustainability.

</details>


### [32] [Legal Requirements Translation from Law](https://arxiv.org/abs/2507.02846)
*Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 论文提出了一种基于文本蕴含和上下文学习的方法，自动生成法律文本的规范化表示，可编码为Python代码，减少对人工标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 小型组织和初创公司缺乏法律专业知识，难以从复杂的法律文本中提取元数据以确保软件合规。现有方法未能充分考虑元数据属性间的交互关系，且依赖人工标注或启发式机器学习，泛化能力不足。

Method: 采用文本蕴含和上下文学习技术，设计了一个Python类结构作为领域特定元模型，捕获法律元数据的结构和语义及其相互关系。

Result: 在13个美国州数据泄露通知法律上评估，生成的表示通过约89.4%的测试用例，精确率和召回率分别为82.2和88.7。

Conclusion: 该方法有效减少了人工标注需求，提升了法律文本元数据提取的泛化能力，适用于新法规。

Abstract: Software systems must comply with legal regulations, which is a
resource-intensive task, particularly for small organizations and startups
lacking dedicated legal expertise. Extracting metadata from regulations to
elicit legal requirements for software is a critical step to ensure compliance.
However, it is a cumbersome task due to the length and complex nature of legal
text. Although prior work has pursued automated methods for extracting
structural and semantic metadata from legal text, key limitations remain: they
do not consider the interplay and interrelationships among attributes
associated with these metadata types, and they rely on manual labeling or
heuristic-driven machine learning, which does not generalize well to new
documents. In this paper, we introduce an approach based on textual entailment
and in-context learning for automatically generating a canonical representation
of legal text, encodable and executable as Python code. Our representation is
instantiated from a manually designed Python class structure that serves as a
domain-specific metamodel, capturing both structural and semantic legal
metadata and their interrelationships. This design choice reduces the need for
large, manually labeled datasets and enhances applicability to unseen
legislation. We evaluate our approach on 13 U.S. state data breach notification
laws, demonstrating that our generated representations pass approximately 89.4%
of test cases and achieve a precision and recall of 82.2 and 88.7,
respectively.

</details>


### [33] [Requirements Elicitation Follow-Up Question Generation](https://arxiv.org/abs/2507.02858)
*Yuchen Shen,Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 论文探讨了利用GPT-4o生成需求访谈中的后续问题，实验表明LLM生成的问题在清晰度、相关性和信息量上不亚于人类编写的问题，且在指导后表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决访谈中因领域不熟、认知负荷和信息过载导致的问题，利用LLM提升需求获取效率。

Method: 基于常见访谈错误类型框架生成问题，并通过实验比较LLM生成与人类编写的问题。

Result: LLM生成的问题在无指导时与人类相当，在指导后表现更优。

Conclusion: LLM可实时提升需求访谈的质量和效率。

Abstract: Interviews are a widely used technique in eliciting requirements to gather
stakeholder needs, preferences, and expectations for a software system.
Effective interviewing requires skilled interviewers to formulate appropriate
interview questions in real time while facing multiple challenges, including
lack of familiarity with the domain, excessive cognitive load, and information
overload that hinders how humans process stakeholders' speech. Recently, large
language models (LLMs) have exhibited state-of-the-art performance in multiple
natural language processing tasks, including text summarization and entailment.
To support interviewers, we investigate the application of GPT-4o to generate
follow-up interview questions during requirements elicitation by building on a
framework of common interviewer mistake types. In addition, we describe methods
to generate questions based on interviewee speech. We report a controlled
experiment to evaluate LLM-generated and human-authored questions with minimal
guidance, and a second controlled experiment to evaluate the LLM-generated
questions when generation is guided by interviewer mistake types. Our findings
demonstrate that, for both experiments, the LLM-generated questions are no
worse than the human-authored questions with respect to clarity, relevancy, and
informativeness. In addition, LLM-generated questions outperform human-authored
questions when guided by common mistakes types. This highlights the potential
of using LLMs to help interviewers improve the quality and ease of requirements
elicitation interviews in real time.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [34] [A Comprehensive Survey on Network Traffic Synthesis: From Statistical Models to Deep Learning](https://arxiv.org/abs/2507.01976)
*Nirhoshan Sivaroopan,Kaushitha Silva,Chamara Madarasingha,Thilini Dahanayaka,Guillaume Jourjon,Anura Jayasumana,Kanchana Thilakarathna*

Main category: cs.NI

TL;DR: 本文综述了合成网络流量生成的方法，重点介绍了深度学习和统计技术，并讨论了挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 解决真实网络数据稀缺、隐私和纯度问题，为数据驱动应用提供替代方案。

Method: 综述了合成网络流量生成的数据类型、生成模型和评估方法，特别关注深度学习技术。

Result: 提供了现有方法的全面分析，并指出商业工具和统计方法的扩展。

Conclusion: 本文为研究者和从业者提供了合成网络流量生成的结构化资源，总结了挑战与机遇。

Abstract: Synthetic network traffic generation has emerged as a promising alternative
for various data-driven applications in the networking domain. It enables the
creation of synthetic data that preserves real-world characteristics while
addressing key challenges such as data scarcity, privacy concerns, and purity
constraints associated with real data. In this survey, we provide a
comprehensive review of synthetic network traffic generation approaches,
covering essential aspects such as data types, generation models, and
evaluation methods. With the rapid advancements in AI and machine learning, we
focus particularly on deep learning-based techniques while also providing a
detailed discussion of statistical methods and their extensions, including
commercially available tools. Furthermore, we highlight open challenges in this
domain and discuss potential future directions for further research and
development. This survey serves as a foundational resource for researchers and
practitioners, offering a structured analysis of existing methods, challenges,
and opportunities in synthetic network traffic generation.

</details>


### [35] [Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers](https://arxiv.org/abs/2507.01988)
*Giyong Jung,Saeid Gorgin,John Kim,Jungrae Kim*

Main category: cs.NI

TL;DR: 论文提出了一种名为ISN的新机制和RXL扩展协议，用于解决多节点芯片互连中的可靠性和可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型对计算能力的需求增加，芯片互连的可靠性和可扩展性成为关键挑战。现有协议在高传输速率下易受错误影响，且多节点配置中管理数据丢失问题复杂。

Method: 提出Implicit Sequence Number (ISN)机制，实现精确的数据丢失检测和有序传输；扩展CXL协议为Reliability Extended Link (RXL)，结合ISN支持多节点互连。

Result: RXL通过提升CRC为传输层机制，结合FEC，实现了高可靠性和可扩展性，同时保持带宽效率。

Conclusion: ISN和RXL为多节点芯片互连提供了高效、可靠的解决方案，兼容现有协议结构。

Abstract: As AI models outpace the capabilities of single processors, interconnects
across chips have become a critical enabler for scalable computing. These
processors exchange massive amounts of data at cache-line granularity,
prompting the adoption of new interconnect protocols like CXL, NVLink, and
UALink, designed for high bandwidth and small payloads. However, the increasing
transfer rates of these protocols heighten susceptibility to errors. While
mechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction
(FEC) are standard for reliable data transmission, scaling chip interconnects
to multi-node configurations introduces new challenges, particularly in
managing silently dropped flits in switching devices. This paper introduces
Implicit Sequence Number (ISN), a novel mechanism that ensures precise flit
drop detection and in-order delivery without adding header overhead.
Additionally, we propose Reliability Extended Link (RXL), an extension of CXL
that incorporates ISN to support scalable, reliable multi-node interconnects
while maintaining compatibility with the existing flit structure. By elevating
CRC to a transport-layer mechanism for end-to-end data and sequence integrity,
and relying on FEC for link-layer error correction and detection, RXL delivers
robust reliability and scalability without compromising bandwidth efficiency.

</details>


### [36] [Curated Collaborative AI Edge with Network Data Analytics for B5G/6G Radio Access Networks](https://arxiv.org/abs/2507.01994)
*Sardar Jaffar Ali,Syed M. Raza,Duc-Tai Le,Rajesh Challa,Min Young Chung,Ness Shroff,Hyunseung Choo*

Main category: cs.NI

TL;DR: 提出了一种结合Curated Collaborative Learning (CCL)和Distributed Unit Pooling Scheme (DUPS)的方法，显著降低5G RAN的能耗和运营成本。


<details>
  <summary>Details</summary>
Motivation: 5G网络中RAN的能耗占比超过50%，现有方法未能充分利用数据潜力，亟需优化以减少运营支出。

Method: 1. 使用CCL框架进行高精度网络流量和用户预测；2. 提出DUPS方案，通过深度强化学习和CCL预测动态优化DU服务器资源。

Result: CCL在预测性能上优于现有方法43.9%~31.35%；DUPS将能效提升89%，显著降低运营成本。

Conclusion: 结合CCL和DUPS的方法为5G RAN的能效和成本效益提供了变革性解决方案。

Abstract: Despite advancements, Radio Access Networks (RAN) still account for over 50\%
of the total power consumption in 5G networks. Existing RAN split options do
not fully harness data potential, presenting an opportunity to reduce
operational expenditures. This paper addresses this opportunity through a
twofold approach. First, highly accurate network traffic and user predictions
are achieved using the proposed Curated Collaborative Learning (CCL) framework,
which selectively collaborates with relevant correlated data for traffic
forecasting. CCL optimally determines whom, when, and what to collaborate with,
significantly outperforming state-of-the-art approaches, including global,
federated, personalized federated, and cyclic institutional incremental
learnings by 43.9%, 39.1%, 40.8%, and 31.35%, respectively. Second, the
Distributed Unit Pooling Scheme (DUPS) is proposed, leveraging deep
reinforcement learning and prediction inferences from CCL to reduce the number
of active DU servers efficiently. DUPS dynamically redirects traffic from
underutilized DU servers to optimize resource use, improving energy efficiency
by up to 89% over conventional strategies, translating into substantial
monetary benefits for operators. By integrating CCL-driven predictions with
DUPS, this paper demonstrates a transformative approach for minimizing energy
consumption and operational costs in 5G RANs, significantly enhancing
efficiency and cost-effectiveness.

</details>


### [37] [Towards a Playground to Democratize Experimentation and Benchmarking of AI Agents for Network Troubleshooting](https://arxiv.org/abs/2507.01997)
*Zhihao Wang,Alessandro Cornacchia,Franco Galante,Carlo Centofanti,Alessio Sacco,Dingde Jiang*

Main category: cs.NI

TL;DR: 论文探讨了AI（尤其是大语言模型）在网络配置和诊断中的应用，并提出了构建标准化、可复现的基准测试平台的必要性。


<details>
  <summary>Details</summary>
Motivation: 网络故障诊断和配置合成的自动化需求推动了AI的应用，但目前缺乏统一的评估平台。

Method: 研究聚焦于AI代理在网络故障诊断中的应用，并提出了构建基准测试平台的初步设想。

Result: 初步工作强调了标准化平台对评估AI代理的重要性。

Conclusion: 需要开发一个低操作成本的标准化平台，以支持AI代理的构建和评估。

Abstract: Recent research has demonstrated the effectiveness of Artificial Intelligence
(AI), and more specifically, Large Language Models (LLMs), in supporting
network configuration synthesis and automating network diagnosis tasks, among
others. In this preliminary work, we restrict our focus to the application of
AI agents to network troubleshooting and elaborate on the need for a
standardized, reproducible, and open benchmarking platform, where to build and
evaluate AI agents with low operational effort.

</details>


### [38] [AI-Empowered Channel Generation for IoV Semantic Communications in Dynamic Conditions](https://arxiv.org/abs/2507.02013)
*Hao Liu,Bo Yang,Zhiwen Yu,Xuelin Cao,George C. Alexandropoulos,Yan Zhang,Chau Yuen*

Main category: cs.NI

TL;DR: 本文提出了一种基于信道感知的语义通信框架，用于提升车联网（IoV）中数据传输的准确性和效率，并通过生成扩散模型和大模型优化动态场景下的信道估计。


<details>
  <summary>Details</summary>
Motivation: 车联网（IoV）需要高效处理大量实时数据，但动态无线信道条件带来挑战。

Method: 提出语义通信框架，结合生成扩散模型进行信道估计，并利用大模型优化动态场景适应性。

Result: 在公开数据集上验证了框架的性能和可靠性。

Conclusion: 该框架提升了车联网服务的质量，尤其在动态场景下表现优越。

Abstract: The Internet of Vehicles (IoV) transforms the transportation ecosystem
promising pervasive connectivity and data-driven approaches. Deep learning and
generative Artificial Intelligence (AI) have the potential to significantly
enhance the operation of applications within IoV by facilitating efficient
decision-making and predictive capabilities, including intelligent navigation,
vehicle safety monitoring, accident prevention, and intelligent traffic
management. Nevertheless, efficiently transmitting and processing the massive
volumes of data generated by the IoV in real-time remains a significant
challenge, particularly in dynamic and unpredictable wireless channel
conditions. To address these challenges, this paper proposes a semantic
communication framework based on channel perception to improve the accuracy and
efficiency of data transmission. The semantic communication model extracts and
compresses the information to be transmitted. In addition, the wireless channel
is estimated by using a generative diffusion model, which is employed to
predict the dynamic channel states, thereby improving the quality of IoV
service. In dynamic scenarios, however, the channel estimation performance may
be degraded when substantially new scenarios take place, which will adversely
affect user experience. To mitigate this limitation, we employ a large model to
fine-tune the channel generation model to enhance its adaptability for varying
scenarios. The performance and reliability of the proposed framework are
evaluated on the two public datasets.

</details>


### [39] [REDUS: Adaptive Resampling for Efficient Deep Learning in Centralized and Federated IoT Networks](https://arxiv.org/abs/2507.02021)
*Eyad Gad,Gad Gad,Mostafa M. Fouda,Mohamed I. Ibrahem,Muhammad Ismail,Zubair Md Fadlullah*

Main category: cs.NI

TL;DR: 论文提出REDUS技术，通过优化深度学习训练样本，减少资源占用，提升智能网络效率。


<details>
  <summary>Details</summary>
Motivation: SDN与深度学习任务共享资源时，资源竞争导致网络性能下降，尤其在IoT环境中。

Method: 提出REDUS技术，基于AdaBoost思想，优先处理误分类样本并排除冗余数据。

Result: 在CICIoT2023数据集上，训练时间减少72.6%，准确率仅损失1.62%。

Conclusion: REDUS为资源受限的边缘设备提供高效、可扩展的解决方案。

Abstract: With the rise of Software-Defined Networking (SDN) for managing traffic and
ensuring seamless operations across interconnected devices, challenges arise
when SDN controllers share infrastructure with deep learning (DL) workloads.
Resource contention between DL training and SDN operations, especially in
latency-sensitive IoT environments, can degrade SDN's responsiveness and
compromise network performance. Federated Learning (FL) helps address some of
these concerns by decentralizing DL training to edge devices, thus reducing
data transmission costs and enhancing privacy. Yet, the computational demands
of DL training can still interfere with SDN's performance, especially under the
continuous data streams characteristic of IoT systems. To mitigate this issue,
we propose REDUS (Resampling for Efficient Data Utilization in Smart-Networks),
a resampling technique that optimizes DL training by prioritizing misclassified
samples and excluding redundant data, inspired by AdaBoost. REDUS reduces the
number of training samples per epoch, thereby conserving computational
resources, reducing energy consumption, and accelerating convergence without
significantly impacting accuracy. Applied within an FL setup, REDUS enhances
the efficiency of model training on resource-limited edge devices while
maintaining network performance. In this paper, REDUS is evaluated on the
CICIoT2023 dataset for IoT attack detection, showing a training time reduction
of up to 72.6% with a minimal accuracy loss of only 1.62%, offering a scalable
and practical solution for intelligent networks.

</details>


### [40] [MULTI-SCOUT: Multistatic Integrated Sensing and Communications in 5G and Beyond for Moving Target Detection, Positioning, and Tracking](https://arxiv.org/abs/2507.02613)
*Yalin E. Sagduyu,Kemal Davaslioglu,Tugba Erpek,Sastry Kompella,Gustave Anderson,Jonathan Ashdown*

Main category: cs.NI

TL;DR: 本文提出了一种基于5G PRS信号的多基地集成感知与通信（ISAC）完整信号处理链，用于目标检测、参数估计和跟踪。


<details>
  <summary>Details</summary>
Motivation: 利用5G PRS信号实现多基地ISAC，解决目标检测、定位和跟踪问题。

Method: 采用分布式架构，通过相干交叉模糊函数（CAF）生成距离-多普勒图，提取目标的双基地延迟和径向速度，并通过非线性最小二乘三边测量和线性反演估计位置和速度。

Result: 实验结果表明，该方法能够高精度地实现移动目标的检测、定位和跟踪。

Conclusion: 该方法在多基地ISAC中表现出色，适用于2D和3D场景，并能处理时间同步偏差和多目标关联问题。

Abstract: This paper presents a complete signal-processing chain for multistatic
integrated sensing and communications (ISAC) using 5G Positioning Reference
Signal (PRS). We consider a distributed architecture in which one gNB transmits
a periodic OFDM-PRS waveform while multiple spatially separated receivers
exploit the same signal for target detection, parameter estimation and
tracking. A coherent cross-ambiguity function (CAF) is evaluated to form a
range-Doppler map from which the bistatic delay and radial velocity are
extracted for every target. For a single target, the resulting bistatic delays
are fused through nonlinear least-squares trilateration, yielding a geometric
position estimate, and a regularized linear inversion of the radial-speed
equations yields a two-dimensional velocity vector, where speed and heading are
obtained. The approach is applied to 2D and 3D settings, extended to account
for time synchronization bias, and generalized to multiple targets by resolving
target association. The sequence of position-velocity estimates is then fed to
standard and extended Kalman filters to obtain smoothed tracks. Our results
show high-fidelity moving-target detection, positioning, and tracking using 5G
PRS signals for multistatic ISAC.

</details>


### [41] [On the Architectural Split and Radio Intelligence Controller Placement in Integrated O-RAN-enabled Non-Terrestrial Networks](https://arxiv.org/abs/2507.02680)
*Jorge Baranda,Marius Caus,Luis Blanco,Cristian J. Vaca-Rubio,Engin Zeydan,Kapal Dev,Zheng Li,Tomaso DeCola*

Main category: cs.NI

TL;DR: 本文探讨了基于O-RAN原则的地面网络（TN）与非地面网络（NTN）集成的架构和功能分割策略，分析了性能、延迟和部署的权衡，并提出了RIC的灵活放置方案。


<details>
  <summary>Details</summary>
Motivation: 解决TN与NTN集成中的异构传播条件、动态拓扑和有限处理能力带来的挑战。

Method: 提出分割选项分类，评估从纯星载DU部署到完整gNB和UPF集成的配置，并讨论RIC的放置策略。

Result: 提供了架构分割与RIC放置的全面映射，强调实现约束和互操作性。

Conclusion: 指出了标准化、模块化和高效TN-NTN集成的关键挑战和未来方向。

Abstract: The integration of Terrestrial Networks (TNs) with Non-Terrestrial Networks
(NTNs) poses unique architectural and functional challenges due to
heterogeneous propagation conditions, dynamic topologies and limited on-board
processing capabilities. This paper examines architectural and functional split
strategies that are consistent with O-RAN principles for future integrated
TN-NTN systems. A taxonomy of split options is proposed that distributes RAN
and core functions between satellites and ground nodes, and trade-offs in terms
of performance, latency, autonomy and deployment are analysed. In particular,
we evaluate configurations ranging from pure on-board DU deployments to full
gNB and UPF integration into satellites, including variations based on intra-
and inter-satellite processing. In addition, the placement of Near-RT and
Non-RT RAN Intelligent Controllers (RICs) is discussed, proposing flexible
split strategies between space and ground to optimise the performance and
scalability of the control loop. A comprehensive mapping between architectural
splits and RIC placement options is provided, emphasising implementation
constraints and interoperability considerations. The paper concludes by
identifying key challenges and outlining future directions to enable
standardised, modular and efficient TN-NTN convergence in the context of the
O-RAN.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [42] [Learnable-Differentiable Finite Volume Solver for Accelerated Simulation of Flows](https://arxiv.org/abs/2507.01975)
*Mengtao Yan,Qi Wang,Haining Wang,Ruizhi Chengze,Yi Zhang,Hongsheng Liu,Zidong Wang,Fan Yu,Qi Qi,Hao Sun*

Main category: cs.LG

TL;DR: 提出了一种可学习和可微分的有限体积求解器LDSolver，用于在粗网格上高效准确地模拟流体流动，解决了传统数值求解器和机器学习方法的问题。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器计算成本高，机器学习方法存在可解释性、泛化性和数据依赖性问题，需要一种更高效的解决方案。

Method: LDSolver包含可微分有限体积求解器和可学习模块，用于近似通量和时间误差校正。

Result: 在多种流动系统中，LDSolver表现优于基线模型，具有高精度和泛化能力。

Conclusion: LDSolver在粗网格上实现了高效准确的流体模拟，为相关领域提供了新工具。

Abstract: Simulation of fluid flows is crucial for modeling physical phenomena like
meteorology, aerodynamics, and biomedicine. Classical numerical solvers often
require fine spatiotemporal grids to satisfy stability, consistency, and
convergence conditions, leading to substantial computational costs. Although
machine learning has demonstrated better efficiency, they typically suffer from
issues of interpretability, generalizability, and data dependency. Hence, we
propose a learnable and differentiable finite volume solver, called LDSolver,
designed for efficient and accurate simulation of fluid flows on spatiotemporal
coarse grids. LDSolver comprises two key components: (1) a differentiable
finite volume solver, and (2) an learnable module providing equivalent
approximation for fluxes (derivatives and interpolations), and temporal error
correction on coarse grids. Even with limited training data (e.g., only a few
trajectories), our model could accelerate the simulation while maintaining a
high accuracy with superior generalizability. Experiments on different flow
systems (e.g., Burgers, decaying, forced and shear flows) show that LDSolver
achieves state-of-the-art performance, surpassing baseline models with notable
margins.

</details>


### [43] [DKGCM: A Spatio-Temporal Prediction Model for Traffic Flow by Fusing Spatial Node Clustering Method and Fourier Bidirectional Mamba Mechanism](https://arxiv.org/abs/2507.01982)
*Siqing Long,Xiangzhi Huang,Jiemin Xie,Ming Cai*

Main category: cs.LG

TL;DR: 提出了一种名为DKGCM的图卷积网络结构，通过动态时间规整和K均值聚类改进空间依赖性捕捉，并结合FFT和双向Mamba框架优化时间依赖性预测，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 交通需求预测的准确性对资源分配至关重要，但复杂的时空关系限制了现有模型的性能。

Method: 提出DK-GCN方法，结合DTW和K-means聚类分组交通节点，并集成FFT和双向Mamba框架捕捉时空依赖性，使用GRPO强化学习优化训练。

Result: 在三个公开数据集上表现优于其他先进方法。

Conclusion: DKGCM模型有效提升了交通需求预测的准确性，为交通管理提供了有力工具。

Abstract: Accurate traffic demand forecasting enables transportation management
departments to allocate resources more effectively, thereby improving their
utilization efficiency. However, complex spatiotemporal relationships in
traffic systems continue to limit the performance of demand forecasting models.
To improve the accuracy of spatiotemporal traffic demand prediction, we propose
a new graph convolutional network structure called DKGCM. Specifically, we
first consider the spatial flow distribution of different traffic nodes and
propose a novel temporal similarity-based clustering graph convolution method,
DK-GCN. This method utilizes Dynamic Time Warping (DTW) and K-means clustering
to group traffic nodes and more effectively capture spatial dependencies. On
the temporal scale, we integrate the Fast Fourier Transform (FFT) within the
bidirectional Mamba deep learning framework to capture temporal dependencies in
traffic demand. To further optimize model training, we incorporate the GRPO
reinforcement learning strategy to enhance the loss function feedback
mechanism. Extensive experiments demonstrate that our model outperforms several
advanced methods and achieves strong results on three public datasets.

</details>


### [44] [Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features](https://arxiv.org/abs/2507.01984)
*Gautam Kishore Shahi*

Main category: cs.LG

TL;DR: 研究探讨了多模态特征（文本、图像和社交特征）在社交媒体虚假信息检测中的有效性，发现结合无监督和监督机器学习模型能显著提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 社交媒体在选举和危机期间充斥虚假信息，现有研究多关注单模态（文本或图像），而多模态特征组合的研究较少。

Method: 采用早期融合方法，结合文本、图像和社交特征，对1,529条推文进行分析，并利用数据增强技术提取视觉和社交特征。

Result: 多模态模型比单模态模型性能提升15%，比双模态模型提升5%，同时分析了虚假信息的传播模式。

Conclusion: 多模态特征组合能更有效地检测虚假信息，并揭示其传播特征。

Abstract: Amid a tidal wave of misinformation flooding social media during elections
and crises, extensive research has been conducted on misinformation detection,
primarily focusing on text-based or image-based approaches. However, only a few
studies have explored multimodal feature combinations, such as integrating text
and images for building a classification model to detect misinformation. This
study investigates the effectiveness of different multimodal feature
combinations, incorporating text, images, and social features using an early
fusion approach for the classification model. This study analyzed 1,529 tweets
containing both text and images during the COVID-19 pandemic and election
periods collected from Twitter (now X). A data enrichment process was applied
to extract additional social features, as well as visual features, through
techniques such as object detection and optical character recognition (OCR).
The results show that combining unsupervised and supervised machine learning
models improves classification performance by 15% compared to unimodal models
and by 5% compared to bimodal models. Additionally, the study analyzes the
propagation patterns of misinformation based on the characteristics of
misinformation tweets and the users who disseminate them.

</details>


### [45] [Positive region preserved random sampling: an efficient feature selection method for massive data](https://arxiv.org/abs/2507.01998)
*Hexiang Bai,Deyu Li,Jiye Liang,Yanhui Zhai*

Main category: cs.LG

TL;DR: 提出了一种基于采样技术和粗糙集理论的新方法，用于处理大规模数据的特征选择问题。该方法通过构建正区域保留样本，快速找到具有高判别能力的特征子集。


<details>
  <summary>Details</summary>
Motivation: 智能机器在处理大规模数据时计算资源有限，需要高效的特征选择方法。

Method: 使用可区分对象对的比例衡量特征集的判别能力，并基于此提出新的特征选择方法。

Result: 在11个数据集上验证，方法能在短时间内找到近似约简，且判别能力高于预估下限。

Conclusion: 该方法在个人计算机上能高效处理大规模数据，具有实用性和高效性。

Abstract: Selecting relevant features is an important and necessary step for
intelligent machines to maximize their chances of success. However, intelligent
machines generally have no enough computing resources when faced with huge
volume of data. This paper develops a new method based on sampling techniques
and rough set theory to address the challenge of feature selection for massive
data. To this end, this paper proposes using the ratio of discernible object
pairs to all object pairs that should be distinguished to measure the
discriminatory ability of a feature set. Based on this measure, a new feature
selection method is proposed. This method constructs positive region preserved
samples from massive data to find a feature subset with high discriminatory
ability. Compared with other methods, the proposed method has two advantages.
First, it is able to select a feature subset that can preserve the
discriminatory ability of all the features of the target massive data set
within an acceptable time on a personal computer. Second, the lower boundary of
the probability of the object pairs that can be discerned using the feature
subset selected in all object pairs that should be distinguished can be
estimated before finding reducts. Furthermore, 11 data sets of different sizes
were used to validate the proposed method. The results show that approximate
reducts can be found in a very short period of time, and the discriminatory
ability of the final reduct is larger than the estimated lower boundary.
Experiments on four large-scale data sets also showed that an approximate
reduct with high discriminatory ability can be obtained in reasonable time on a
personal computer.

</details>


### [46] [Continuous Wavelet Transform and Siamese Network-Based Anomaly Detection in Multi-variate Semiconductor Process Time Series](https://arxiv.org/abs/2507.01999)
*Bappaditya Dey,Daniel Sorensen,Minjin Hwang,Sandip Halder*

Main category: cs.LG

TL;DR: 提出了一种基于机器学习的多变量时间序列异常检测方法，通过连续小波变换将数据转换为图像，并利用预训练的VGG-16架构和孪生网络进行高效分类。


<details>
  <summary>Details</summary>
Motivation: 半导体制造中多变量时间序列数据的高维性、类别不平衡、噪声和非平稳行为等问题，使得异常检测和根因分析极具挑战性。

Method: 1. 使用连续小波变换将时间序列数据转换为图像；2. 微调预训练的VGG-16架构用于多类图像分类；3. 构建孪生网络比较输入图像的嵌入向量以检测异常。

Result: 在真实半导体制造数据集上表现出高精度的异常检测能力，适用于离线和半监督场景。

Conclusion: 该方法为复杂制造环境中的异常检测提供了灵活且高效的解决方案。

Abstract: Semiconductor manufacturing is an extremely complex process, characterized by
thousands of interdependent parameters collected across diverse tools and
process steps. Multi-variate time-series (MTS) analysis has emerged as a
critical methodology for enabling real-time monitoring, fault detection, and
predictive maintenance in such environments. However, anomaly prediction in
semiconductor fabrication presents several critical challenges, including high
data dimensionality, severe class imbalance due to the rarity of true faults,
noisy and missing measurements, and non-stationary behavior of production
systems. Furthermore, the complex interdependencies between variables and the
delayed emergence of faults across downstream stages complicate both anomaly
detection and root-cause-analysis. This paper presents a novel and generic
approach for anomaly detection in MTS data using machine learning. The proposed
methodology consists of three main steps: a) converting MTS data into
image-based representations using the Continuous Wavelet Transform, b)
developing a multi-class image classifier by fine-tuning a pretrained VGG-16
architecture on custom CWT image datasets, and c) constructing a Siamese
network composed of two identical sub-networks, each utilizing the fine-tuned
VGG-16 as a backbone. The network takes pairs of CWT images as input -one
serving as a reference or anchor (representing a known-good signal), and the
other as a query (representing an unknown signal). The model then compares the
embeddings of both inputs to determine whether they belong to the same class at
a given time step. Our approach demonstrates high accuracy in identifying
anomalies on a real FAB process time-series dataset, offering a promising
solution for offline anomaly detection in process and tool trace data.
Moreover, the approach is flexible and can be applied in both supervised and
semi-supervised settings.

</details>


### [47] [Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames](https://arxiv.org/abs/2507.02001)
*Anurag Arnab,Ahmet Iscen,Mathilde Caron,Alireza Fathi,Cordelia Schmid*

Main category: cs.LG

TL;DR: 提出了一种名为Temporal Chain of Thought的推理策略，通过迭代选择视频中最相关的帧来提升长视频问答的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的视觉语言模型（VLMs）能处理约1000帧输入，但在长视频理解中仍难以有效利用上下文，且易受无关内容干扰。

Method: 使用VLM自身迭代识别并提取视频中最相关的帧，作为输入上下文用于问答。

Result: 在4个不同的视频问答数据集上取得最佳性能，尤其在长视频（超过1小时）上表现突出，优于传统推理方法。

Conclusion: 通过推理时计算优化上下文选择，显著提升了长视频问答的准确性，验证了推理时扩展LLMs的有效性。

Abstract: Despite recent advances in Vision-Language Models (VLMs), long-video
understanding remains a challenging problem. Although state-of-the-art
long-context VLMs can process around 1000 input frames, they still struggle to
effectively leverage this sequence length, and succumb to irrelevant
distractors within the context window. We present Temporal Chain of Thought, an
inference strategy for video question-answering that curates the model's input
context. We use the VLM itself to iteratively identify and extract the most
relevant frames from the video, which are then used for answering. We
demonstrate how leveraging more computation at inference-time to select the
most relevant context leads to improvements in accuracy, in agreement with
recent work on inference-time scaling of LLMs. Moreover, we achieve
state-of-the-art results on 4 diverse video question-answering datasets,
showing consistent improvements with 3 different VLMs. In particular, our
method shines on longer videos which would not otherwise fit within the model's
context window: On longer videos of more than 1 hour on LVBench, our approach
using a context window of 32K outperforms the same VLM using standard inference
with a 700K context window by 2.8 points.

</details>


### [48] [AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design](https://arxiv.org/abs/2507.02006)
*Shakya Jayakody,Youpeng Zhao,Jun Wang*

Main category: cs.LG

TL;DR: AIRES是一种算法-系统协同设计解决方案，旨在加速GCN中的外核SpGEMM计算，通过块级数据对齐和三阶段动态调度显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: 随着图数据规模的扩大，外核SpGEMM面临高I/O延迟和GPU利用率不足的问题，现有系统在稀疏格式数据对齐和内存分配上存在性能瓶颈。

Method: AIRES提出块级稀疏格式数据对齐和分块算法，并采用三阶段动态调度策略，结合GPU内存、GDS和主机内存以减少I/O延迟。

Result: AIRES在真实图处理基准测试中表现优异，延迟降低高达1.8倍。

Conclusion: AIRES通过算法和系统协同设计，有效解决了外核SpGEMM的性能瓶颈，显著提升了GCN的计算效率。

Abstract: Graph convolutional networks (GCNs) are fundamental in various scientific
applications, ranging from biomedical protein-protein interactions (PPI) to
large-scale recommendation systems. An essential component for modeling graph
structures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As
the size of graph data continues to scale up, SpGEMMs are often conducted in an
out-of-core fashion due to limited GPU memory space in resource-constrained
systems. Albeit recent efforts that aim to alleviate the memory constraints of
out-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory
layout, or performing the computation in sparse format, current systems suffer
from both high I/O latency and GPU under-utilization issues.
  In this paper, we first identify the problems of existing systems, where
sparse format data alignment and memory allocation are the main performance
bottlenecks, and propose AIRES, a novel algorithm-system co-design solution to
accelerate out-of-core SpGEMM computation for GCNs. Specifically, from the
algorithm angle, AIRES proposes to alleviate the data alignment issues on the
block level for matrices in sparse formats and develops a tiling algorithm to
facilitate row block-wise alignment. On the system level, AIRES employs a
three-phase dynamic scheduling that features a dual-way data transfer strategy
utilizing a tiered memory system: integrating GPU memory, GPU Direct Storage
(GDS), and host memory to reduce I/O latency and improve throughput.
Evaluations show that AIRES significantly outperforms the state-of-the-art
methods, achieving up to 1.8x lower latency in real-world graph processing
benchmarks.

</details>


### [49] [GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters](https://arxiv.org/abs/2507.02085)
*Wanjia Zhao,Jiaqi Han,Siyi Gu,Mingjian Jiang,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: GeoAda是一种SE(3)-等变适配器框架，用于高效微调几何扩散模型，保持几何一致性并避免过拟合。


<details>
  <summary>Details</summary>
Motivation: 几何扩散模型在下游任务中的高效微调尚未充分探索，需要一种灵活且参数高效的方法。

Method: GeoAda通过结构化适配器设计（耦合操作、可训练层副本、解耦操作和零初始化卷积）实现微调。

Result: GeoAda在多种几何控制任务中表现优异，保持原始任务准确性，优于其他基线方法。

Conclusion: GeoAda为几何扩散模型提供了一种高效、灵活的微调方法，适用于广泛的应用领域。

Abstract: Geometric diffusion models have shown remarkable success in molecular
dynamics and structure generation. However, efficiently fine-tuning them for
downstream tasks with varying geometric controls remains underexplored. In this
work, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables
flexible and parameter-efficient fine-tuning for controlled generative tasks
without modifying the original model architecture. GeoAda introduces a
structured adapter design: control signals are first encoded through coupling
operators, then processed by a trainable copy of selected pretrained model
layers, and finally projected back via decoupling operators followed by an
equivariant zero-initialized convolution. By fine-tuning only these lightweight
adapter modules, GeoAda preserves the model's geometric consistency while
mitigating overfitting and catastrophic forgetting. We theoretically prove that
the proposed adapters maintain SE(3)-equivariance, ensuring that the geometric
inductive biases of the pretrained diffusion model remain intact during
adaptation. We demonstrate the wide applicability of GeoAda across diverse
geometric control types, including frame control, global control, subgraph
control, and a broad range of application domains such as particle dynamics,
molecular dynamics, human motion prediction, and molecule generation. Empirical
results show that GeoAda achieves state-of-the-art fine-tuning performance
while preserving original task accuracy, whereas other baselines experience
significant performance degradation due to overfitting and catastrophic
forgetting.

</details>


### [50] [Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions](https://arxiv.org/abs/2507.02087)
*Eitan Anzenberg,Arunava Samajpati,Sivasankaran Chandrasekar,Varun Kacholia*

Main category: cs.LG

TL;DR: 研究表明，专有领域模型Match Score在招聘中比通用LLMs更准确且公平。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在招聘中的潜在偏见问题，并验证专有模型是否能同时实现高准确性和公平性。

Method: 比较多种LLMs与专有模型Match Score在10,000个真实招聘数据上的表现，评估准确性和公平性指标。

Result: Match Score在ROC AUC（0.85 vs 0.77）和公平性（种族影响比0.957 vs 0.809）上优于LLMs。

Conclusion: 专有模型能有效减少偏见，强调在高风险领域需结合领域特定建模和偏见审核。

Abstract: The use of large language models (LLMs) in hiring promises to streamline
candidate screening, but it also raises serious concerns regarding accuracy and
algorithmic bias where sufficient safeguards are not in place. In this work, we
benchmark several state-of-the-art foundational LLMs - including models from
OpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our
proprietary domain-specific hiring model (Match Score) for job candidate
matching. We evaluate each model's predictive accuracy (ROC AUC,
Precision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis
across declared gender, race, and intersectional subgroups). Our experiments on
a dataset of roughly 10,000 real-world recent candidate-job pairs show that
Match Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs
0.77) and achieves significantly more equitable outcomes across demographic
groups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957
(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the
intersectionals, respectively). We discuss why pretraining biases may cause
LLMs with insufficient safeguards to propagate societal biases in hiring
scenarios, whereas a bespoke supervised model can more effectively mitigate
these biases. Our findings highlight the importance of domain-specific modeling
and bias auditing when deploying AI in high-stakes domains such as hiring, and
caution against relying on off-the-shelf LLMs for such tasks without extensive
fairness safeguards. Furthermore, we show with empirical evidence that there
shouldn't be a dichotomy between choosing accuracy and fairness in hiring: a
well-designed algorithm can achieve both accuracy in hiring and fairness in
outcomes.

</details>


### [51] [Sample Complexity Bounds for Linear Constrained MDPs with a Generative Model](https://arxiv.org/abs/2507.02089)
*Xingtu Liu,Lin F. Yang,Sharan Vaswani*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider infinite-horizon $\gamma$-discounted (linear) constrained Markov
decision processes (CMDPs) where the objective is to find a policy that
maximizes the expected cumulative reward subject to expected cumulative
constraints. Given access to a generative model, we propose to solve CMDPs with
a primal-dual framework that can leverage any black-box unconstrained MDP
solver. For linear CMDPs with feature dimension $d$, we instantiate the
framework by using mirror descent value iteration
(\texttt{MDVI})~\citep{kitamura2023regularization} an example MDP solver. We
provide sample complexity bounds for the resulting CMDP algorithm in two cases:
(i) relaxed feasibility, where small constraint violations are allowed, and
(ii) strict feasibility, where the output policy is required to exactly satisfy
the constraint. For (i), we prove that the algorithm can return an
$\epsilon$-optimal policy with high probability by using
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^4\epsilon^2}\right)$ samples. We note
that these results exhibit a near-optimal dependence on both $d$ and
$\epsilon$. For (ii), we show that the algorithm requires
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^6\epsilon^2\zeta^2}\right)$ samples,
where $\zeta$ is the problem-dependent Slater constant that characterizes the
size of the feasible region. Finally, we instantiate our framework for tabular
CMDPs and show that it can be used to recover near-optimal sample complexities
in this setting.

</details>


### [52] [Online Conformal Prediction with Efficiency Guarantees](https://arxiv.org/abs/2507.02496)
*Vaidehi Srinivas*

Main category: cs.LG

TL;DR: 研究在线框架下的保形预测问题，直接优化效率，比较交换序列和任意序列的表现差异。


<details>
  <summary>Details</summary>
Motivation: 探索在线保形预测问题，目标是实现覆盖率和效率的平衡，特别是在交换序列和任意序列下的不同表现。

Method: 提出算法，针对交换序列和任意序列分别设计策略，分析覆盖率和区间长度的最优性。

Result: 交换序列下可实现接近最优覆盖率和长度；任意序列下需权衡长度和错误率。

Conclusion: 交换序列和任意序列表现差异显著，提出算法可接近最优权衡。

Abstract: We study the problem of conformal prediction in a novel online framework that
directly optimizes efficiency. In our problem, we are given a target
miscoverage rate $\alpha > 0$, and a time horizon $T$. On each day $t \le T$ an
algorithm must output an interval $I_t \subseteq [0, 1]$, then a point $y_t \in
[0, 1]$ is revealed. The goal of the algorithm is to achieve coverage, that is,
$y_t \in I_t$ on (close to) a $(1 - \alpha)$-fraction of days, while
maintaining efficiency, that is, minimizing the average volume (length) of the
intervals played. This problem is an online analogue to the problem of
constructing efficient confidence intervals.
  We study this problem over arbitrary and exchangeable (random order) input
sequences. For exchangeable sequences, we show that it is possible to construct
intervals that achieve coverage $(1 - \alpha) - o(1)$, while having length
upper bounded by the best fixed interval that achieves coverage in hindsight.
For arbitrary sequences however, we show that any algorithm that achieves a
$\mu$-approximation in average length compared to the best fixed interval
achieving coverage in hindsight, must make a multiplicative factor more
mistakes than $\alpha T$, where the multiplicative factor depends on $\mu$ and
the aspect ratio of the problem. Our main algorithmic result is a matching
algorithm that can recover all Pareto-optimal settings of $\mu$ and number of
mistakes. Furthermore, our algorithm is deterministic and therefore robust to
an adaptive adversary.
  This gap between the exchangeable and arbitrary settings is in contrast to
the classical online learning problem. In fact, we show that no single
algorithm can simultaneously be Pareto-optimal for arbitrary sequences and
optimal for exchangeable sequences. On the algorithmic side, we give an
algorithm that achieves the near-optimal tradeoff between the two cases.

</details>


### [53] [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092)
*Alexi Gladstone,Ganesh Nanduru,Md Mofijul Islam,Peixuan Han,Hyeonjeong Ha,Aman Chadha,Yilun Du,Heng Ji,Jundong Li,Tariq Iqbal*

Main category: cs.LG

TL;DR: 论文提出了一种新型的Energy-Based Transformers（EBTs），通过无监督学习实现类似人类System 2 Thinking的推理能力，并在多模态任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有推理时计算方法存在模态或问题特定性，且需要额外监督训练。本文探索是否可以通过无监督学习实现通用推理能力。

Method: 训练EBTs模型，通过能量最小化验证输入与候选预测的兼容性，实现推理优化。

Result: EBTs在训练和推理中表现优于Transformer++和Diffusion Transformers，数据效率更高，性能提升显著。

Conclusion: EBTs是一种有前景的新范式，可扩展模型的学习和推理能力。

Abstract: Inference-time computation techniques, analogous to human System 2 Thinking,
have recently become popular for improving model performances. However, most
existing approaches suffer from several limitations: they are modality-specific
(e.g., working only in text), problem-specific (e.g., verifiable domains like
math and coding), or require additional supervision/training on top of
unsupervised pretraining (e.g., verifiers or verifiable rewards). In this
paper, we ask the question "Is it possible to generalize these System 2
Thinking approaches, and develop models that learn to think solely from
unsupervised learning?" Interestingly, we find the answer is yes, by learning
to explicitly verify the compatibility between inputs and
candidate-predictions, and then re-framing prediction problems as optimization
with respect to this verifier. Specifically, we train Energy-Based Transformers
(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy
value to every input and candidate-prediction pair, enabling predictions
through gradient descent-based energy minimization until convergence. Across
both discrete (text) and continuous (visual) modalities, we find EBTs scale
faster than the dominant Transformer++ approach during training, achieving an
up to 35% higher scaling rate with respect to data, batch size, parameters,
FLOPs, and depth. During inference, EBTs improve performance with System 2
Thinking by 29% more than the Transformer++ on language tasks, and EBTs
outperform Diffusion Transformers on image denoising while using fewer forward
passes. Further, we find that EBTs achieve better results than existing models
on most downstream tasks given the same or worse pretraining performance,
suggesting that EBTs generalize better than existing approaches. Consequently,
EBTs are a promising new paradigm for scaling both the learning and thinking
capabilities of models.

</details>


### [54] [Parametric Neural Amp Modeling with Active Learning](https://arxiv.org/abs/2507.02109)
*Florian Grötschla,Luca A. Lanzendörfer,Longxiang Jiao,Roger Wattenhofer*

Main category: cs.LG

TL;DR: PANAMA是一个基于WaveNet架构的主动学习框架，用于训练端到端参数化吉他放大器模型。


<details>
  <summary>Details</summary>
Motivation: 通过主动学习策略最小化数据点（如放大器旋钮设置）的使用量，创建虚拟放大器。

Method: 使用基于梯度的优化算法确定最佳采样数据点。

Result: 在有限样本数量下，该方法有效。

Conclusion: PANAMA框架能够高效训练吉他放大器模型，减少所需数据量。

Abstract: We introduce PANAMA, an active learning framework for the training of
end-to-end parametric guitar amp models using a WaveNet-like architecture. With
\model, one can create a virtual amp by recording samples that are determined
by an active learning strategy to use a minimum amount of datapoints (i.e., amp
knob settings). We show that gradient-based optimization algorithms can be used
to determine the optimal datapoints to sample, and that the approach helps
under a constrained number of samples.

</details>


### [55] [Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks](https://arxiv.org/abs/2507.02119)
*Shikai Qiu,Lechao Xiao,Andrew Gordon Wilson,Jeffrey Pennington,Atish Agarwala*

Main category: cs.LG

TL;DR: 研究发现，在模型规模和训练时间同步增长时，神经网络训练动态遵循一种精确的普适性规律，称为“超坍缩”。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络训练动态在模型规模和训练时间增长时的普适性规律。

Method: 通过归一化训练计算和损失，分析不同模型、数据集和架构的损失曲线，并结合SGD噪声动态模型进行解释。

Result: 发现损失曲线在归一化后能精确坍缩为一条普适曲线（超坍缩），且这种现象在超参数优化时尤为明显。

Conclusion: 超坍缩现象为神经网络训练的优化提供了精确且实用的指标，并揭示了其与典型神经缩放定律的幂律结构之间的联系。

Abstract: What scaling limits govern neural network training dynamics when model size
and training time grow in tandem? We show that despite the complex interactions
between architecture, training algorithms, and data, compute-optimally trained
models exhibit a remarkably precise universality. Specifically, loss curves
from models of varying sizes collapse onto a single universal curve when
training compute and loss are normalized to unity at the end of training. With
learning rate decay, the collapse becomes so tight that differences in the
normalized curves across models fall below the noise floor of individual loss
curves across random seeds, a phenomenon we term supercollapse. We observe
supercollapse across learning rate schedules, datasets, and architectures,
including transformers trained on next-token prediction, and find it breaks
down when hyperparameters are scaled suboptimally, providing a precise and
practical indicator of good scaling. We explain these phenomena by connecting
collapse to the power-law structure in typical neural scaling laws, and
analyzing a simple yet surprisingly effective model of SGD noise dynamics that
accurately predicts loss curves across various learning rate schedules and
quantitatively explains the origin of supercollapse.

</details>


### [56] [CROP: Circuit Retrieval and Optimization with Parameter Guidance using LLMs](https://arxiv.org/abs/2507.02128)
*Jingyu Pan,Isaac Jacobson,Zheng Zhao,Tung-Chieh Chen,Guanglei Zhou,Chen-Chia Chang,Vineet Rashingkar,Yiran Chen*

Main category: cs.LG

TL;DR: CROP是一个基于大型语言模型（LLM）的自动VLSI设计流程调优框架，通过向量化RTL代码、相似电路匹配和检索增强生成技术，显著提升设计优化效率。


<details>
  <summary>Details</summary>
Motivation: 现代VLSI设计复杂度高，参数空间庞大，手动调优效率低且依赖专家经验，亟需自动化解决方案。

Method: CROP包括RTL代码向量化、相似电路匹配系统和检索增强的LLM参数搜索系统。

Result: 实验表明，CROP在工业设计中以更少迭代次数实现更优结果，如功耗降低9.9%。

Conclusion: CROP为VLSI设计自动化提供了高效解决方案，显著提升优化效果。

Abstract: Modern very large-scale integration (VLSI) design requires the implementation
of integrated circuits using electronic design automation (EDA) tools. Due to
the complexity of EDA algorithms, the vast parameter space poses a huge
challenge to chip design optimization, as the combination of even moderate
numbers of parameters creates an enormous solution space to explore. Manual
parameter selection remains industrial practice despite being excessively
laborious and limited by expert experience. To address this issue, we present
CROP, the first large language model (LLM)-powered automatic VLSI design flow
tuning framework. Our approach includes: (1) a scalable methodology for
transforming RTL source code into dense vector representations, (2) an
embedding-based retrieval system for matching designs with semantically similar
circuits, and (3) a retrieval-augmented generation (RAG)-enhanced LLM-guided
parameter search system that constrains the search process with prior knowledge
from similar designs. Experiment results demonstrate CROP's ability to achieve
superior quality-of-results (QoR) with fewer iterations than existing
approaches on industrial designs, including a 9.9% reduction in power
consumption.

</details>


### [57] [Generative Latent Diffusion for Efficient Spatiotemporal Data Reduction](https://arxiv.org/abs/2507.02129)
*Xiao Li,Liangji Zhu,Anand Rangarajan,Sanjay Ranka*

Main category: cs.LG

TL;DR: 提出了一种结合变分自编码器和条件扩散模型的高效潜在扩散框架，显著提升了数据压缩的性能。


<details>
  <summary>Details</summary>
Motivation: 生成模型在条件设置下表现优异，但可控性和重建精度限制了其在数据压缩中的实际应用。

Method: 通过变分自编码器和条件扩散模型结合，仅压缩少量关键帧到潜在空间，并通过生成插值重建其余帧。

Result: 实验表明，该方法压缩比优于SZ3等规则压缩方法10倍，比学习型方法性能提升63%。

Conclusion: 该方法在降低存储成本的同时实现了高精度的时空重建。

Abstract: Generative models have demonstrated strong performance in conditional
settings and can be viewed as a form of data compression, where the condition
serves as a compact representation. However, their limited controllability and
reconstruction accuracy restrict their practical application to data
compression. In this work, we propose an efficient latent diffusion framework
that bridges this gap by combining a variational autoencoder with a conditional
diffusion model. Our method compresses only a small number of keyframes into
latent space and uses them as conditioning inputs to reconstruct the remaining
frames via generative interpolation, eliminating the need to store latent
representations for every frame. This approach enables accurate spatiotemporal
reconstruction while significantly reducing storage costs. Experimental results
across multiple datasets show that our method achieves up to 10 times higher
compression ratios than rule-based state-of-the-art compressors such as SZ3,
and up to 63 percent better performance than leading learning-based methods
under the same reconstruction error.

</details>


### [58] [Non-exchangeable Conformal Prediction for Temporal Graph Neural Networks](https://arxiv.org/abs/2507.02151)
*Tuo Wang,Jian Kang,Yujun Yan,Adithya Kulkarni,Dawei Zhou*

Main category: cs.LG

TL;DR: NCPNET是一种新颖的端到端共形预测框架，专为时态图设计，解决了现有方法在动态图上的局限性，提升了预测效率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有共形预测方法主要针对静态图，忽略了真实世界图的动态性，导致时态依赖性问题。

Method: 提出基于扩散的非共形性评分，捕捉拓扑和时态不确定性，并开发效率感知优化算法。

Result: 在多个真实时态图数据集上验证，NCPNET显著减少预测集大小（如WIKI数据集减少31%），提升效率。

Conclusion: NCPNET成功扩展了共形预测到时态图，解决了覆盖率问题，提升了计算效率。

Abstract: Conformal prediction for graph neural networks (GNNs) offers a promising
framework for quantifying uncertainty, enhancing GNN reliability in high-stakes
applications. However, existing methods predominantly focus on static graphs,
neglecting the evolving nature of real-world graphs. Temporal dependencies in
graph structure, node attributes, and ground truth labels violate the
fundamental exchangeability assumption of standard conformal prediction
methods, limiting their applicability. To address these challenges, in this
paper, we introduce NCPNET, a novel end-to-end conformal prediction framework
tailored for temporal graphs. Our approach extends conformal prediction to
dynamic settings, mitigating statistical coverage violations induced by
temporal dependencies. To achieve this, we propose a diffusion-based
non-conformity score that captures both topological and temporal uncertainties
within evolving networks. Additionally, we develop an efficiency-aware
optimization algorithm that improves the conformal prediction process,
enhancing computational efficiency and reducing coverage violations. Extensive
experiments on diverse real-world temporal graphs, including WIKI, REDDIT,
DBLP, and IBM Anti-Money Laundering dataset, demonstrate NCPNET's capability to
ensure guaranteed coverage in temporal graphs, achieving up to a 31% reduction
in prediction set size on the WIKI dataset, significantly improving efficiency
compared to state-of-the-art methods. Our data and code are available at
https://github.com/ODYSSEYWT/NCPNET.

</details>


### [59] [Statistical Inference for Responsiveness Verification](https://arxiv.org/abs/2507.02169)
*Seung Hyun Cheon,Meredith Stewart,Bogdan Kulynych,Tsui-Wei Weng,Berk Ustun*

Main category: cs.LG

TL;DR: 论文提出了一种验证机器学习模型预测响应性的方法，通过敏感性分析评估个体特征变化对预测的影响，支持黑盒模型的应用。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在贷款、招聘等内容审核场景中常因忽略个体输入变化而导致安全失败，需验证预测的响应性。

Method: 提出基于敏感性分析的验证框架，通过约束干预和下游效应分布估计预测响应性，适用于任何模型和数据集。

Result: 开发了算法生成可达点均匀样本，应用于累犯预测、器官移植优先级和内容审核等实际场景。

Conclusion: 该方法能有效提升机器学习模型在实际应用中的安全性。

Abstract: Many safety failures in machine learning arise when models are used to assign
predictions to people (often in settings like lending, hiring, or content
moderation) without accounting for how individuals can change their inputs. In
this work, we introduce a formal validation procedure for the responsiveness of
predictions with respect to interventions on their features. Our procedure
frames responsiveness as a type of sensitivity analysis in which practitioners
control a set of changes by specifying constraints over interventions and
distributions over downstream effects. We describe how to estimate
responsiveness for the predictions of any model and any dataset using only
black-box access, and how to use these estimates to support tasks such as
falsification and failure probability estimation. We develop algorithms that
construct these estimates by generating a uniform sample of reachable points,
and demonstrate how they can promote safety in real-world applications such as
recidivism prediction, organ transplant prioritization, and content moderation.

</details>


### [60] [Metric Design != Metric Behavior: Improving Metric Selection for the Unbiased Evaluation of Dimensionality Reduction](https://arxiv.org/abs/2507.02225)
*Jiyeon Bae,Hyeon Jeon,Jinwook Seo*

Main category: cs.LG

TL;DR: 提出了一种通过聚类评估指标以减少维度降维（DR）评估偏见的流程。


<details>
  <summary>Details</summary>
Motivation: 解决维度降维评估中因选择高度相关指标导致的偏见问题。

Method: 基于指标间的经验相关性聚类，减少重叠，并从每个聚类中选择代表性指标。

Result: 实验表明该方法提高了DR评估的稳定性，减少了评估偏见。

Conclusion: 提出的流程有效减少了维度降维评估中的偏见。

Abstract: Evaluating the accuracy of dimensionality reduction (DR) projections in
preserving the structure of high-dimensional data is crucial for reliable
visual analytics. Diverse evaluation metrics targeting different structural
characteristics have thus been developed. However, evaluations of DR
projections can become biased if highly correlated metrics--those measuring
similar structural characteristics--are inadvertently selected, favoring DR
techniques that emphasize those characteristics. To address this issue, we
propose a novel workflow that reduces bias in the selection of evaluation
metrics by clustering metrics based on their empirical correlations rather than
on their intended design characteristics alone. Our workflow works by computing
metric similarity using pairwise correlations, clustering metrics to minimize
overlap, and selecting a representative metric from each cluster. Quantitative
experiments demonstrate that our approach improves the stability of DR
evaluation, which indicates that our workflow contributes to mitigating
evaluation bias.

</details>


### [61] [PhysicsCorrect: A Training-Free Approach for Stable Neural PDE Simulations](https://arxiv.org/abs/2507.02227)
*Xinquan Huang,Paris Perdikaris*

Main category: cs.LG

TL;DR: PhysicsCorrect是一种无需训练的校正框架，通过基于PDE残差的线性化逆问题在每个预测步骤中强制PDE一致性，显著减少神经网络在长期预测中的误差累积。


<details>
  <summary>Details</summary>
Motivation: 神经网络作为PDE求解的替代方法具有计算速度优势，但长期预测中误差累积导致结果偏离物理有效解。

Method: 提出PhysicsCorrect框架，利用离线预计算Jacobian及其伪逆的高效缓存策略，降低计算开销。

Result: 在三种典型PDE系统中，PhysicsCorrect将预测误差降低高达100倍，且推理时间增加可忽略不计（低于5%）。

Conclusion: PhysicsCorrect将不稳定的神经代理转化为可靠的模拟工具，平衡了深度学习的高效计算与实际科学应用对物理保真度的需求。

Abstract: Neural networks have emerged as powerful surrogates for solving partial
differential equations (PDEs), offering significant computational speedups over
traditional methods. However, these models suffer from a critical limitation:
error accumulation during long-term rollouts, where small inaccuracies compound
exponentially, eventually causing complete divergence from physically valid
solutions. We present PhysicsCorrect, a training-free correction framework that
enforces PDE consistency at each prediction step by formulating correction as a
linearized inverse problem based on PDE residuals. Our key innovation is an
efficient caching strategy that precomputes the Jacobian and its pseudoinverse
during an offline warm-up phase, reducing computational overhead by two orders
of magnitude compared to standard correction approaches. Across three
representative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and
the chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction
errors by up to 100x while adding negligible inference time (under 5\%). The
framework integrates seamlessly with diverse architectures including Fourier
Neural Operators, UNets, and Vision Transformers, effectively transforming
unstable neural surrogates into reliable simulation tools that bridge the gap
between deep learning's computational efficiency and the physical fidelity
demanded by practical scientific applications.

</details>


### [62] [VERBA: Verbalizing Model Differences Using Large Language Models](https://arxiv.org/abs/2507.02241)
*Shravan Doda,Shashidhar Reddy Javaji,Zining Zhu*

Main category: cs.LG

TL;DR: 论文提出了VERBA方法，利用大语言模型（LLM）生成模型间的差异描述，解决了模型选择中大量成对比较的难题。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习中存在大量性能相似但行为不同的模型，手动比较和文档化这些模型的差异成本过高。

Method: VERBA通过LLM采样生成模型差异的文本描述，并通过模拟评估其信息量。

Result: VERBA在决策树模型上实现了80%的准确率，结合结构信息后提升至90%。

Conclusion: VERBA为提升机器学习模型的透明度和可比性提供了新思路。

Abstract: In the current machine learning landscape, we face a "model lake" phenomenon:
Given a task, there is a proliferation of trained models with similar
performances despite different behavior. For model users attempting to navigate
and select from the models, documentation comparing model pairs is helpful.
However, for every $N$ models there could be $O(N^2)$ pairwise comparisons, a
number prohibitive for the model developers to manually perform pairwise
comparisons and prepare documentations. To facilitate fine-grained pairwise
comparisons among models, we introduced $\textbf{VERBA}$. Our approach
leverages a large language model (LLM) to generate verbalizations of model
differences by sampling from the two models. We established a protocol that
evaluates the informativeness of the verbalizations via simulation. We also
assembled a suite with a diverse set of commonly used machine learning models
as a benchmark. For a pair of decision tree models with up to 5% performance
difference but 20-25% behavioral differences, $\textbf{VERBA}$ effectively
verbalizes their variations with up to 80% overall accuracy. When we included
the models' structural information, the verbalization's accuracy further
improved to 90%. $\textbf{VERBA}$ opens up new research avenues for improving
the transparency and comparability of machine learning models in a post-hoc
manner.

</details>


### [63] [Order Acquisition Under Competitive Pressure: A Rapidly Adaptive Reinforcement Learning Approach for Ride-Hailing Subsidy Strategies](https://arxiv.org/abs/2507.02244)
*Fangzhou Shi,Xiaopeng Ke,Xinye Xiong,Kexin Meng,Chang Men,Zhengdan Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种基于强化学习的补贴策略框架FCA-RL，用于动态适应竞争对手的价格调整，优化订单获取并满足预算约束。


<details>
  <summary>Details</summary>
Motivation: 网约车聚合平台的竞争排名机制促使服务提供商通过降价策略增加订单量，但现有研究缺乏动态适应市场波动的有效方法。

Method: 结合快速竞争适应（FCA）和强化拉格朗日调整（RLA）技术，并开发了专用模拟环境RideGym。

Result: 实验表明，FCA-RL在多种市场条件下均优于基线方法。

Conclusion: FCA-RL框架能有效优化网约车服务提供商的补贴策略，适应动态竞争环境。

Abstract: The proliferation of ride-hailing aggregator platforms presents significant
growth opportunities for ride-service providers by increasing order volume and
gross merchandise value (GMV). On most ride-hailing aggregator platforms,
service providers that offer lower fares are ranked higher in listings and,
consequently, are more likely to be selected by passengers. This competitive
ranking mechanism creates a strong incentive for service providers to adopt
coupon strategies that lower prices to secure a greater number of orders, as
order volume directly influences their long-term viability and sustainability.
Thus, designing an effective coupon strategy that can dynamically adapt to
market fluctuations while optimizing order acquisition under budget constraints
is a critical research challenge. However, existing studies in this area remain
scarce.
  To bridge this gap, we propose FCA-RL, a novel reinforcement learning-based
subsidy strategy framework designed to rapidly adapt to competitors' pricing
adjustments. Our approach integrates two key techniques: Fast Competition
Adaptation (FCA), which enables swift responses to dynamic price changes, and
Reinforced Lagrangian Adjustment (RLA), which ensures adherence to budget
constraints while optimizing coupon decisions on new price landscape.
Furthermore, we introduce RideGym, the first dedicated simulation environment
tailored for ride-hailing aggregators, facilitating comprehensive evaluation
and benchmarking of different pricing strategies without compromising
real-world operational efficiency. Experimental results demonstrate that our
proposed method consistently outperforms baseline approaches across diverse
market conditions, highlighting its effectiveness in subsidy optimization for
ride-hailing service providers.

</details>


### [64] [Uncertainty-aware Reward Design Process](https://arxiv.org/abs/2507.02256)
*Yang Yang,Xiaolu Zhou,Bosong Ding,Miao Xin*

Main category: cs.LG

TL;DR: URDP框架结合LLMs和贝叶斯优化，提升RL奖励函数设计的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 传统奖励函数设计效率低且不一致，LLMs在数值优化中表现不佳，进化搜索资源利用低效。

Method: 提出URDP框架，通过自一致性分析量化奖励函数不确定性，引入UABO优化超参数配置，并采用双层优化架构。

Result: 在35个任务中，URDP生成更高质量的奖励函数，显著提升设计效率。

Conclusion: URDP通过结合LLMs的逻辑推理和贝叶斯优化的数值优化能力，实现了更高效的奖励函数设计。

Abstract: Designing effective reward functions is a cornerstone of reinforcement
learning (RL), yet it remains a challenging process due to the inefficiencies
and inconsistencies inherent in conventional reward engineering methodologies.
Recent advances have explored leveraging large language models (LLMs) to
automate reward function design. However, their suboptimal performance in
numerical optimization often yields unsatisfactory reward quality, while the
evolutionary search paradigm demonstrates inefficient utilization of simulation
resources, resulting in prohibitively lengthy design cycles with
disproportionate computational overhead. To address these challenges, we
propose the Uncertainty-aware Reward Design Process (URDP), a novel framework
that integrates large language models to streamline reward function design and
evaluation in RL environments. URDP quantifies candidate reward function
uncertainty based on self-consistency analysis, enabling simulation-free
identification of ineffective reward components while discovering novel reward
components. Furthermore, we introduce uncertainty-aware Bayesian optimization
(UABO), which incorporates uncertainty estimation to significantly enhance
hyperparameter configuration efficiency. Finally, we construct a bi-level
optimization architecture by decoupling the reward component optimization and
the hyperparameter tuning. URDP orchestrates synergistic collaboration between
the reward logic reasoning of the LLMs and the numerical optimization strengths
of the Bayesian Optimization. We conduct a comprehensive evaluation of URDP
across 35 diverse tasks spanning three benchmark environments. Our experimental
results demonstrate that URDP not only generates higher-quality reward
functions but also achieves significant improvements in the efficiency of
automated reward design compared to existing approaches.

</details>


### [65] [Knowledge Graph-Based Explainable and Generalized Zero-Shot Semantic Communications](https://arxiv.org/abs/2507.02291)
*Zhaoyu Zhang,Lingyi Wang,Wei Wu,Fuhui Zhou,Qihui Wu*

Main category: cs.LG

TL;DR: 提出了一种基于知识图谱的零样本语义通信网络（KGZS-SC），通过结构化语义信息提升泛化能力和推理能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动的语义通信缺乏可解释性和泛化能力，尤其在处理未见数据时表现不佳。

Method: 利用知识图谱语义知识库（KG-SKB）对齐语义特征，结合零样本学习（ZSL）实现未见数据的直接分类。

Result: 在APY数据集上的实验表明，KGZS-SC网络在分类未见类别时表现优异，且在不同信噪比下均具鲁棒性。

Conclusion: KGZS-SC网络通过知识图谱和零样本学习显著提升了语义通信的泛化能力和效率。

Abstract: Data-driven semantic communication is based on superficial statistical
patterns, thereby lacking interpretability and generalization, especially for
applications with the presence of unseen data. To address these challenges, we
propose a novel knowledge graph-enhanced zero-shot semantic communication
(KGZS-SC) network. Guided by the structured semantic information from a
knowledge graph-based semantic knowledge base (KG-SKB), our scheme provides
generalized semantic representations and enables reasoning for unseen cases.
Specifically, the KG-SKB aligns the semantic features in a shared category
semantics embedding space and enhances the generalization ability of the
transmitter through aligned semantic features, thus reducing communication
overhead by selectively transmitting compact visual semantics. At the receiver,
zero-shot learning (ZSL) is leveraged to enable direct classification for
unseen cases without the demand for retraining or additional computational
overhead, thereby enhancing the adaptability and efficiency of the
classification process in dynamic or resource-constrained environments. The
simulation results conducted on the APY datasets show that the proposed KGZS-SC
network exhibits robust generalization and significantly outperforms existing
SC frameworks in classifying unseen categories across a range of SNR levels.

</details>


### [66] [Holistic Continual Learning under Concept Drift with Adaptive Memory Realignment](https://arxiv.org/abs/2507.02310)
*Alif Ashrafee,Jedrzej Kozal,Michal Wozniak,Bartosz Krawczyk*

Main category: cs.LG

TL;DR: 论文提出了一种应对概念漂移的持续学习框架AMR，通过动态调整记忆缓冲区来平衡稳定性和适应性，显著减少了标注和计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法假设数据分布静态，忽略了现实数据流中的概念漂移问题，需要同时兼顾稳定性和快速适应能力。

Method: 提出AMR方法，选择性移除过时样本并补充新样本，动态调整记忆缓冲区，以轻量级方式适应概念漂移。

Result: AMR在多个基准数据集上表现优异，性能接近完全重新训练（FR），但显著减少了标注和计算需求。

Conclusion: AMR是一种可扩展的解决方案，能够在非静态环境中有效平衡稳定性和可塑性。

Abstract: Traditional continual learning methods prioritize knowledge retention and
focus primarily on mitigating catastrophic forgetting, implicitly assuming that
the data distribution of previously learned tasks remains static. This
overlooks the dynamic nature of real-world data streams, where concept drift
permanently alters previously seen data and demands both stability and rapid
adaptation.
  We introduce a holistic framework for continual learning under concept drift
that simulates realistic scenarios by evolving task distributions. As a
baseline, we consider Full Relearning (FR), in which the model is retrained
from scratch on newly labeled samples from the drifted distribution. While
effective, this approach incurs substantial annotation and computational
overhead. To address these limitations, we propose Adaptive Memory Realignment
(AMR), a lightweight alternative that equips rehearsal-based learners with a
drift-aware adaptation mechanism. AMR selectively removes outdated samples of
drifted classes from the replay buffer and repopulates it with a small number
of up-to-date instances, effectively realigning memory with the new
distribution. This targeted resampling matches the performance of FR while
reducing the need for labeled data and computation by orders of magnitude.
  To enable reproducible evaluation, we introduce four concept-drift variants
of standard vision benchmarks: Fashion-MNIST-CD, CIFAR10-CD, CIFAR100-CD, and
Tiny-ImageNet-CD, where previously seen classes reappear with shifted
representations. Comprehensive experiments on these datasets using several
rehearsal-based baselines show that AMR consistently counters concept drift,
maintaining high accuracy with minimal overhead. These results position AMR as
a scalable solution that reconciles stability and plasticity in non-stationary
continual learning environments.

</details>


### [67] [Improving Constrained Generation in Language Models via Self-Distilled Twisted Sequential Monte Carlo](https://arxiv.org/abs/2507.02315)
*Sooyeon Kim,Giung Nam,Juho Lee*

Main category: cs.LG

TL;DR: 通过自蒸馏迭代优化基础模型，解决约束文本生成中奖励信号稀疏的问题，显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 在约束文本生成中，目标分布集中在基础模型不太可能生成的输出上，导致学习困难。

Method: 采用自蒸馏方法迭代优化基础模型，使其逐步与目标对齐。

Result: 实验表明，该方法显著提高了生成质量。

Conclusion: 自蒸馏是解决约束文本生成中奖励信号稀疏问题的有效方法。

Abstract: Recent work has framed constrained text generation with autoregressive
language models as a probabilistic inference problem. Among these, Zhao et al.
(2024) introduced a promising approach based on twisted Sequential Monte Carlo,
which incorporates learned twist functions and twist-induced proposals to guide
the generation process. However, in constrained generation settings where the
target distribution concentrates on outputs that are unlikely under the base
model, learning becomes challenging due to sparse and uninformative reward
signals. We show that iteratively refining the base model through
self-distillation alleviates this issue by making the model progressively more
aligned with the target, leading to substantial gains in generation quality.

</details>


### [68] [Transformer-based EEG Decoding: A Survey](https://arxiv.org/abs/2507.02320)
*Haodong Zhang,Hongqi Li*

Main category: cs.LG

TL;DR: 本文综述了Transformer模型在EEG解码中的最新应用，探讨了其架构演变、混合模型设计及未来挑战。


<details>
  <summary>Details</summary>
Motivation: EEG解码是脑机接口研究的关键，传统方法依赖机器学习，而深度学习（尤其是Transformer）通过端到端架构自动提取特征，推动了领域发展。

Method: 文章梳理了Transformer在EEG解码中的基础应用、与其他深度学习技术的混合架构（如卷积/循环/图神经网络），以及定制化Transformer的改进结构。

Result: 总结了Transformer在EEG任务中的优势（如注意力机制处理时序数据）及混合模型的多样化应用。

Conclusion: 尽管Transformer在EEG解码中表现优异，仍需解决数据稀缺、计算效率等挑战，未来可能结合更多新兴技术进一步突破。

Abstract: Electroencephalography (EEG) is one of the most common signals used to
capture the electrical activity of the brain, and the decoding of EEG, to
acquire the user intents, has been at the forefront of brain-computer/machine
interfaces (BCIs/BMIs) research. Compared to traditional EEG analysis methods
with machine learning, the advent of deep learning approaches have gradually
revolutionized the field by providing an end-to-end long-cascaded architecture,
which can learn more discriminative features automatically. Among these,
Transformer is renowned for its strong handling capability of sequential data
by the attention mechanism, and the application of Transformers in various EEG
processing tasks is increasingly prevalent. This article delves into a relevant
survey, summarizing the latest application of Transformer models in EEG
decoding since it appeared. The evolution of the model architecture is followed
to sort and organize the related advances, in which we first elucidate the
fundamentals of the Transformer that benefits EEG decoding and its direct
application. Then, the common hybrid architectures by integrating basic
Transformer with other deep learning techniques
(convolutional/recurrent/graph/spiking neural netwo-rks, generative adversarial
networks, diffusion models, etc.) is overviewed in detail. The research
advances of applying the modified intrinsic structures of customized
Transformer have also been introduced. Finally, the current challenges and
future development prospects in this rapidly evolving field are discussed. This
paper aims to help readers gain a clear understanding of the current state of
Transformer applications in EEG decoding and to provide valuable insights for
future research endeavors.

</details>


### [69] [DeltaSHAP: Explaining Prediction Evolutions in Online Patient Monitoring with Shapley Values](https://arxiv.org/abs/2507.02342)
*Changhun Kim,Yechan Mun,Sangchul Hahn,Eunho Yang*

Main category: cs.LG

TL;DR: DeltaSHAP是一种新型可解释人工智能算法，专为在线患者监测系统设计，解决了临床时间序列解释的独特需求。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法无法满足临床时间序列解释的需求，特别是在实时性和动态变化解释方面。

Method: DeltaSHAP通过调整Shapley值以适应时间序列，捕捉特征联合效应，并提供实时解释。

Result: 在MIMIC-III基准测试中，DeltaSHAP在解释质量上优于现有方法62%，计算效率提升33%。

Conclusion: DeltaSHAP为临床环境提供了一种高效、实时的解释工具，显著提升了患者监测系统的实用性。

Abstract: This study proposes DeltaSHAP, a novel explainable artificial intelligence
(XAI) algorithm specifically designed for online patient monitoring systems. In
clinical environments, discovering the causes driving patient risk evolution is
critical for timely intervention, yet existing XAI methods fail to address the
unique requirements of clinical time series explanation tasks. To this end,
DeltaSHAP addresses three key clinical needs: explaining the changes in the
consecutive predictions rather than isolated prediction scores, providing both
magnitude and direction of feature attributions, and delivering these insights
in real time. By adapting Shapley values to temporal settings, our approach
accurately captures feature coalition effects. It further attributes prediction
changes using only the actually observed feature combinations, making it
efficient and practical for time-sensitive clinical applications. We also
introduce new evaluation metrics to evaluate the faithfulness of the
attributions for online time series, and demonstrate through experiments on
online patient monitoring tasks that DeltaSHAP outperforms state-of-the-art XAI
methods in both explanation quality as 62% and computational efficiency as 33%
time reduction on the MIMIC-III decompensation benchmark. We release our code
at https://github.com/AITRICS/DeltaSHAP.

</details>


### [70] [Offline Reinforcement Learning with Penalized Action Noise Injection](https://arxiv.org/abs/2507.02356)
*JunHyeok Oh,Byung-Jun Lee*

Main category: cs.LG

TL;DR: 论文提出了一种名为PANI的方法，通过注入噪声动作并惩罚噪声量来提升离线强化学习的性能，无需复杂的扩散模型。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在环境交互成本高时很实用，但需要提升泛化能力。扩散模型虽有效但计算成本高，因此探索更简单的方法。

Method: 提出PANI方法，通过噪声注入动作覆盖整个动作空间，并根据噪声量进行惩罚，理论证明其解决修改后的MDP问题。

Result: PANI与多种离线RL算法兼容，在多个基准测试中表现出显著性能提升。

Conclusion: PANI是一种简单有效的方法，无需复杂扩散模型即可提升离线RL性能。

Abstract: Offline reinforcement learning (RL) optimizes a policy using only a fixed
dataset, making it a practical approach in scenarios where interaction with the
environment is costly. Due to this limitation, generalization ability is key to
improving the performance of offline RL algorithms, as demonstrated by recent
successes of offline RL with diffusion models. However, it remains questionable
whether such diffusion models are necessary for highly performing offline RL
algorithms, given their significant computational requirements during
inference. In this paper, we propose Penalized Action Noise Injection (PANI), a
method that simply enhances offline learning by utilizing noise-injected
actions to cover the entire action space, while penalizing according to the
amount of noise injected. This approach is inspired by how diffusion models
have worked in offline RL algorithms. We provide a theoretical foundation for
this method, showing that offline RL algorithms with such noise-injected
actions solve a modified Markov Decision Process (MDP), which we call the noisy
action MDP. PANI is compatible with a wide range of existing off-policy and
offline RL algorithms, and despite its simplicity, it demonstrates significant
performance improvements across various benchmarks.

</details>


### [71] [Deep Reinforcement Learning-Based DRAM Equalizer Parameter Optimization Using Latent Representations](https://arxiv.org/abs/2507.02365)
*Muhammad Usama,Dong Eui Chang*

Main category: cs.LG

TL;DR: 提出了一种基于数据驱动和强化学习的信号完整性优化方法，显著提升了高速DRAM系统的性能。


<details>
  <summary>Details</summary>
Motivation: 高速DRAM系统中信号完整性优化计算量大且依赖模型，需要高效且模型无关的解决方案。

Method: 使用学习的潜在信号表示进行快速信号完整性评估，结合无模型的Advantage Actor-Critic强化学习代理优化参数。

Result: 在行业标准DRAM波形上，眼图窗口面积显著提升：42.7%（CTLE+DFE结构）和36.8%（仅DFE结构）。

Conclusion: 该方法在性能、计算效率和泛化能力上优于现有技术，适用于复杂均衡器架构。

Abstract: Equalizer parameter optimization for signal integrity in high-speed Dynamic
Random Access Memory systems is crucial but often computationally demanding or
model-reliant. This paper introduces a data-driven framework employing learned
latent signal representations for efficient signal integrity evaluation,
coupled with a model-free Advantage Actor-Critic reinforcement learning agent
for parameter optimization. The latent representation captures vital signal
integrity features, offering a fast alternative to direct eye diagram analysis
during optimization, while the reinforcement learning agent derives optimal
equalizer settings without explicit system models. Applied to industry-standard
Dynamic Random Access Memory waveforms, the method achieved significant
eye-opening window area improvements: 42.7\% for cascaded Continuous-Time
Linear Equalizer and Decision Feedback Equalizer structures, and 36.8\% for
Decision Feedback Equalizer-only configurations. These results demonstrate
superior performance, computational efficiency, and robust generalization
across diverse Dynamic Random Access Memory units compared to existing
techniques. Core contributions include an efficient latent signal integrity
metric for optimization, a robust model-free reinforcement learning strategy,
and validated superior performance for complex equalizer architectures.

</details>


### [72] [Improving Consistency in Vehicle Trajectory Prediction Through Preference Optimization](https://arxiv.org/abs/2507.02406)
*Caio Azevedo,Lina Achaji,Stefano Sabatini,Nicola Poerio,Grzegorz Bartyzel,Sascha Hornauer,Fabien Moutarde*

Main category: cs.LG

TL;DR: 论文提出了一种通过偏好优化微调轨迹预测模型的方法，以提升多智能体场景中的预测一致性，同时保持预测精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的轨迹预测模型在复杂交互场景中难以捕捉智能体间的相互依赖关系，导致预测不一致。

Method: 利用偏好优化方法微调轨迹预测模型，输入自动计算的预测未来偏好排名。

Result: 实验表明，该方法显著提升了场景一致性，同时几乎不影响预测精度，且无需增加推理时的计算负担。

Conclusion: 偏好优化是提升多智能体轨迹预测一致性的有效方法，适用于实际自动驾驶场景。

Abstract: Trajectory prediction is an essential step in the pipeline of an autonomous
vehicle. Inaccurate or inconsistent predictions regarding the movement of
agents in its surroundings lead to poorly planned maneuvers and potentially
dangerous situations for the end-user. Current state-of-the-art
deep-learning-based trajectory prediction models can achieve excellent accuracy
on public datasets. However, when used in more complex, interactive scenarios,
they often fail to capture important interdependencies between agents, leading
to inconsistent predictions among agents in the traffic scene. Inspired by the
efficacy of incorporating human preference into large language models, this
work fine-tunes trajectory prediction models in multi-agent settings using
preference optimization. By taking as input automatically calculated preference
rankings among predicted futures in the fine-tuning process, our
experiments--using state-of-the-art models on three separate datasets--show
that we are able to significantly improve scene consistency while minimally
sacrificing trajectory prediction accuracy and without adding any excess
computational requirements at inference time.

</details>


### [73] [S2FGL: Spatial Spectral Federated Graph Learning](https://arxiv.org/abs/2507.02409)
*Zihan Tan,Suyuan Huang,Guancheng Wan,Wenke Huang,He Li,Mang Ye*

Main category: cs.LG

TL;DR: 论文提出了一种结合空间和频谱策略的联邦图学习框架S2FGL，以解决子图联邦学习中标签信号中断和频谱客户端漂移问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅从结构角度处理子图联邦学习，忽略了图信号在空间和频谱域的传播问题，导致标签信号中断和频谱客户端漂移。

Method: 提出全局知识库缓解标签信号中断，并通过频率对齐解决频谱客户端漂移，形成框架S2FGL。

Result: 在多个数据集上的实验表明S2FGL的优越性。

Conclusion: S2FGL有效解决了子图联邦学习中的空间和频谱问题，提升了全局泛化能力。

Abstract: Federated Graph Learning (FGL) combines the privacy-preserving capabilities
of federated learning (FL) with the strong graph modeling capability of Graph
Neural Networks (GNNs). Current research addresses subgraph-FL only from the
structural perspective, neglecting the propagation of graph signals on spatial
and spectral domains of the structure. From a spatial perspective, subgraph-FL
introduces edge disconnections between clients, leading to disruptions in label
signals and a degradation in the class knowledge of the global GNN. From a
spectral perspective, spectral heterogeneity causes inconsistencies in signal
frequencies across subgraphs, which makes local GNNs overfit the local signal
propagation schemes. As a result, spectral client drifts occur, undermining
global generalizability. To tackle the challenges, we propose a global
knowledge repository to mitigate label signal disruption and a frequency
alignment to address spectral client drifts. The combination of spatial and
spectral strategies forms our framework S2FGL. Extensive experiments on
multiple datasets demonstrate the superiority of S2FGL. The code is available
at https://github.com/Wonder7racer/S2FGL.git.

</details>


### [74] [Variational Kolmogorov-Arnold Network](https://arxiv.org/abs/2507.02466)
*Francesco Alesiani,Henrik Christiansen,Federico Errica*

Main category: cs.LG

TL;DR: InfinityKAN通过变分推断自适应学习无限基函数，扩展了Kolmogorov Arnold Networks（KANs）的适用性。


<details>
  <summary>Details</summary>
Motivation: KANs基于Kolmogorov-Arnold定理，但作为多层感知机（MLP）的替代方案时，其性能受限于基函数数量的选择。

Method: 将问题建模为变分推断优化问题，通过反向传播自适应学习无限基函数。

Result: 提出的InfinityKAN扩展了KANs的潜在应用范围。

Conclusion: InfinityKAN通过动态学习基函数数量，提升了KANs的灵活性和实用性。

Abstract: Kolmogorov Arnold Networks (KANs) are an emerging architecture for building
machine learning models. KANs are based on the theoretical foundation of the
Kolmogorov-Arnold Theorem and its expansions, which provide an exact
representation of a multi-variate continuous bounded function as the
composition of a limited number of univariate continuous functions. While such
theoretical results are powerful, their use as a representation learning
alternative to a multi-layer perceptron (MLP) hinges on the ad-hoc choice of
the number of bases modeling each of the univariate functions. In this work, we
show how to address this problem by adaptively learning a potentially infinite
number of bases for each univariate function during training. We therefore
model the problem as a variational inference optimization problem. Our
proposal, called InfinityKAN, which uses backpropagation, extends the potential
applicability of KANs by treating an important hyperparameter as part of the
learning process.

</details>


### [75] [Continual Gradient Low-Rank Projection Fine-Tuning for LLMs](https://arxiv.org/abs/2507.02503)
*Chenxu Wang,Yilin Lyu,Zicheng Sun,Liping Jing*

Main category: cs.LG

TL;DR: GORP是一种新的训练策略，通过结合全参数和低秩参数，在统一低秩梯度子空间中联合更新，解决了LLMs持续微调中效率与表达能力的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决低秩适应（LoRA）在持续学习中的局限性，如低秩性质和对显式参数约束的依赖，导致模型学习新任务和知识迁移能力受限。

Method: 提出GORP，结合全参数和低秩参数，在统一低秩梯度子空间中联合更新，扩展优化空间同时保持效率。

Result: 在持续学习基准测试中，GORP表现优于现有最先进方法。

Conclusion: GORP有效解决了LLMs持续微调中的效率与表达能力问题，并减轻了灾难性遗忘。

Abstract: Continual fine-tuning of Large Language Models (LLMs) is hampered by the
trade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA)
offers efficiency but constrains the model's ability to learn new tasks and
transfer knowledge due to its low-rank nature and reliance on explicit
parameter constraints. We propose GORP (Gradient LOw Rank Projection) for
Continual Learning, a novel training strategy that overcomes these limitations
by synergistically combining full and low-rank parameters and jointly updating
within a unified low-rank gradient subspace. GORP expands the optimization
space while preserving efficiency and mitigating catastrophic forgetting.
Extensive experiments on continual learning benchmarks demonstrate GORP's
superior performance compared to existing state-of-the-art approaches. Code is
available at https://github.com/Wcxwcxw/GORP.

</details>


### [76] [TFOC-Net: A Short-time Fourier Transform-based Deep Learning Approach for Enhancing Cross-Subject Motor Imagery Classification](https://arxiv.org/abs/2507.02510)
*Ahmed G. Habashi,Ahmed M. Azab,Seif Eldawlatly,Gamal M. Aly*

Main category: cs.LG

TL;DR: 论文提出了一种优化预处理和深度学习的跨被试运动想象分类方法，显著提升了性能，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决跨被试脑机接口中因脑电图模式差异导致的分类精度低的问题，推动无校准脑机接口的实际应用。

Method: 采用短时傅里叶变换预处理脑电图数据，优化变换参数，并结合卷积神经网络和平衡批处理策略进行分类。

Result: 在多个数据集上取得显著提升，分类准确率分别为67.60%（IV-1）、65.96%（IV-2A）和80.22%（IV-2B）。

Conclusion: 该方法为跨被试运动想象分类设立了新基准，并贡献了一个开放数据集，推动了该领域的研究。

Abstract: Cross-subject motor imagery (CS-MI) classification in brain-computer
interfaces (BCIs) is a challenging task due to the significant variability in
Electroencephalography (EEG) patterns across different individuals. This
variability often results in lower classification accuracy compared to
subject-specific models, presenting a major barrier to developing
calibration-free BCIs suitable for real-world applications. In this paper, we
introduce a novel approach that significantly enhances cross-subject MI
classification performance through optimized preprocessing and deep learning
techniques. Our approach involves direct classification of Short-Time Fourier
Transform (STFT)-transformed EEG data, optimized STFT parameters, and a
balanced batching strategy during training of a Convolutional Neural Network
(CNN). This approach is uniquely validated across four different datasets,
including three widely-used benchmark datasets leading to substantial
improvements in cross-subject classification, achieving 67.60% on the BCI
Competition IV Dataset 1 (IV-1), 65.96% on Dataset 2A (IV-2A), and 80.22% on
Dataset 2B (IV-2B), outperforming state-of-the-art techniques. Additionally, we
systematically investigate the classification performance using MI windows
ranging from the full 4-second window to 1-second windows. These results
establish a new benchmark for generalizable, calibration-free MI classification
in addition to contributing a robust open-access dataset to advance research in
this domain.

</details>


### [77] [RetrySQL: text-to-SQL training with retry data for self-correcting query generation](https://arxiv.org/abs/2507.02529)
*Alicja Rączkowska,Riccardo Belluzzo,Piotr Zieliński,Joanna Baran,Paweł Olszewski*

Main category: cs.LG

TL;DR: RetrySQL是一种新的文本到SQL生成模型训练方法，通过引入自我纠正机制，显著提高了生成准确性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL任务的研究多依赖于黑盒语言模型，缺乏针对SQL的生成模型研究，且自我纠正策略的应用尚未探索。

Method: 通过准备参考SQL查询的推理步骤并故意引入错误，生成包含错误和纠正步骤的“重试数据”，并基于开源编码模型进行持续预训练。

Result: 使用重试数据预训练的模型在整体和挑战性执行准确率上提升了4个百分点，且自我纠正行为被模型成功学习。

Conclusion: RetrySQL证明了自我纠正机制在文本到SQL任务中的有效性，为SQL导向的语言模型提供了一种新的准确性提升方法。

Abstract: The text-to-SQL task is an active challenge in Natural Language Processing.
Many existing solutions focus on using black-box language models extended with
specialized components within customized end-to-end text-to-SQL pipelines.
While these solutions use both closed-source proprietary language models and
coding-oriented open-source models, there is a lack of research regarding
SQL-specific generative models. At the same time, recent advancements in
self-correcting generation strategies show promise for improving the
capabilities of existing architectures. The application of these concepts to
the text-to-SQL task remains unexplored. In this paper, we introduce RetrySQL,
a new approach to training text-to-SQL generation models. We prepare reasoning
steps for reference SQL queries and then corrupt them to create retry data that
contains both incorrect and corrected steps, divided with a special token. We
continuously pre-train an open-source coding model with this data and
demonstrate that retry steps yield an improvement of up to 4 percentage points
in both overall and challenging execution accuracy metrics, compared to
pre-training without retry data. Additionally, we confirm that supervised
fine-tuning with LoRA is ineffective for learning from retry data and that
full-parameter pre-training is a necessary requirement for that task. We
showcase that the self-correcting behavior is learned by the model and the
increase in downstream accuracy metrics is a result of this additional skill.
Finally, we incorporate RetrySQL-trained models into the full text-to-SQL
pipeline and showcase that they are competitive in terms of execution accuracy
with proprietary models that contain orders of magnitude more parameters.
RetrySQL demonstrates that self-correction can be learned in the text-to-SQL
task and provides a novel way of improving generation accuracy for SQL-oriented
language models.

</details>


### [78] [Position: A Theory of Deep Learning Must Include Compositional Sparsity](https://arxiv.org/abs/2507.02550)
*David A. Danhofer,Davide D'Ascenzo,Rafael Dubach,Tomaso Poggio*

Main category: cs.LG

TL;DR: 深度神经网络（DNNs）通过利用目标函数的组合稀疏结构取得成功，这种结构普遍存在于高效可计算函数中。


<details>
  <summary>Details</summary>
Motivation: 理解DNNs学习动态的基本原理，尤其是其如何利用组合稀疏性。

Method: 提出组合稀疏性是DNNs成功的关键，并探讨其在理论和学习问题中的普遍性。

Result: 组合稀疏性是高效可计算函数的共同特性，对DNNs的近似和泛化有重要影响。

Conclusion: 深入研究组合稀疏性对完善深度学习理论及人工智能发展至关重要。

Abstract: Overparametrized Deep Neural Networks (DNNs) have demonstrated remarkable
success in a wide variety of domains too high-dimensional for classical shallow
networks subject to the curse of dimensionality. However, open questions about
fundamental principles, that govern the learning dynamics of DNNs, remain. In
this position paper we argue that it is the ability of DNNs to exploit the
compositionally sparse structure of the target function driving their success.
As such, DNNs can leverage the property that most practically relevant
functions can be composed from a small set of constituent functions, each of
which relies only on a low-dimensional subset of all inputs. We show that this
property is shared by all efficiently Turing-computable functions and is
therefore highly likely present in all current learning problems. While some
promising theoretical insights on questions concerned with approximation and
generalization exist in the setting of compositionally sparse functions,
several important questions on the learnability and optimization of DNNs
remain. Completing the picture of the role of compositional sparsity in deep
learning is essential to a comprehensive theory of artificial, and even
general, intelligence.

</details>


### [79] [Transformers Don't Need LayerNorm at Inference Time: Scaling LayerNorm Removal to GPT-2 XL and the Implications for Mechanistic Interpretability](https://arxiv.org/abs/2507.02559)
*Luca Baroni,Galvin Khara,Joachim Schaeffer,Marat Subkhankulov,Stefan Heimersheim*

Main category: cs.LG

TL;DR: 研究表明，层归一化（LN）在GPT-2模型中并非必要，移除后仅导致轻微性能损失，且不影响语言建模能力。


<details>
  <summary>Details</summary>
Motivation: 探索LN在推理阶段的作用及其对模型可解释性的影响。

Method: 移除GPT-2模型中的所有LN层，并评估性能变化及可解释性技术的效果。

Result: 移除LN后验证损失仅小幅增加（如GPT-2 XL的交叉熵损失+0.03），且可解释性技术效果未显著提升。

Conclusion: LN在语言建模中作用有限，移除后模型仍能正常工作，为可解释性研究提供了新工具。

Abstract: Layer-wise normalization (LN) is an essential component of virtually all
transformer-based large language models. While its effects on training
stability are well documented, its role at inference time is poorly understood.
Additionally, LN layers hinder mechanistic interpretability by introducing
additional nonlinearities and increasing the interconnectedness of individual
model components. Here, we show that all LN layers can be removed from every
GPT-2 model with only a small increase in validation loss (e.g. +0.03
cross-entropy loss for GPT-2 XL). Thus, LN cannot play a substantial role in
language modeling. We find that the amount of fine-tuning data needed for LN
removal grows sublinearly with model parameters, suggesting scaling to larger
models is feasible. We release a suite of LN-free GPT-2 models on Hugging Face.
Furthermore, we test interpretability techniques on LN-free models. Direct
logit attribution now gives the exact direct effect of individual components,
while the accuracy of attribution patching does not significantly improve. We
also confirm that GPT-2's "confidence neurons" are inactive in the LN-free
models. Our work clarifies the role of LN layers in language modeling, showing
that GPT-2-class models can function without LN layers. We hope that our
LN-free analogs of the GPT-2 family of models will enable more precise
interpretability research and improve our understanding of language models.

</details>


### [80] [Scalable Interconnect Learning in Boolean Networks](https://arxiv.org/abs/2507.02585)
*Fabian Kresse,Emily Yu,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 扩展可微分布尔逻辑网络（DBNs）的可训练互联结构，并提出两种剪枝方法以提高模型效率。


<details>
  <summary>Details</summary>
Motivation: 解决DBNs在输入宽度增加时参数数量增长的问题，同时提升模型压缩与准确性的平衡。

Method: 1. 引入可训练的微分互联结构；2. 提出基于SAT的逻辑等价剪枝和基于相似性的数据驱动剪枝。

Result: 实现了更宽的输入层扩展，同时保持了准确性，剪枝方法优于贪心基线。

Conclusion: 扩展的DBNs在资源受限硬件上表现更优，剪枝方法显著提升效率。

Abstract: Learned Differentiable Boolean Logic Networks (DBNs) already deliver
efficient inference on resource-constrained hardware. We extend them with a
trainable, differentiable interconnect whose parameter count remains constant
as input width grows, allowing DBNs to scale to far wider layers than earlier
learnable-interconnect designs while preserving their advantageous accuracy. To
further reduce model size, we propose two complementary pruning stages: an
SAT-based logic equivalence pass that removes redundant gates without affecting
performance, and a similarity-based, data-driven pass that outperforms a
magnitude-style greedy baseline and offers a superior compression-accuracy
trade-off.

</details>


### [81] [Padé Approximant Neural Networks for Enhanced Electric Motor Fault Diagnosis Using Vibration and Acoustic Data](https://arxiv.org/abs/2507.02599)
*Sertac Kilickaya,Levent Eren*

Main category: cs.LG

TL;DR: 研究旨在通过Padé Approximant Neuron (PAON)模型提升感应电机故障诊断性能，PadéNets在振动和声学数据诊断中优于传统CNN和Self-ONNs。


<details>
  <summary>Details</summary>
Motivation: 传统感应电机故障诊断方法（如加速度计和麦克风）存在局限性，而具有非线性神经元架构的深度学习模型（如PadéNets）有望提升诊断性能。

Method: 比较了一维CNN、Self-ONNs和PadéNets三种深度学习架构在振动和声学数据上的诊断能力，PadéNets设计为增强非线性并兼容无界激活函数。

Result: PadéNets在多个传感器数据上表现优异，诊断准确率最高达99.96%，显著优于基线模型。

Conclusion: PadéNets因其增强的非线性和兼容无界激活函数的特性，显著提升了感应电机故障诊断性能。

Abstract: Purpose: The primary aim of this study is to enhance fault diagnosis in
induction machines by leveraging the Pad\'e Approximant Neuron (PAON) model.
While accelerometers and microphones are standard in motor condition
monitoring, deep learning models with nonlinear neuron architectures offer
promising improvements in diagnostic performance. This research addresses the
question: Can Pad\'e Approximant Neural Networks (Pad\'eNets) outperform
conventional Convolutional Neural Networks (CNNs) and Self-Organized
Operational Neural Networks (Self-ONNs) in diagnosing electrical and mechanical
faults using vibration and acoustic data?
  Methods: We evaluate and compare the diagnostic capabilities of three deep
learning architectures: one-dimensional CNNs, Self-ONNs, and Pad\'eNets. These
models are tested on the University of Ottawa's publicly available
constant-speed induction motor datasets, which include both vibration and
acoustic sensor data. The Pad\'eNet model is designed to introduce enhanced
nonlinearity and is compatible with unbounded activation functions such as
Leaky ReLU.
  Results and Conclusion: Pad\'eNets consistently outperformed the baseline
models, achieving diagnostic accuracies of 99.96%, 98.26%, 97.61%, and 98.33%
for accelerometers 1, 2, 3, and the acoustic sensor, respectively. The enhanced
nonlinearity of Pad\'eNets, together with their compatibility with unbounded
activation functions, significantly improves fault diagnosis performance in
induction motor condition monitoring.

</details>


### [82] [Lost in Latent Space: An Empirical Study of Latent Diffusion Models for Physics Emulation](https://arxiv.org/abs/2507.02608)
*François Rozet,Ruben Ohana,Michael McCabe,Gilles Louppe,François Lanusse,Shirley Ho*

Main category: cs.LG

TL;DR: 研究探讨了在动力学系统仿真中应用潜在空间扩散模型的可行性及其效果，发现其在高达1000倍压缩率下仍保持高精度，且优于非生成式方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在推理时的高计算成本限制了其作为快速物理仿真器的应用，本研究旨在探索潜在空间仿真是否能解决这一问题。

Method: 通过在自编码器的潜在空间而非像素空间生成数据，研究了潜在空间扩散模型在动力学系统仿真中的表现。

Result: 潜在空间仿真在高达1000倍压缩率下仍保持高精度，且扩散模型比非生成式方法更准确，预测多样性更强。

Conclusion: 潜在空间扩散模型是一种高效且准确的动力学系统仿真方法，同时提供了实用的训练设计建议。

Abstract: The steep computational cost of diffusion models at inference hinders their
use as fast physics emulators. In the context of image and video generation,
this computational drawback has been addressed by generating in the latent
space of an autoencoder instead of the pixel space. In this work, we
investigate whether a similar strategy can be effectively applied to the
emulation of dynamical systems and at what cost. We find that the accuracy of
latent-space emulation is surprisingly robust to a wide range of compression
rates (up to 1000x). We also show that diffusion-based emulators are
consistently more accurate than non-generative counterparts and compensate for
uncertainty in their predictions with greater diversity. Finally, we cover
practical design choices, spanning from architectures to optimizers, that we
found critical to train latent-space emulators.

</details>


### [83] [L-VAE: Variational Auto-Encoder with Learnable Beta for Disentangled Representation](https://arxiv.org/abs/2507.02619)
*Hazal Mogultay Ozcan,Sinan Kalkan,Fatos T. Yarman-Vural*

Main category: cs.LG

TL;DR: L-VAE是一种新型变分自编码器，通过学习损失函数的超参数和模型参数，动态平衡解缠和重构损失，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决β-VAE中超参数η需经验调整的问题，动态平衡解缠和重构损失。

Method: 提出L-VAE模型，学习损失函数权重和模型参数，并添加正则化项防止偏置。

Result: 在多个数据集上表现最佳或次佳，成功解缠CelebA中的面部属性。

Conclusion: L-VAE有效平衡重构和解缠，优于现有方法。

Abstract: In this paper, we propose a novel model called Learnable VAE (L-VAE), which
learns a disentangled representation together with the hyperparameters of the
cost function. L-VAE can be considered as an extension of \b{eta}-VAE, wherein
the hyperparameter, \b{eta}, is empirically adjusted. L-VAE mitigates the
limitations of \b{eta}-VAE by learning the relative weights of the terms in the
loss function to control the dynamic trade-off between disentanglement and
reconstruction losses. In the proposed model, the weight of the loss terms and
the parameters of the model architecture are learned concurrently. An
additional regularization term is added to the loss function to prevent bias
towards either reconstruction or disentanglement losses. Experimental analyses
show that the proposed L-VAE finds an effective balance between reconstruction
fidelity and disentangling the latent dimensions. Comparisons of the proposed
L-VAE against \b{eta}-VAE, VAE, ControlVAE, DynamicVAE, and {\sigma}-VAE on
datasets, such as dSprites, MPI3D-complex, Falcor3D, and Isaac3D reveals that
L-VAE consistently provides the best or the second best performances measured
by a set of disentanglement metrics. Moreover, qualitative experiments on
CelebA dataset, confirm the success of the L-VAE model for disentangling the
facial attributes.

</details>


### [84] [A Matrix Variational Auto-Encoder for Variant Effect Prediction in Pharmacogenes](https://arxiv.org/abs/2507.02624)
*Antoine Honoré,Borja Rodríguez Gálvez,Yoomi Park,Yitian Zhou,Volker M. Lauschke,Ming Xiao*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer的矩阵变分自编码器（matVAE），用于评估蛋白质变体的功能影响，并在DMS数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖多序列比对（MSA），但假设自然变体是适应的，这在药物基因组学中受到挑战。DMS数据集提供了替代方案。

Method: 使用matVAE模型，结合结构化先验，并在33个DMS数据集上评估性能。

Result: matVAE-MSA在零样本预测中优于DeepSequence，且参数更少、计算需求更低。结合AlphaFold结构后性能进一步提升。

Conclusion: DMS数据集有望替代MSA，且性能损失小，值得进一步开发和研究。

Abstract: Variant effect predictors (VEPs) aim to assess the functional impact of
protein variants, traditionally relying on multiple sequence alignments (MSAs).
This approach assumes that naturally occurring variants are fit, an assumption
challenged by pharmacogenomics, where some pharmacogenes experience low
evolutionary pressure. Deep mutational scanning (DMS) datasets provide an
alternative by offering quantitative fitness scores for variants. In this work,
we propose a transformer-based matrix variational auto-encoder (matVAE) with a
structured prior and evaluate its performance on 33 DMS datasets corresponding
to 26 drug target and ADME proteins from the ProteinGym benchmark. Our model
trained on MSAs (matVAE-MSA) outperforms the state-of-the-art DeepSequence
model in zero-shot prediction on DMS datasets, despite using an order of
magnitude fewer parameters and requiring less computation at inference time. We
also compare matVAE-MSA to matENC-DMS, a model of similar capacity trained on
DMS data, and find that the latter performs better on supervised prediction
tasks. Additionally, incorporating AlphaFold-generated structures into our
transformer model further improves performance, achieving results comparable to
DeepSequence trained on MSAs and finetuned on DMS. These findings highlight the
potential of DMS datasets to replace MSAs without significant loss in
predictive performance, motivating further development of DMS datasets and
exploration of their relationships to enhance variant effect prediction.

</details>


### [85] [Medical Data Pecking: A Context-Aware Approach for Automated Quality Evaluation of Structured Medical Data](https://arxiv.org/abs/2507.02628)
*Irena Girshovitz,Atai Ambus,Moni Shahar,Ran Gilad-Bachrach*

Main category: cs.LG

TL;DR: 论文提出了一种基于软件工程单元测试和覆盖率概念的医学数据质量评估方法（MDPT），通过自动化测试生成和执行框架识别数据质量问题，提高了电子健康记录（EHR）数据的可靠性。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）数据在流行病学研究和人工智能训练中的应用日益广泛，但其数据质量问题（如偏差、系统性错误）影响了研究结果的可靠性。现有质量评估方法不足，缺乏系统性评估流程。

Method: 采用软件工程中的单元测试和覆盖率概念，开发了医学数据啄取工具（MDPT），包括自动化测试生成器和数据测试框架，利用大语言模型生成测试套件并执行测试。

Result: 在三个数据集（All of Us、MIMIC-III、SyntheticMass）上评估MDPT，生成55-73个测试，识别出20-43个数据问题，验证了测试套件的准确性和参考基础。

Conclusion: 该方法通过结合外部医学知识，实现了上下文敏感的数据质量测试，为数据分析和结果有效性提供了改进基础，未来可扩展至更多数据模态和优化参考基础方法。

Abstract: Background: The use of Electronic Health Records (EHRs) for epidemiological
studies and artificial intelligence (AI) training is increasing rapidly. The
reliability of the results depends on the accuracy and completeness of EHR
data. However, EHR data often contain significant quality issues, including
misrepresentations of subpopulations, biases, and systematic errors, as they
are primarily collected for clinical and billing purposes. Existing quality
assessment methods remain insufficient, lacking systematic procedures to assess
data fitness for research.
  Methods: We present the Medical Data Pecking approach, which adapts unit
testing and coverage concepts from software engineering to identify data
quality concerns. We demonstrate our approach using the Medical Data Pecking
Tool (MDPT), which consists of two main components: (1) an automated test
generator that uses large language models and grounding techniques to create a
test suite from data and study descriptions, and (2) a data testing framework
that executes these tests, reporting potential errors and coverage.
  Results: We evaluated MDPT on three datasets: All of Us (AoU), MIMIC-III, and
SyntheticMass, generating 55-73 tests per cohort across four conditions. These
tests correctly identified 20-43 non-aligned or non-conforming data issues. We
present a detailed analysis of the LLM-generated test suites in terms of
reference grounding and value accuracy.
  Conclusion: Our approach incorporates external medical knowledge to enable
context-sensitive data quality testing as part of the data analysis workflow to
improve the validity of its outcomes. Our approach tackles these challenges
from a quality assurance perspective, laying the foundation for further
development such as additional data modalities and improved grounding methods.

</details>


### [86] [High-Order Deep Meta-Learning with Category-Theoretic Interpretation](https://arxiv.org/abs/2507.02634)
*David H. Mguni*

Main category: cs.LG

TL;DR: 提出了一种新的分层深度学习框架，通过递归高阶元学习使神经网络能够构建、解决和泛化任务层次结构。


<details>
  <summary>Details</summary>
Motivation: 解决传统机器学习依赖人类生成数据的局限性，通过生成虚拟任务和软约束，实现自主学习和泛化。

Method: 采用生成机制创建虚拟任务，通过元学习器迭代优化约束区域，并结合范畴论视角构建分层学习结构。

Result: 框架能够自主生成任务和约束，增强归纳偏置，支持跨任务的抽象和知识转移。

Conclusion: 该架构有望推动机器学习向通用人工智能发展，支持自主生成任务和解决方案。

Abstract: We introduce a new hierarchical deep learning framework for recursive
higher-order meta-learning that enables neural networks (NNs) to construct,
solve, and generalise across hierarchies of tasks. Central to this approach is
a generative mechanism that creates \emph{virtual tasks} -- synthetic problem
instances designed to enable the meta-learner to learn \emph{soft constraints}
and unknown generalisable rules across related tasks. Crucially, this enables
the framework to generate its own informative, task-grounded datasets thereby
freeing machine learning (ML) training from the limitations of relying entirely
on human-generated data. By actively exploring the virtual point landscape and
seeking out tasks lower-level learners find difficult, the meta-learner
iteratively refines constraint regions. This enhances inductive biases,
regularises the adaptation process, and produces novel, unanticipated tasks and
constraints required for generalisation. Each meta-level of the hierarchy
corresponds to a progressively abstracted generalisation of problems solved at
lower levels, enabling a structured and interpretable learning progression. By
interpreting meta-learners as category-theoretic \emph{functors} that generate
and condition a hierarchy of subordinate learners, we establish a compositional
structure that supports abstraction and knowledge transfer across progressively
generalised tasks. The category-theoretic perspective unifies existing
meta-learning models and reveals how learning processes can be transformed and
compared through functorial relationships, while offering practical design
principles for structuring meta-learning. We speculate this architecture may
underpin the next generation of NNs capable of autonomously generating novel,
instructive tasks and their solutions, thereby advancing ML towards general
artificial intelligence.

</details>


### [87] [On Efficient Bayesian Exploration in Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.02639)
*Alberto Caron,Chris Hicks,Vasilios Mavroudis*

Main category: cs.LG

TL;DR: 该论文提出了一种基于信息论的内在动机方法，通过目标化认知不确定性而非环境固有噪声，实现数据高效的强化学习探索。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中数据高效探索的挑战，填补IG方法缺乏理论基础的空白。

Method: 提出PTS-BE框架，结合模型规划与信息论奖励，并通过稀疏变分高斯过程等方法实现近似计算。

Result: PTS-BE在稀疏奖励或纯探索任务中显著优于基线方法。

Conclusion: 认知不确定性奖励能有效指导探索，PTS-BE框架为样本高效探索提供了理论和实践支持。

Abstract: In this work, we address the challenge of data-efficient exploration in
reinforcement learning by examining existing principled, information-theoretic
approaches to intrinsic motivation. Specifically, we focus on a class of
exploration bonuses that targets epistemic uncertainty rather than the
aleatoric noise inherent in the environment. We prove that these bonuses
naturally signal epistemic information gains and converge to zero once the
agent becomes sufficiently certain about the environment's dynamics and
rewards, thereby aligning exploration with genuine knowledge gaps. Our analysis
provides formal guarantees for IG-based approaches, which previously lacked
theoretical grounding. To enable practical use, we also discuss tractable
approximations via sparse variational Gaussian Processes, Deep Kernels and Deep
Ensemble models. We then outline a general framework - Predictive Trajectory
Sampling with Bayesian Exploration (PTS-BE) - which integrates model-based
planning with information-theoretic bonuses to achieve sample-efficient deep
exploration. We empirically demonstrate that PTS-BE substantially outperforms
other baselines across a variety of environments characterized by sparse
rewards and/or purely exploratory tasks.

</details>


### [88] [Fair Deepfake Detectors Can Generalize](https://arxiv.org/abs/2507.02645)
*Harry Cheng,Ming-Hui Liu,Yangyang Guo,Tianyi Wang,Liqiang Nie,Mohan Kankanhalli*

Main category: cs.LG

TL;DR: 论文提出了DAID框架，通过因果分析解决深度伪造检测中的公平性与泛化性冲突，并在实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决深度伪造检测模型中公平性与泛化性之间的冲突，揭示其因果关系。

Method: 提出DAID框架，包括数据重平衡和特征聚合两部分，通过因果调整优化模型。

Result: 在三个跨域基准测试中，DAID在公平性和泛化性上均优于现有方法。

Conclusion: DAID通过因果干预有效解决了公平性与泛化性的冲突，具有理论和实践价值。

Abstract: Deepfake detection models face two critical challenges: generalization to
unseen manipulations and demographic fairness among population groups. However,
existing approaches often demonstrate that these two objectives are inherently
conflicting, revealing a trade-off between them. In this paper, we, for the
first time, uncover and formally define a causal relationship between fairness
and generalization. Building on the back-door adjustment, we show that
controlling for confounders (data distribution and model capacity) enables
improved generalization via fairness interventions. Motivated by this insight,
we propose Demographic Attribute-insensitive Intervention Detection (DAID), a
plug-and-play framework composed of: i) Demographic-aware data rebalancing,
which employs inverse-propensity weighting and subgroup-wise feature
normalization to neutralize distributional biases; and ii) Demographic-agnostic
feature aggregation, which uses a novel alignment loss to suppress
sensitive-attribute signals. Across three cross-domain benchmarks, DAID
consistently achieves superior performance in both fairness and generalization
compared to several state-of-the-art detectors, validating both its theoretical
foundation and practical effectiveness.

</details>


### [89] [OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding](https://arxiv.org/abs/2507.02659)
*Ramchalam Kinattinkara Ramakrishnan,Zhaocong Yuan,Shaojie Zhuo,Chen Feng,Yicheng Lin,Chenzheng Su,Xiaopeng Zhang*

Main category: cs.LG

TL;DR: OmniDraft是一个统一框架，使单个草稿模型能与任何目标模型配合，并通过动态适应用户数据提升解码速度。


<details>
  <summary>Details</summary>
Motivation: 解决在线部署中草稿模型与目标模型不兼容及延迟优化的问题。

Method: 采用在线n-gram缓存和混合蒸馏微调解决词汇不匹配，结合自适应草稿技术提升速度。

Result: OmniDraft使单个Llama-68M模型能与多种目标模型配合，解码速度提升1.5-2倍。

Conclusion: OmniDraft适用于设备端LLM应用，实现了“一个草稿模型适配所有”的范式。

Abstract: Speculative decoding generally dictates having a small, efficient draft model
that is either pretrained or distilled offline to a particular target model
series, for instance, Llama or Qwen models. However, within online deployment
settings, there are two major challenges: 1) usage of a target model that is
incompatible with the draft model; 2) expectation of latency improvements over
usage and time. In this work, we propose OmniDraft, a unified framework that
enables a single draft model to operate with any target model and adapt
dynamically to user data. We introduce an online n-gram cache with hybrid
distillation fine-tuning to address the cross-vocabulary mismatch across draft
and target models; and further improve decoding speed by leveraging adaptive
drafting techniques. OmniDraft is particularly suitable for on-device LLM
applications where model cost, efficiency and user customization are the major
points of contention. This further highlights the need to tackle the above
challenges and motivates the \textit{``one drafter for all''} paradigm. We
showcase the proficiency of the OmniDraft framework by performing online
learning on math reasoning, coding and text generation tasks. Notably,
OmniDraft enables a single Llama-68M model to pair with various target models
including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;
and additionally provides up to 1.5-2x speedup.

</details>


### [90] [Guided Generation for Developable Antibodies](https://arxiv.org/abs/2507.02670)
*Siqi Zhao,Joshua Moller,Porfi Quintero-Cadena,Lood van Niekerk*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型的抗体序列优化方法，结合SVDD模块提升可开发性。


<details>
  <summary>Details</summary>
Motivation: 抗体治疗需要高亲和力、稳定性和安全性，但传统方法难以同时满足这些要求。

Method: 使用离散扩散模型训练自然抗体序列，结合SVDD模块引导生成。

Result: 模型能生成符合自然抗体特征和临床要求的序列，SVDD显著提升可开发性评分。

Conclusion: 该框架为抗体设计提供了一种高效的计算方法，结合实验可实现迭代优化。

Abstract: Therapeutic antibodies require not only high-affinity target engagement, but
also favorable manufacturability, stability, and safety profiles for clinical
effectiveness. These properties are collectively called `developability'. To
enable a computational framework for optimizing antibody sequences for
favorable developability, we introduce a guided discrete diffusion model
trained on natural paired heavy- and light-chain sequences from the Observed
Antibody Space (OAS) and quantitative developability measurements for 246
clinical-stage antibodies. To steer generation toward biophysically viable
candidates, we integrate a Soft Value-based Decoding in Diffusion (SVDD) Module
that biases sampling without compromising naturalness. In unconstrained
sampling, our model reproduces global features of both the natural repertoire
and approved therapeutics, and under SVDD guidance we achieve significant
enrichment in predicted developability scores over unguided baselines. When
combined with high-throughput developability assays, this framework enables an
iterative, ML-driven pipeline for designing antibodies that satisfy binding and
biophysical criteria in tandem.

</details>


### [91] [Embedding-Based Federated Data Sharing via Differentially Private Conditional VAEs](https://arxiv.org/abs/2507.02671)
*Francesco Di Salvo,Hanh Huyen My Nguyen,Christian Ledig*

Main category: cs.LG

TL;DR: 提出一种基于差分隐私生成模型的数据共享方法，通过联合训练DP-CVAE实现隐私保护，同时支持多样化下游任务。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在医学影像中因数据稀缺和隐私限制导致的问题，同时克服联邦学习的高通信成本和任务单一性。

Method: 采用基础模型提取紧凑嵌入，联合训练差分隐私条件变分自编码器（DP-CVAE），建模全局隐私感知数据分布。

Result: 方法在隐私性、可扩展性和效率上优于传统联邦学习分类器，且DP-CVAE比DP-CGAN生成更高保真嵌入，参数减少5倍。

Conclusion: 该方法为医学影像中的深度学习提供了隐私保护、高效且灵活的数据共享解决方案。

Abstract: Deep Learning (DL) has revolutionized medical imaging, yet its adoption is
constrained by data scarcity and privacy regulations, limiting access to
diverse datasets. Federated Learning (FL) enables decentralized training but
suffers from high communication costs and is often restricted to a single
downstream task, reducing flexibility. We propose a data-sharing method via
Differentially Private (DP) generative models. By adopting foundation models,
we extract compact, informative embeddings, reducing redundancy and lowering
computational overhead. Clients collaboratively train a Differentially Private
Conditional Variational Autoencoder (DP-CVAE) to model a global, privacy-aware
data distribution, supporting diverse downstream tasks. Our approach, validated
across multiple feature extractors, enhances privacy, scalability, and
efficiency, outperforming traditional FL classifiers while ensuring
differential privacy. Additionally, DP-CVAE produces higher-fidelity embeddings
than DP-CGAN while requiring $5{\times}$ fewer parameters.

</details>


### [92] [Multi-Agent Reinforcement Learning for Dynamic Pricing in Supply Chains: Benchmarking Strategic Agent Behaviours under Realistically Simulated Market Conditions](https://arxiv.org/abs/2507.02698)
*Thomas Hazenberg,Yao Ma,Seyed Sahand Mohammadi Ziabari,Marijn van Rijswijk*

Main category: cs.LG

TL;DR: 研究探讨多智能体强化学习（MARL）如何改进供应链动态定价策略，对比三种MARL算法与静态规则方法，发现MARL能引入静态规则无法捕捉的战略行为。


<details>
  <summary>Details</summary>
Motivation: 传统ERP系统的静态定价方法忽略了市场参与者间的战略互动，而现有强化学习研究多为单智能体，未能模拟真实供应链的相互依赖特性。

Method: 在模拟环境中评估三种MARL算法（MADDPG、MADQN、QMIX）与静态规则基线的性能，使用真实电商交易数据和LightGBM需求预测模型。

Result: 静态规则方法公平性高但缺乏竞争动态；MADQN定价最激进，公平性最低；MADDPG平衡了市场竞争与公平性。

Conclusion: MARL能引入静态规则无法实现的战略行为，为动态定价的未来发展提供参考。

Abstract: This study investigates how Multi-Agent Reinforcement Learning (MARL) can
improve dynamic pricing strategies in supply chains, particularly in contexts
where traditional ERP systems rely on static, rule-based approaches that
overlook strategic interactions among market actors. While recent research has
applied reinforcement learning to pricing, most implementations remain
single-agent and fail to model the interdependent nature of real-world supply
chains. This study addresses that gap by evaluating the performance of three
MARL algorithms: MADDPG, MADQN, and QMIX against static rule-based baselines,
within a simulated environment informed by real e-commerce transaction data and
a LightGBM demand prediction model. Results show that rule-based agents achieve
near-perfect fairness (Jain's Index: 0.9896) and the highest price stability
(volatility: 0.024), but they fully lack competitive dynamics. Among MARL
agents, MADQN exhibits the most aggressive pricing behaviour, with the highest
volatility and the lowest fairness (0.5844). MADDPG provides a more balanced
approach, supporting market competition (share volatility: 9.5 pp) while
maintaining relatively high fairness (0.8819) and stable pricing. These
findings suggest that MARL introduces emergent strategic behaviour not captured
by static pricing rules and may inform future developments in dynamic pricing.

</details>


### [93] [Fluid Democracy in Federated Data Aggregation](https://arxiv.org/abs/2507.02710)
*Aditya Vema Reddy Kesari,Krishna Reddy Kesari*

Main category: cs.LG

TL;DR: 论文提出了一种基于共识的联邦学习协议（FedVRD），通过动态限制恶意客户端的影响并优化数据传输成本，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 减少联邦学习中不必要的数据传输成本，并提升模型权重选择的效率。

Method: 提出了一种新的流体民主协议（viscous-retained democracy），并开发了动态限制恶意客户端影响的算法（FedVRD）。

Result: FedVRD在相同假设下优于传统方法（如1p1v），且能防止影响力积累和对抗攻击。

Conclusion: FedVRD是一种高效且安全的联邦学习协议，适用于动态和对抗性环境。

Abstract: Federated learning (FL) mechanisms typically require each client to transfer
their weights to a central server, irrespective of how useful they are. In
order to avoid wasteful data transfer costs from clients to the central server,
we propose the use of consensus based protocols to identify a subset of clients
with most useful model weights at each data transfer step. First, we explore
the application of existing fluid democracy protocols to FL from a performance
standpoint, comparing them with traditional one-person-one-vote (also known as
1p1v or FedAvg). We propose a new fluid democracy protocol named
viscous-retained democracy that always does better than 1p1v under the same
assumptions as existing fluid democracy protocols while also not allowing for
influence accumulation. Secondly, we identify weaknesses of fluid democracy
protocols from an adversarial lens in terms of their dependence on topology
and/ or number of adversaries required to negatively impact the global model
weights. To this effect, we propose an algorithm (FedVRD) that dynamically
limits the effect of adversaries while minimizing cost by leveraging the
delegation topology.

</details>


### [94] [A Forget-and-Grow Strategy for Deep Reinforcement Learning Scaling in Continuous Control](https://arxiv.org/abs/2507.02712)
*Zilin Kang,Chenyuan Hu,Yu Luo,Zhecheng Yuan,Ruijie Zheng,Huazhe Xu*

Main category: cs.LG

TL;DR: 论文提出FoG算法，通过遗忘早期经验和动态扩展网络容量，提升深度强化学习的样本效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习方法存在早期经验过拟合问题（primacy bias），影响样本效率和泛化性。受人类婴儿遗忘现象启发，提出FoG算法。

Method: FoG包含两个机制：1) 经验回放衰减（ER Decay），逐步减少早期经验的影响；2) 网络扩展，动态增加参数以提升模型能力。

Result: 在40多个任务的四个连续控制基准测试中，FoG表现优于现有SoTA算法（如BRO、SimBa、TD-MPC2）。

Conclusion: FoG通过模仿人类遗忘与成长机制，有效解决了深度强化学习中的primacy bias问题，提升了性能。

Abstract: Deep reinforcement learning for continuous control has recently achieved
impressive progress. However, existing methods often suffer from primacy bias,
a tendency to overfit early experiences stored in the replay buffer, which
limits an RL agent's sample efficiency and generalizability. In contrast,
humans are less susceptible to such bias, partly due to infantile amnesia,
where the formation of new neurons disrupts early memory traces, leading to the
forgetting of initial experiences. Inspired by this dual processes of
forgetting and growing in neuroscience, in this paper, we propose Forget and
Grow (FoG), a new deep RL algorithm with two mechanisms introduced. First,
Experience Replay Decay (ER Decay) "forgetting early experience", which
balances memory by gradually reducing the influence of early experiences.
Second, Network Expansion, "growing neural capacity", which enhances agents'
capability to exploit the patterns of existing data by dynamically adding new
parameters during training. Empirical results on four major continuous control
benchmarks with more than 40 tasks demonstrate the superior performance of FoG
against SoTA existing deep RL algorithms, including BRO, SimBa, and TD-MPC2.

</details>


### [95] [A Comprehensive Machine Learning Framework for Micromobility Demand Prediction](https://arxiv.org/abs/2507.02715)
*Omri Porat,Michael Fire,Eran Ben-Elia*

Main category: cs.LG

TL;DR: 本文提出了一种整合空间、时间和网络依赖性的框架，用于改进无桩电动滑板车的需求预测，显著提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 无桩电动滑板车作为环保灵活的交通工具，其需求预测对优化车队分布和基础设施规划至关重要。

Method: 提出了一种整合空间、时间和网络依赖性的框架，用于需求预测。

Result: 该框架将需求预测准确性提高了27%至49%。

Conclusion: 研究支持数据驱动的微移动管理，有助于优化车队分布、降低成本及可持续城市规划。

Abstract: Dockless e-scooters, a key micromobility service, have emerged as
eco-friendly and flexible urban transport alternatives. These services improve
first and last-mile connectivity, reduce congestion and emissions, and
complement public transport for short-distance travel. However, effective
management of these services depends on accurate demand prediction, which is
crucial for optimal fleet distribution and infrastructure planning. While
previous studies have focused on analyzing spatial or temporal factors in
isolation, this study introduces a framework that integrates spatial, temporal,
and network dependencies for improved micromobility demand forecasting. This
integration enhances accuracy while providing deeper insights into urban
micromobility usage patterns. Our framework improves demand prediction accuracy
by 27 to 49% over baseline models, demonstrating its effectiveness in capturing
micromobility demand patterns. These findings support data-driven micromobility
management, enabling optimized fleet distribution, cost reduction, and
sustainable urban planning.

</details>


### [96] [Hierarchical Multi-Label Contrastive Learning for Protein-Protein Interaction Prediction Across Organisms](https://arxiv.org/abs/2507.02724)
*Shiyi Liu,Buwen Liang,Yuetong Fang,Zixuan Jiang,Renjing Xu*

Main category: cs.LG

TL;DR: HIPPO是一种基于层次对比学习的蛋白质-蛋白质相互作用预测框架，通过多层次的生物表示匹配和对比损失函数，实现了跨物种的高性能预测和零样本迁移能力。


<details>
  <summary>Details</summary>
Motivation: 解决蛋白质-蛋白质相互作用预测中数据异构性和跨物种迁移的挑战，尤其是在数据稀疏或分布不平衡的情况下。

Method: 提出层次对比学习框架HIPPO，结合蛋白质序列和层次属性，通过多层次的表示匹配和数据驱动的惩罚机制优化嵌入空间。

Result: 在基准数据集上表现优于现有方法，尤其在低数据量和零样本迁移场景下表现出色。

Conclusion: HIPPO为跨物种PPI预测提供了统一框架，特别适用于数据稀疏或分布不平衡的场景。

Abstract: Recent advances in AI for science have highlighted the power of contrastive
learning in bridging heterogeneous biological data modalities. Building on this
paradigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction
across Organisms), a hierarchical contrastive framework for protein-protein
interaction(PPI) prediction, where protein sequences and their hierarchical
attributes are aligned through multi-tiered biological representation matching.
The proposed approach incorporates hierarchical contrastive loss functions that
emulate the structured relationship among functional classes of proteins. The
framework adaptively incorporates domain and family knowledge through a
data-driven penalty mechanism, enforcing consistency between the learned
embedding space and the intrinsic hierarchy of protein functions. Experiments
on benchmark datasets demonstrate that HIPPO achieves state-of-the-art
performance, outperforming existing methods and showing robustness in low-data
regimes. Notably, the model demonstrates strong zero-shot transferability to
other species without retraining, enabling reliable PPI prediction and
functional inference even in less characterized or rare organisms where
experimental data are limited. Further analysis reveals that hierarchical
feature fusion is critical for capturing conserved interaction determinants,
such as binding motifs and functional annotations. This work advances
cross-species PPI prediction and provides a unified framework for interaction
prediction in scenarios with sparse or imbalanced multi-species data.

</details>


### [97] [Classification by Separating Hypersurfaces: An Entropic Approach](https://arxiv.org/abs/2507.02732)
*Argimiro Arratia,Mahmoud El Daou,Henryk Gzyl*

Main category: cs.LG

TL;DR: 提出了一种基于熵最小化的新型分类方法，通过在高维超立方体中搜索参数向量，支持线性和非线性数据分离。


<details>
  <summary>Details</summary>
Motivation: 解决传统分类方法（如支持向量机和梯度下降）在处理复杂决策边界时的局限性。

Method: 在高维超立方体中搜索参数向量，并通过熵最小化优化目标函数，扩展到多项式曲面。

Result: 数值实验表明该方法在处理线性和非线性分类任务时高效且灵活。

Conclusion: 该方法为分类问题提供了一种鲁棒的替代方案，适用于复杂决策边界。

Abstract: We consider the following classification problem: Given a population of
individuals characterized by a set of attributes represented as a vector in
${\mathbb R}^N$, the goal is to find a hyperplane in ${\mathbb R}^N$ that
separates two sets of points corresponding to two distinct classes. This
problem, with a history dating back to the perceptron model, remains central to
machine learning. In this paper we propose a novel approach by searching for a
vector of parameters in a bounded $N$-dimensional hypercube centered at the
origin and a positive vector in ${\mathbb R}^M$, obtained through the
minimization of an entropy-based function defined over the space of unknown
variables. The method extends to polynomial surfaces, allowing the separation
of data points by more complex decision boundaries. This provides a robust
alternative to traditional linear or quadratic optimization techniques, such as
support vector machines and gradient descent. Numerical experiments demonstrate
the efficiency and versatility of the method in handling diverse classification
tasks, including linear and non-linear separability.

</details>


### [98] [Fast and Simplex: 2-Simplicial Attention in Triton](https://arxiv.org/abs/2507.02754)
*Aurko Roy,Timothy Chou,Sai Surya Duvvuri,Sijia Chen,Jiecao Yu,Xiaodong Wang,Manzil Zaheer,Rohan Anil*

Main category: cs.LG

TL;DR: 论文研究了2-单纯形Transformer架构，通过改进注意力机制提升令牌效率，在数学、编程和逻辑任务中优于标准Transformer。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型依赖海量数据，但现有缩放定律假设数据无限且计算受限，实际中令牌效率更重要。

Method: 提出2-单纯形Transformer，将标准点积注意力推广为三线性函数，并通过Triton内核高效实现。

Result: 在相同令牌预算下，2-单纯形Transformer在数学、编程和推理任务中表现更优，并改变了知识任务的缩放定律指数。

Conclusion: 2-单纯形Transformer通过提升令牌效率，为计算资源有限的任务提供了更优的架构选择。

Abstract: Recent work has shown that training loss scales as a power law with both
model size and the number of tokens, and that achieving compute-optimal models
requires scaling model size and token count together. However, these scaling
laws assume an infinite supply of data and apply primarily in compute-bound
settings. As modern large language models increasingly rely on massive
internet-scale datasets, the assumption that they are compute-bound is becoming
less valid. This shift highlights the need for architectures that prioritize
token efficiency.
  In this work, we investigate the use of the 2-simplicial Transformer, an
architecture that generalizes standard dot-product attention to trilinear
functions through an efficient Triton kernel implementation. We demonstrate
that the 2-simplicial Transformer achieves better token efficiency than
standard Transformers: for a fixed token budget, similarly sized models
outperform their dot-product counterparts on tasks involving mathematics,
coding, reasoning, and logic. We quantify these gains by demonstrating that
$2$-simplicial attention changes the exponent in the scaling laws for knowledge
and reasoning tasks compared to dot product attention.

</details>


### [99] [Contextual Online Pricing with (Biased) Offline Data](https://arxiv.org/abs/2507.02762)
*Yixuan Zhang,Ruihao Zhu,Qiaomin Xie*

Main category: cs.LG

TL;DR: 论文研究了带有偏置离线数据的上下文在线定价问题，提出了基于乐观面对不确定性（OFU）的策略，实现了最优的遗憾界，并扩展到一般价格弹性情况。


<details>
  <summary>Details</summary>
Motivation: 解决在存在偏置离线数据时，如何优化在线定价策略的统计复杂性和遗憾界的问题。

Method: 使用OFU策略，结合离线数据的偏置界限、数据量和分散性，推导出最优的遗憾界。

Result: 在标量价格弹性情况下，实现了实例依赖的遗憾界；在一般价格弹性情况下，实现了最坏情况下的最优遗憾界。

Conclusion: 论文首次提供了在偏置离线数据下上下文定价的紧遗憾界，方法可推广到带偏置离线数据的随机线性bandit问题。

Abstract: We study contextual online pricing with biased offline data. For the scalar
price elasticity case, we identify the instance-dependent quantity $\delta^2$
that measures how far the offline data lies from the (unknown) online optimum.
We show that the time length $T$, bias bound $V$, size $N$ and dispersion
$\lambda_{\min}(\hat{\Sigma})$ of the offline data, and $\delta^2$ jointly
determine the statistical complexity. An Optimism-in-the-Face-of-Uncertainty
(OFU) policy achieves a minimax-optimal, instance-dependent regret bound
$\tilde{\mathcal{O}}\big(d\sqrt{T} \wedge (V^2T +
\frac{dT}{\lambda_{\min}(\hat{\Sigma}) + (N \wedge T) \delta^2})\big)$. For
general price elasticity, we establish a worst-case, minimax-optimal rate
$\tilde{\mathcal{O}}\big(d\sqrt{T} \wedge (V^2T + \frac{dT
}{\lambda_{\min}(\hat{\Sigma})})\big)$ and provide a generalized OFU algorithm
that attains it. When the bias bound $V$ is unknown, we design a robust variant
that always guarantees sub-linear regret and strictly improves on purely online
methods whenever the exact bias is small. These results deliver the first tight
regret guarantees for contextual pricing in the presence of biased offline
data. Our techniques also transfer verbatim to stochastic linear bandits with
biased offline data, yielding analogous bounds.

</details>


### [100] [Understanding and Improving Length Generalization in Recurrent Models](https://arxiv.org/abs/2507.02782)
*Ricardo Buitrago Ruiz,Albert Gu*

Main category: cs.LG

TL;DR: 论文研究了循环模型在长度泛化上的失败原因，提出了未探索状态假说，并通过简单训练干预显著提升了模型的长序列处理能力。


<details>
  <summary>Details</summary>
Motivation: 循环模型理论上能处理任意长序列，但实际性能在超出训练上下文长度时会显著下降。本文旨在探究其失败原因并提出解决方案。

Method: 通过实证和理论分析支持未探索状态假说，并测试了初始化状态的高斯噪声和不同输入序列最终状态等训练干预措施。

Result: 仅用500步后训练（约0.1%的预训练预算），干预措施使模型能处理比训练上下文长多个数量级的序列（如2k→128k），并在长上下文任务中表现更优。

Conclusion: 简单高效的训练干预可显著提升循环模型的长度泛化能力。

Abstract: Recently, recurrent models such as state space models and linear attention
have become popular due to their linear complexity in the sequence length.
Thanks to their recurrent nature, in principle they can process arbitrarily
long sequences, but their performance sometimes drops considerably beyond their
training context lengths-i.e. they fail to length generalize. In this work, we
provide comprehensive empirical and theoretical analysis to support the
unexplored states hypothesis, which posits that models fail to length
generalize when during training they are only exposed to a limited subset of
the distribution of all attainable states (i.e. states that would be attained
if the recurrence was applied to long sequences). Furthermore, we investigate
simple training interventions that aim to increase the coverage of the states
that the model is trained on, e.g. by initializing the state with Gaussian
noise or with the final state of a different input sequence. With only 500
post-training steps ($\sim 0.1\%$ of the pre-training budget), these
interventions enable length generalization for sequences that are orders of
magnitude longer than the training context (e.g. $2k\longrightarrow 128k$) and
show improved performance in long context tasks, thus presenting a simple and
efficient way to enable robust length generalization in general recurrent
models.

</details>


### [101] [In-Training Multicalibrated Survival Analysis for Healthcare via Constrained Optimization](https://arxiv.org/abs/2507.02807)
*Thiti Suttaket,Stanley Kok*

Main category: cs.LG

TL;DR: GRADUATE是一种生存分析模型，通过多校准确保所有子群体的预测概率接近真实概率，优化了校准和区分能力。


<details>
  <summary>Details</summary>
Motivation: 现有生存模型通常仅在群体层面校准，可能导致少数子群体校准不佳，影响临床决策。

Method: GRADUATE将多校准建模为约束优化问题，优化校准和区分能力，数学证明其解接近最优且可行。

Result: 在真实临床数据集上，GRADUATE优于现有基线模型，展示了其有效性。

Conclusion: GRADUATE解决了少数子群体校准问题，平衡了校准和区分能力，适用于临床决策。

Abstract: Survival analysis is an important problem in healthcare because it models the
relationship between an individual's covariates and the onset time of an event
of interest (e.g., death). It is important for survival models to be
well-calibrated (i.e., for their predicted probabilities to be close to
ground-truth probabilities) because badly calibrated systems can result in
erroneous clinical decisions. Existing survival models are typically calibrated
at the population level only, and thus run the risk of being poorly calibrated
for one or more minority subpopulations. We propose a model called GRADUATE
that achieves multicalibration by ensuring that all subpopulations are
well-calibrated too. GRADUATE frames multicalibration as a constrained
optimization problem, and optimizes both calibration and discrimination
in-training to achieve a good balance between them. We mathematically prove
that the optimization method used yields a solution that is both near-optimal
and feasible with high probability. Empirical comparisons against
state-of-the-art baselines on real-world clinical datasets demonstrate
GRADUATE's efficacy. In a detailed analysis, we elucidate the shortcomings of
the baselines vis-a-vis GRADUATE's strengths.

</details>


### [102] [Replicable Distribution Testing](https://arxiv.org/abs/2507.02814)
*Ilias Diakonikolas,Jingyi Gao,Daniel Kane,Sihan Liu,Christopher Ye*

Main category: cs.LG

TL;DR: 论文系统研究了算法可重复性框架下的分布测试问题，提出了新的可重复算法用于测试离散分布的接近性和独立性，并开发了新的下界证明方法。


<details>
  <summary>Details</summary>
Motivation: 研究在算法可重复性框架下分布测试的样本复杂度，填补现有研究的空白。

Method: 开发新的可重复算法测试离散分布的接近性和独立性；提出新的下界证明方法。

Result: 建立了近乎最优的样本复杂度下界，解决了先前工作中的开放性问题。

Conclusion: 论文为可重复分布测试提供了理论和算法支持，具有广泛的应用潜力。

Abstract: We initiate a systematic investigation of distribution testing in the
framework of algorithmic replicability. Specifically, given independent samples
from a collection of probability distributions, the goal is to characterize the
sample complexity of replicably testing natural properties of the underlying
distributions. On the algorithmic front, we develop new replicable algorithms
for testing closeness and independence of discrete distributions. On the lower
bound front, we develop a new methodology for proving sample complexity lower
bounds for replicable testing that may be of broader interest. As an
application of our technique, we establish near-optimal sample complexity lower
bounds for replicable uniformity testing -- answering an open question from
prior work -- and closeness testing.

</details>


### [103] [ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning](https://arxiv.org/abs/2507.02834)
*Ruiyang Zhou,Shuozhe Li,Amy Zhang,Liu Leqi*

Main category: cs.LG

TL;DR: 论文提出了一种名为ExPO的新方法，通过结合真实答案生成高质量样本，解决了传统强化学习后训练中样本依赖初始模型能力的局限性，提升了推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习后训练方法依赖模型初始能力生成正样本，难以解决模型初始失败的推理任务。

Method: 提出了Self-Explanation Policy Optimization (ExPO)，通过结合真实答案生成符合当前策略且能提升正确预测概率的样本。

Result: ExPO在推理任务（如MATH level-5）中表现优于基于专家演示的方法，提升了学习效率和最终性能。

Conclusion: ExPO通过高效探索和样本优化，显著提升了模型在挑战性推理任务中的表现。

Abstract: Recent advances in large language models have been driven by reinforcement
learning (RL)-style post-training, which improves reasoning by optimizing model
outputs based on reward or preference signals. GRPO-style approaches implement
this by using self-generated samples labeled by an outcome-based verifier.
However, these methods depend heavily on the model's initial ability to produce
positive samples. They primarily refine what the model already knows
(distribution sharpening) rather than enabling the model to solve problems
where it initially fails. This limitation is especially problematic in
early-stage RL training and on challenging reasoning tasks, where positive
samples are unlikely to be generated. To unlock reasoning ability in such
settings, the model must explore new reasoning trajectories beyond its current
output distribution. Such exploration requires access to sufficiently good
positive samples to guide the learning. While expert demonstrations seem like a
natural solution, we find that they are often ineffective in RL post-training.
Instead, we identify two key properties of effective positive samples: they
should (1) be likely under the current policy, and (2) increase the model's
likelihood of predicting the correct answer. Based on these insights, we
propose $\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and
modular framework that generates such samples by conditioning on the
ground-truth answer. ExPO enables efficient exploration and guides the model to
produce reasoning trajectories more aligned with its policy than expert-written
CoTs, while ensuring higher quality than its own (incorrect) samples.
Experiments show that ExPO improves both learning efficiency and final
performance on reasoning benchmarks, surpassing expert-demonstration-based
methods in challenging settings such as MATH level-5, where the model initially
struggles the most.

</details>


### [104] [LLM-Driven Treatment Effect Estimation Under Inference Time Text Confounding](https://arxiv.org/abs/2507.02843)
*Yuchen Ma,Dennis Frauen,Jonas Schweisthal,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 论文提出了一种解决训练与推断数据不一致导致的治疗效应估计偏差的方法，结合大语言模型和双重稳健学习器。


<details>
  <summary>Details</summary>
Motivation: 临床实践中，治疗效应估计模型在训练时使用结构化数据，而推断时依赖文本描述，导致偏差。

Method: 提出新框架，利用大语言模型和双重稳健学习器，解决推断时文本混杂问题。

Result: 实验证明框架在实际应用中的有效性。

Conclusion: 该框架能有效减少因数据不一致导致的治疗效应估计偏差。

Abstract: Estimating treatment effects is crucial for personalized decision-making in
medicine, but this task faces unique challenges in clinical practice. At
training time, models for estimating treatment effects are typically trained on
well-structured medical datasets that contain detailed patient information.
However, at inference time, predictions are often made using textual
descriptions (e.g., descriptions with self-reported symptoms), which are
incomplete representations of the original patient information. In this work,
we make three contributions. (1) We show that the discrepancy between the data
available during training time and inference time can lead to biased estimates
of treatment effects. We formalize this issue as an inference time text
confounding problem, where confounders are fully observed during training time
but only partially available through text at inference time. (2) To address
this problem, we propose a novel framework for estimating treatment effects
that explicitly accounts for inference time text confounding. Our framework
leverages large language models together with a custom doubly robust learner to
mitigate biases caused by the inference time text confounding. (3) Through a
series of experiments, we demonstrate the effectiveness of our framework in
real-world applications.

</details>


### [105] [MvHo-IB: Multi-View Higher-Order Information Bottleneck for Brain Disorder Diagnosis](https://arxiv.org/abs/2507.02847)
*Kunyu Zhang,Qiang Li,Shujian Yu*

Main category: cs.LG

TL;DR: MvHo-IB是一种多视图学习框架，通过结合高阶相互作用（HOIs）和成对相互作用，提升fMRI数据的诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效提取和利用fMRI数据中的高阶相互作用，限制了诊断准确性。

Method: 提出MvHo-IB框架，结合信息论的O信息和Renyi熵估计器量化HOIs，使用Brain3DCNN编码器，并引入多视图学习信息瓶颈目标。

Result: 在三个基准fMRI数据集上，MvHo-IB表现优于现有方法，包括最新的超图技术。

Conclusion: MvHo-IB通过有效利用HOIs，显著提升了诊断性能，为fMRI数据分析提供了新工具。

Abstract: Recent evidence suggests that modeling higher-order interactions (HOIs) in
functional magnetic resonance imaging (fMRI) data can enhance the diagnostic
accuracy of machine learning systems. However, effectively extracting and
utilizing HOIs remains a significant challenge. In this work, we propose
MvHo-IB, a novel multi-view learning framework that integrates both pairwise
interactions and HOIs for diagnostic decision-making, while automatically
compressing task-irrelevant redundant information. MvHo-IB introduces several
key innovations: (1) a principled method that combines O-information from
information theory with a matrix-based Renyi alpha-order entropy estimator to
quantify and extract HOIs, (2) a purpose-built Brain3DCNN encoder to
effectively utilize these interactions, and (3) a new multi-view learning
information bottleneck objective to enhance representation learning.
Experiments on three benchmark fMRI datasets demonstrate that MvHo-IB achieves
state-of-the-art performance, significantly outperforming previous methods,
including recent hypergraph-based techniques. The implementation of MvHo-IB is
available at https://github.com/zky04/MvHo-IB.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [106] [Predicting and Explaining Customer Data Sharing in the Open Banking](https://arxiv.org/abs/2507.01987)
*João B. G. de Brito,Rodrigo Heldt,Cleo S. Silveira,Matthias Bogaert,Guilherme B. Bucco,Fernando B. Luce,João L. Becker,Filipe J. Zabala,Michel J. Anzanello*

Main category: q-fin.GN

TL;DR: 论文提出了一种预测客户在开放银行中数据共享倾向的框架，结合EMA解释行为，通过XGBoost模型实现高准确率预测，并识别了影响决策的关键特征。


<details>
  <summary>Details</summary>
Motivation: 开放银行改变了金融数据管理方式，金融机构需平衡数据流入与流出以提升竞争力，因此需预测客户数据共享行为。

Method: 采用混合数据平衡策略（ADASYN和NEARMISS）训练XGBoost模型，结合SHAP和CART进行解释性分析。

Result: 模型在数据流入和流出预测上分别达到91.39%和91.53%的准确率，关键特征包括移动渠道交易和信用相关行为。

Conclusion: 移动渠道互动和信用特征是客户数据共享行为的主要驱动因素，为金融机构在开放银行环境中提供战略指导。

Abstract: The emergence of Open Banking represents a significant shift in financial
data management, influencing financial institutions' market dynamics and
marketing strategies. This increased competition creates opportunities and
challenges, as institutions manage data inflow to improve products and services
while mitigating data outflow that could aid competitors. This study introduces
a framework to predict customers' propensity to share data via Open Banking and
interprets this behavior through Explanatory Model Analysis (EMA). Using data
from a large Brazilian financial institution with approximately 3.2 million
customers, a hybrid data balancing strategy incorporating ADASYN and NEARMISS
techniques was employed to address the infrequency of data sharing and enhance
the training of XGBoost models. These models accurately predicted customer data
sharing, achieving 91.39% accuracy for inflow and 91.53% for outflow. The EMA
phase combined the Shapley Additive Explanations (SHAP) method with the
Classification and Regression Tree (CART) technique, revealing the most
influential features on customer decisions. Key features included the number of
transactions and purchases in mobile channels, interactions within these
channels, and credit-related features, particularly credit card usage across
the national banking system. These results highlight the critical role of
mobile engagement and credit in driving customer data-sharing behaviors,
providing financial institutions with strategic insights to enhance
competitiveness and innovation in the Open Banking environment.

</details>


### [107] [Integrating Large Language Models in Financial Investments and Market Analysis: A Survey](https://arxiv.org/abs/2507.01990)
*Sedigheh Mahdavi,Jiating,Chen,Pradeep Kumar Joshi,Lina Huertas Guativa,Upmanyu Singh*

Main category: q-fin.GN

TL;DR: 本文综述了大型语言模型（LLMs）在金融决策中的应用，包括股票选择、风险评估、情感分析、交易和金融预测，并总结了其能力、挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统金融决策方法（如定量模型和基本面分析）存在局限性，LLMs能够处理大量结构化和非结构化数据，提供实时决策支持。

Method: 研究通过四大框架（LLM基础框架与流程、混合集成方法、微调与适应方法、基于代理的架构）对LLMs在金融领域的应用进行了分类综述。

Result: LLMs在金融领域展现出强大的数据处理和实时分析能力，但也面临数据质量和模型可解释性等挑战。

Conclusion: LLMs为金融决策提供了新工具，未来需进一步解决技术挑战并探索更多应用场景。

Abstract: Large Language Models (LLMs) have been employed in financial decision making,
enhancing analytical capabilities for investment strategies. Traditional
investment strategies often utilize quantitative models, fundamental analysis,
and technical indicators. However, LLMs have introduced new capabilities to
process and analyze large volumes of structured and unstructured data, extract
meaningful insights, and enhance decision-making in real-time. This survey
provides a structured overview of recent research on LLMs within the financial
domain, categorizing research contributions into four main frameworks:
LLM-based Frameworks and Pipelines, Hybrid Integration Methods, Fine-Tuning and
Adaptation Approaches, and Agent-Based Architectures. This study provides a
structured review of recent LLMs research on applications in stock selection,
risk assessment, sentiment analysis, trading, and financial forecasting. By
reviewing the existing literature, this study highlights the capabilities,
challenges, and potential directions of LLMs in financial markets.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [108] [A robust and adaptive MPC formulation for Gaussian process models](https://arxiv.org/abs/2507.02098)
*Mathieu Dubied,Amon Lahr,Melanie N. Zeilinger,Johannes Köhler*

Main category: eess.SY

TL;DR: 提出了一种基于高斯过程（GPs）和收缩度量的鲁棒自适应模型预测控制（MPC）框架，用于处理不确定非线性系统。


<details>
  <summary>Details</summary>
Motivation: 解决受有界扰动和未建模非线性影响的非线性系统的控制问题。

Method: 使用高斯过程学习不确定动态，结合收缩度量生成鲁棒预测，并融入MPC框架。

Result: 设计保证了递归可行性、鲁棒约束满足和高概率收敛到参考状态，数值实验验证了方法的有效性。

Conclusion: 该方法通过鲁棒预测和在线学习显著提升了系统性能。

Abstract: In this paper, we present a robust and adaptive model predictive control
(MPC) framework for uncertain nonlinear systems affected by bounded
disturbances and unmodeled nonlinearities. We use Gaussian Processes (GPs) to
learn the uncertain dynamics based on noisy measurements, including those
collected during system operation. As a key contribution, we derive robust
predictions for GP models using contraction metrics, which are incorporated in
the MPC formulation. The proposed design guarantees recursive feasibility,
robust constraint satisfaction and convergence to a reference state, with high
probability. We provide a numerical example of a planar quadrotor subject to
difficult-to-model ground effects, which highlights significant improvements
achieved through the proposed robust prediction method and through online
learning.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [109] [Forecasting Nigerian Equity Stock Returns Using Long Short-Term Memory Technique](https://arxiv.org/abs/2507.01964)
*Adebola K. Ojo,Ifechukwude Jude Okafor*

Main category: q-fin.ST

TL;DR: 研究使用LSTM模型预测股票市场，准确率超90%，建议未来结合其他深度学习技术。


<details>
  <summary>Details</summary>
Motivation: 投资者和分析师在预测股票回报时面临挑战，需提升预测准确性以增强信心。

Method: 使用LSTM模型分析尼日利亚证券交易所历史数据，并与CNN等模型对比。

Result: LSTM模型预测准确率超过90%，表现优于其他深度学习模型。

Conclusion: LSTM模型在金融时间序列预测中有效，建议未来研究结合其他技术构建混合模型。

Abstract: Investors and stock market analysts face major challenges in predicting stock
returns and making wise investment decisions. The predictability of equity
stock returns can boost investor confidence, but it remains a difficult task.
To address this issue, a study was conducted using a Long Short-term Memory
(LSTM) model to predict future stock market movements. The study used a
historical dataset from the Nigerian Stock Exchange (NSE), which was cleaned
and normalized to design the LSTM model. The model was evaluated using
performance metrics and compared with other deep learning models like
Artificial and Convolutional Neural Networks (CNN). The experimental results
showed that the LSTM model can predict future stock market prices and returns
with over 90% accuracy when trained with a reliable dataset. The study
concludes that LSTM models can be useful in predicting financial
time-series-related problems if well-trained. Future studies should explore
combining LSTM models with other deep learning techniques like CNN to create
hybrid models that mitigate the risks associated with relying on a single model
for future equity stock predictions.

</details>


### [110] [News Sentiment Embeddings for Stock Price Forecasting](https://arxiv.org/abs/2507.01970)
*Ayaan Qayyum*

Main category: q-fin.ST

TL;DR: 利用新闻标题数据（如WSJ）和OpenAI文本嵌入模型预测SPY股票价格，结果显示标题数据能提升预测性能至少40%。


<details>
  <summary>Details</summary>
Motivation: 探讨新闻标题对股票价格的影响，尤其是时间依赖和非时间依赖的复杂效应，以改进预测模型。

Method: 使用OpenAI文本嵌入模型和PCA提取标题特征，结合金融数据（如DXY和国债收益率），训练390多个机器学习模型。

Result: 标题数据嵌入显著提升预测性能，比无标题数据的模型至少提高40%。

Conclusion: 新闻标题数据是预测股票价格的重要特征，结合金融数据可显著提升模型性能。

Abstract: This paper will discuss how headline data can be used to predict stock
prices. The stock price in question is the SPDR S&P 500 ETF Trust, also known
as SPY that tracks the performance of the largest 500 publicly traded
corporations in the United States. A key focus is to use news headlines from
the Wall Street Journal (WSJ) to predict the movement of stock prices on a
daily timescale with OpenAI-based text embedding models used to create vector
encodings of each headline with principal component analysis (PCA) to exact the
key features. The challenge of this work is to capture the time-dependent and
time-independent, nuanced impacts of news on stock prices while handling
potential lag effects and market noise. Financial and economic data were
collected to improve model performance; such sources include the U.S. Dollar
Index (DXY) and Treasury Interest Yields. Over 390 machine-learning inference
models were trained. The preliminary results show that headline data embeddings
greatly benefit stock price prediction by at least 40% compared to training and
optimizing a machine learning system without headline data embeddings.

</details>


### [111] [DeepSupp: Attention-Driven Correlation Pattern Analysis for Dynamic Time Series Support and Resistance Levels Identification](https://arxiv.org/abs/2507.01971)
*Boris Kriuk,Logic Ng,Zarif Al Hossain*

Main category: q-fin.ST

TL;DR: DeepSupp是一种基于深度学习的支持位检测方法，通过多头注意力机制分析市场关系，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统支持与阻力位识别方法难以适应现代复杂市场，现有研究多关注价格预测而非结构识别。

Method: 采用多头注意力机制和动态相关矩阵，结合基于注意力的自编码器和DBSCAN聚类提取支持位。

Result: 在S&P 500股票上表现优于六种基线方法，在六项金融指标中达到最优。

Conclusion: DeepSupp填补了支持位检测的空白，展示了注意力架构在改进技术交易策略中的潜力。

Abstract: Support and resistance (SR) levels are central to technical analysis, guiding
traders in entry, exit, and risk management. Despite widespread use,
traditional SR identification methods often fail to adapt to the complexities
of modern, volatile markets. Recent research has introduced machine learning
techniques to address the following challenges, yet most focus on price
prediction rather than structural level identification. This paper presents
DeepSupp, a new deep learning approach for detecting financial support levels
using multi-head attention mechanisms to analyze spatial correlations and
market microstructure relationships. DeepSupp integrates advanced feature
engineering, constructing dynamic correlation matrices that capture evolving
market relationships, and employs an attention-based autoencoder for robust
representation learning. The final support levels are extracted through
unsupervised clustering, leveraging DBSCAN to identify significant price
thresholds. Comprehensive evaluations on S&P 500 tickers demonstrate that
DeepSupp outperforms six baseline methods, achieving state-of-the-art
performance across six financial metrics, including essential support accuracy
and market regime sensitivity. With consistent results across diverse market
conditions, DeepSupp addresses critical gaps in SR level detection, offering a
scalable and reliable solution for modern financial analysis. Our approach
highlights the potential of attention-based architectures to uncover nuanced
market patterns and improve technical trading strategies.

</details>


### [112] [Forecasting Labor Markets with LSTNet: A Multi-Scale Deep Learning Approach](https://arxiv.org/abs/2507.01979)
*Adam Nelson-Archer,Aleia Sen,Meena Al Hasani,Sofia Davila,Jessica Le,Omar Abbouchi*

Main category: q-fin.ST

TL;DR: 提出了一种基于深度学习的短期就业变化预测和长期行业健康评估方法，利用LSTNet处理多元时间序列数据，表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 利用劳动力市场数据预测就业变化和评估行业健康，为政策制定和企业决策提供支持。

Method: 采用LSTNet处理就业水平、工资、流动率和职位空缺等多元时间序列数据，输出7天就业预测和行业就业健康指数（IEHI）。

Result: 模型在多数行业表现优于基线模型，IEHI排名与实际就业波动高度一致。

Conclusion: 讨论了误差模式、行业特定表现，并提出了未来改进可解释性和泛化性的方向。

Abstract: We present a deep learning approach for forecasting short-term employment
changes and assessing long-term industry health using labor market data from
the U.S. Bureau of Labor Statistics. Our system leverages a Long- and
Short-Term Time-series Network (LSTNet) to process multivariate time series
data, including employment levels, wages, turnover rates, and job openings. The
model outputs both 7-day employment forecasts and an interpretable Industry
Employment Health Index (IEHI). Our approach outperforms baseline models across
most sectors, particularly in stable industries, and demonstrates strong
alignment between IEHI rankings and actual employment volatility. We discuss
error patterns, sector-specific performance, and future directions for
improving interpretability and generalization.

</details>


### [113] [Detecting Fraud in Financial Networks: A Semi-Supervised GNN Approach with Granger-Causal Explanations](https://arxiv.org/abs/2507.01980)
*Linh Nguyen,Marcel Boersma,Erman Acar*

Main category: q-fin.ST

TL;DR: SAGE-FIN是一种基于半监督图神经网络的金融欺诈检测方法，通过Granger因果关系提供解释性。


<details>
  <summary>Details</summary>
Motivation: 金融欺诈每年造成巨大损失，但传统机器学习方法因数据稀疏性和模型不透明性难以有效应用。

Method: 提出SAGE-FIN，利用半监督图神经网络处理稀疏标记数据，并通过Granger因果关系解释检测结果。

Result: 在真实数据集Elliptic++上验证了SAGE-FIN的有效性，无需网络结构假设即可提供解释性。

Conclusion: SAGE-FIN结合了半监督学习和可解释性，为金融欺诈检测提供了可行解决方案。

Abstract: Fraudulent activity in the financial industry costs billions annually.
Detecting fraud, therefore, is an essential yet technically challenging task
that requires carefully analyzing large volumes of data. While machine learning
(ML) approaches seem like a viable solution, applying them successfully is not
so easy due to two main challenges: (1) the sparsely labeled data, which makes
the training of such approaches challenging (with inherent labeling costs), and
(2) lack of explainability for the flagged items posed by the opacity of ML
models, that is often required by business regulations. This article proposes
SAGE-FIN, a semi-supervised graph neural network (GNN) based approach with
Granger causal explanations for Financial Interaction Networks. SAGE-FIN learns
to flag fraudulent items based on weakly labeled (or unlabelled) data points.
To adhere to regulatory requirements, the flagged items are explained by
highlighting related items in the network using Granger causality. We
empirically validate the favorable performance of SAGE-FIN on a real-world
dataset, Bipartite Edge-And-Node Attributed financial network (Elliptic++),
with Granger-causal explanations for the identified fraudulent items without
any prior assumption on the network structure.

</details>


### [114] [NGAT: A Node-level Graph Attention Network for Long-term Stock Prediction](https://arxiv.org/abs/2507.02018)
*Yingjie Niu,Mingchuan Zhao,Valerio Poti,Ruihai Dong*

Main category: q-fin.ST

TL;DR: 论文提出了一种针对公司关系图的节点级图注意力网络（NGAT），解决了现有方法在下游任务设计、模型复杂度和图结构比较方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有图表示学习方法在金融应用中存在三个主要问题：下游任务设计限制了关系信息的优势、股票预测图模型过于复杂且泛化能力差、公司关系图构建缺乏有效的结构比较。

Method: 提出长期股票预测任务，并设计了一种专为公司关系图优化的节点级图注意力网络（NGAT）。

Result: 在两个数据集上的实验证明了所提任务和模型的有效性。

Conclusion: NGAT解决了现有方法的局限性，实验验证了其有效性，项目已开源以促进复现和未来研究。

Abstract: Graph representation learning methods have been widely adopted in financial
applications to enhance company representations by leveraging inter-firm
relationships. However, current approaches face three key challenges: (1) The
advantages of relational information are obscured by limitations in downstream
task designs; (2) Existing graph models specifically designed for stock
prediction often suffer from excessive complexity and poor generalization; (3)
Experience-based construction of corporate relationship graphs lacks effective
comparison of different graph structures. To address these limitations, we
propose a long-term stock prediction task and develop a Node-level Graph
Attention Network (NGAT) specifically tailored for corporate relationship
graphs. Furthermore, we experimentally demonstrate the limitations of existing
graph comparison methods based on model downstream task performance.
Experimental results across two datasets consistently demonstrate the
effectiveness of our proposed task and model. The project is publicly available
on GitHub to encourage reproducibility and future research.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [115] [An Easy Proof of a Weak Version of Chernoff inequality](https://arxiv.org/abs/2507.02759)
*Sariel Har-Peled*

Main category: math.PR

TL;DR: 证明了Chernoff不等式的一个简单但非常弱的版本，即在6M次公平硬币投掷中，最多得到M次正面的概率不超过1/2^M。


<details>
  <summary>Details</summary>
Motivation: 为了展示Chernoff不等式的一个简化版本，便于理解和应用。

Method: 通过数学推导和概率论的基本原理，证明了在6M次公平硬币投掷中，最多M次正面的概率上限。

Result: 证明了概率上限为1/2^M。

Conclusion: 提供了一个简单但有用的概率界限，适用于某些特定场景。

Abstract: We prove an easy but very weak version of Chernoff inequality. Namely, that
the probability that in $6M$ throws of a fair coin, one gets at most $M$ heads
is $\leq 1/2^M$.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [116] [Towards Bio-Inspired Robotic Trajectory Planning via Self-Supervised RNN](https://arxiv.org/abs/2507.02171)
*Miroslav Cibula,Kristína Malinovská,Matthias Kerzel*

Main category: cs.RO

TL;DR: 论文提出了一种基于认知启发的自监督学习方案，用于机器人轨迹规划，通过递归架构生成轨迹模型，避免了传统采样规划的高计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统采样规划方法计算成本高，而完全监督方法仅模仿轨迹，无法学习是否成功到达目标。本文旨在提出一种更高效且能自适应学习的轨迹规划方法。

Method: 采用自监督学习方案，基于递归架构构建轨迹模型，利用正向和逆向运动学模型生成轨迹。

Result: 模型能够仅通过给定的正向和逆向运动学模型学习生成轨迹，适用于需要自适应解决方案的复杂操作任务。

Conclusion: 该方法为复杂机器人操作任务提供了一种高效且自适应的轨迹规划解决方案。

Abstract: Trajectory planning in robotics is understood as generating a sequence of
joint configurations that will lead a robotic agent, or its manipulator, from
an initial state to the desired final state, thus completing a manipulation
task while considering constraints like robot kinematics and the environment.
Typically, this is achieved via sampling-based planners, which are
computationally intensive. Recent advances demonstrate that trajectory planning
can also be performed by supervised sequence learning of trajectories, often
requiring only a single or fixed number of passes through a neural
architecture, thus ensuring a bounded computation time. Such fully supervised
approaches, however, perform imitation learning; they do not learn based on
whether the trajectories can successfully reach a goal, but try to reproduce
observed trajectories. In our work, we build on this approach and propose a
cognitively inspired self-supervised learning scheme based on a recurrent
architecture for building a trajectory model. We evaluate the feasibility of
the proposed method on a task of kinematic planning for a robotic arm. The
results suggest that the model is able to learn to generate trajectories only
using given paired forward and inverse kinematics models, and indicate that
this novel method could facilitate planning for more complex manipulation tasks
requiring adaptive solutions.

</details>


### [117] [cVLA: Towards Efficient Camera-Space VLAs](https://arxiv.org/abs/2507.02190)
*Max Argus,Jelena Bratulic,Houman Masnavi,Maxim Velikanov,Nick Heppert,Abhinav Valada,Thomas Brox*

Main category: cs.RO

TL;DR: 提出了一种基于视觉语言动作（VLA）的轻量级模型，通过预测轨迹路径点而非低级控制信号，实现高效训练和机器人无关性。


<details>
  <summary>Details</summary>
Motivation: 解决传统VLA模型训练成本高的问题，同时探索深度图像和解码策略的潜力。

Method: 利用视觉语言模型（VLM）在2D图像上的性能，直接推断机器人末端执行器的位姿，并采用轻量级的下一个标记预测架构。

Result: 模型在模拟和真实数据上均表现出色，具备良好的模拟到现实的迁移能力。

Conclusion: 该方法在轻量级设计下仍能生成可执行的机器人轨迹，为复杂任务提供了高效解决方案。

Abstract: Vision-Language-Action (VLA) models offer a compelling framework for tackling
complex robotic manipulation tasks, but they are often expensive to train. In
this paper, we propose a novel VLA approach that leverages the competitive
performance of Vision Language Models (VLMs) on 2D images to directly infer
robot end-effector poses in image frame coordinates. Unlike prior VLA models
that output low-level controls, our model predicts trajectory waypoints, making
it both more efficient to train and robot embodiment agnostic. Despite its
lightweight design, our next-token prediction architecture effectively learns
meaningful and executable robot trajectories. We further explore the
underutilized potential of incorporating depth images, inference-time
techniques such as decoding strategies, and demonstration-conditioned action
generation. Our model is trained on a simulated dataset and exhibits strong
sim-to-real transfer capabilities. We evaluate our approach using a combination
of simulated and real data, demonstrating its effectiveness on a real robotic
system.

</details>


### [118] [Path Planning using a One-shot-sampling Skeleton Map](https://arxiv.org/abs/2507.02328)
*Gabriel O. Flores-Aquino,Octavio Gutierrez-Frias,Juan Irving Vasquez*

Main category: cs.RO

TL;DR: 提出了一种基于深度去噪自编码器（DDAE）的路径规划方法SkelUnet，用于高效生成骨架地图，平衡路径安全性、响应时间和长度。


<details>
  <summary>Details</summary>
Motivation: 传统骨架化算法资源消耗大，且主要面向图像处理，需要一种更高效的方法来生成骨架地图以支持路径规划。

Method: 利用基于U-Net架构的DDAE（SkelUnet）生成骨架地图，并通过一次性采样（OSS）探索工作空间。

Result: 在12,500张地图上训练和测试，并在250张新地图上验证，SkelUnet显著提高了路径安全性、连通性和处理效率。

Conclusion: SkelUnet方法适用于结构化环境中的移动服务机器人，提供了更安全、高效的路径规划解决方案。

Abstract: Path planning algorithms aim to compute a collision-free path, and many works
focus on finding the optimal distance path. However, for some applications, a
more suitable approach is to balance response time, safety of the paths, and
path length. In this context, a skeleton map is a useful tool in graph-based
schemes, as it provides an intrinsic representation of free configuration
space. However, skeletonization algorithms are very resource-intensive, being
primarily oriented towards image processing tasks. We propose an efficient
path-planning methodology that finds safe paths within an acceptable processing
time. This methodology leverages a Deep Denoising Auto-Encoder (DDAE) based on
U-Net architecture to compute a skeletonized version of the navigation map,
which we refer to as SkelUnet. The SkelUnet network facilitates exploration of
the entire workspace through one-shot sampling (OSS), as opposed to the
iterative process used by exact algorithms or the probabilistic sampling
process. SkelUnet is trained and tested on a dataset consisting of 12,500
bi-dimensional dungeon maps. The motion planning methodology is evaluated in a
simulation environment for an Unmanned Aerial Vehicle (UAV) using 250
previously unseen maps, and assessed with various navigation metrics to
quantify the navigability of the computed paths. The results demonstrate that
using SkelUnet to construct a roadmap offers significant advantages, such as
connecting all regions of free workspace, providing safer paths, and reducing
processing times. These characteristics make this method particularly suitable
for mobile service robots in structured environments.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [119] [Accelerated Portfolio Optimization and Option Pricing with Reinforcement Learning](https://arxiv.org/abs/2507.01972)
*Hadi Keramati,Samaneh Jazayeri*

Main category: q-fin.PM

TL;DR: 提出了一种基于强化学习的框架，用于优化迭代求解器中块预条件子的大小，以加速收敛并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决传统预条件技术需要手动调参的问题，特别是在投资组合优化和期权定价中的大规模线性系统中。

Method: 利用强化学习动态调整块预条件子的大小，优化迭代求解器的收敛速度。

Result: 在真实投资组合优化矩阵上的测试表明，该框架显著加速了收敛并减少了计算成本。

Conclusion: 该加速求解器支持动态投资组合分配和实时期权定价中的快速决策。

Abstract: We present a reinforcement learning (RL)-driven framework for optimizing
block-preconditioner sizes in iterative solvers used in portfolio optimization
and option pricing. The covariance matrix in portfolio optimization or the
discretization of differential operators in option pricing models lead to large
linear systems of the form $\mathbf{A}\textbf{x}=\textbf{b}$. Direct inversion
of high-dimensional portfolio or fine-grid option pricing incurs a significant
computational cost. Therefore, iterative methods are usually used for
portfolios in real-world situations. Ill-conditioned systems, however, suffer
from slow convergence. Traditional preconditioning techniques often require
problem-specific parameter tuning. To overcome this limitation, we rely on RL
to dynamically adjust the block-preconditioner sizes and accelerate iterative
solver convergence. Evaluations on a suite of real-world portfolio optimization
matrices demonstrate that our RL framework can be used to adjust
preconditioning and significantly accelerate convergence and reduce
computational cost. The proposed accelerated solver supports faster
decision-making in dynamic portfolio allocation and real-time option pricing.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [120] [DNN-Based Precoding in RIS-Aided mmWave MIMO Systems With Practical Phase Shift](https://arxiv.org/abs/2507.02824)
*Po-Heng Chou,Ching-Wen Chen,Wan-Jen Huang,Walid Saad,Yu Tsao,Ronald Y. Chang*

Main category: eess.SP

TL;DR: 论文研究了毫米波MIMO系统中预编码设计，利用RIS增强传输效率，提出基于DNN的快速码字选择方法以降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统穷举搜索方法在连续相位偏移下计算量大且耗时，需更高效的方法优化RIS辅助系统的性能。

Method: 采用置换离散傅里叶变换向量设计码本，并开发训练DNN模型以加速码字选择。

Result: 仿真表明，DNN在测试阶段能保持次优频谱效率，即使终端与RIS距离变化。

Conclusion: DNN在RIS辅助系统中具有潜力，能显著提升性能并降低计算复杂度。

Abstract: In this paper, the precoding design is investigated for maximizing the
throughput of millimeter wave (mmWave) multiple-input multiple-output (MIMO)
systems with obstructed direct communication paths. In particular, a
reconfigurable intelligent surface (RIS) is employed to enhance MIMO
transmissions, considering mmWave characteristics related to line-of-sight
(LoS) and multipath effects. The traditional exhaustive search (ES) for optimal
codewords in the continuous phase shift is computationally intensive and
time-consuming. To reduce computational complexity, permuted discrete Fourier
transform (DFT) vectors are used for finding codebook design, incorporating
amplitude responses for practical or ideal RIS systems. However, even if the
discrete phase shift is adopted in the ES, it results in significant
computation and is time-consuming. Instead, the trained deep neural network
(DNN) is developed to facilitate faster codeword selection. Simulation results
show that the DNN maintains sub-optimal spectral efficiency even as the
distance between the end-user and the RIS has variations in the testing phase.
These results highlight the potential of DNN in advancing RIS-aided systems.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [121] [Adaptive Iterative Soft-Thresholding Algorithm with the Median Absolute Deviation](https://arxiv.org/abs/2507.02084)
*Yining Feng,Ivan Selesnick*

Main category: stat.ML

TL;DR: 本文对自适应ISTA算法进行了理论分析，证明了其固定点的性质、局部线性收敛和全局收敛行为。


<details>
  <summary>Details</summary>
Motivation: 尽管自适应ISTA在实践中表现良好，但缺乏理论支持，本文旨在填补这一空白。

Method: 采用基于中位数绝对偏差的阈值策略，分析自适应ISTA的固定点性质、收敛性和稳定性。

Result: 证明了算法的尺度等变性、非唯一性、局部稳定性、局部线性收敛和全局收敛。

Conclusion: 自适应ISTA在理论和实践上均具有可靠性和有效性。

Abstract: The adaptive Iterative Soft-Thresholding Algorithm (ISTA) has been a popular
algorithm for finding a desirable solution to the LASSO problem without
explicitly tuning the regularization parameter $\lambda$. Despite that the
adaptive ISTA is a successful practical algorithm, few theoretical results
exist. In this paper, we present the theoretical analysis on the adaptive ISTA
with the thresholding strategy of estimating noise level by median absolute
deviation. We show properties of the fixed points of the algorithm, including
scale equivariance, non-uniqueness, and local stability, prove the local linear
convergence guarantee, and show its global convergence behavior.

</details>


### [122] [Hybrid least squares for learning functions from highly noisy data](https://arxiv.org/abs/2507.02215)
*Ben Adcock,Bernhard Hientzsch,Akil Narayan,Yiming Xu*

Main category: stat.ML

TL;DR: 提出了一种结合Christoffel采样和最优实验设计的混合方法，用于高效估计条件期望，尤其在噪声较大的数据中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在小噪声情况下表现良好，但在大噪声情况下效果不佳，需要一种更高效的条件期望估计方法。

Method: 结合Christoffel采样和最优实验设计，提出混合算法，并扩展到凸约束和随机场期望的场景。

Result: 算法在样本点生成和噪声抑制方面具有最优性，计算效率和样本复杂度优于现有方法。

Conclusion: 理论分析和数值实验验证了方法的有效性，尤其在计算金融的随机模拟问题中表现优异。

Abstract: Motivated by the need for efficient estimation of conditional expectations,
we consider a least-squares function approximation problem with heavily
polluted data. Existing methods that are powerful in the small noise regime are
suboptimal when large noise is present. We propose a hybrid approach that
combines Christoffel sampling with certain types of optimal experimental design
to address this issue. We show that the proposed algorithm enjoys appropriate
optimality properties for both sample point generation and noise mollification,
leading to improved computational efficiency and sample complexity compared to
existing methods. We also extend the algorithm to convex-constrained settings
with similar theoretical guarantees. When the target function is defined as the
expectation of a random field, we extend our approach to leverage adaptive
random subspaces and establish results on the approximation capacity of the
adaptive procedure. Our theoretical findings are supported by numerical studies
on both synthetic data and on a more challenging stochastic simulation problem
in computational finance.

</details>


### [123] [Transfer Learning for Matrix Completion](https://arxiv.org/abs/2507.02248)
*Dali Liu,Haolei Weng*

Main category: stat.ML

TL;DR: 本文研究了矩阵补全设置下的知识迁移，通过利用辅助数据提升低秩目标矩阵的估计，提出了一种基于先验信息的迁移学习方法，并证明了其最优性。


<details>
  <summary>Details</summary>
Motivation: 在矩阵补全任务中，如何利用辅助数据（源数据集）提升目标矩阵的估计精度是一个重要问题。本文旨在通过迁移学习方法解决这一问题。

Method: 提出了一种基于先验信息的迁移学习流程，利用源数据集与目标矩阵的相似性提升估计效果，并通过先进的浓度不等式优化收敛速度。

Result: 当源矩阵与目标矩阵足够接近时，该方法优于传统单目标数据方法，且通过检测程序可有效识别相关源数据集。

Conclusion: 本文方法在理论和实验中均表现出优越性，为矩阵补全中的知识迁移提供了有效解决方案。

Abstract: In this paper, we explore the knowledge transfer under the setting of matrix
completion, which aims to enhance the estimation of a low-rank target matrix
with auxiliary data available. We propose a transfer learning procedure given
prior information on which source datasets are favorable. We study its
convergence rates and prove its minimax optimality. Our analysis reveals that
with the source matrices close enough to the target matrix, out method
outperforms the traditional method using the single target data. In particular,
we leverage the advanced sharp concentration inequalities introduced in
\cite{brailovskaya2024universality} to eliminate a logarithmic factor in the
convergence rate, which is crucial for proving the minimax optimality. When the
relevance of source datasets is unknown, we develop an efficient detection
procedure to identify informative sources and establish its selection
consistency. Simulations and real data analysis are conducted to support the
validity of our methodology.

</details>


### [124] [It's Hard to Be Normal: The Impact of Noise on Structure-agnostic Estimation](https://arxiv.org/abs/2507.02275)
*Jikai Jin,Lester Mackey,Vasilis Syrgkanis*

Main category: stat.ML

TL;DR: 论文研究了结构无关因果推断中处理效应估计的性能，发现其依赖于处理噪声的分布。对于高斯噪声，双机器学习（DML）估计器是最优的；而对于非高斯噪声，DML表现不佳，提出了新的高阶稳健性方法（ACE）。


<details>
  <summary>Details</summary>
Motivation: 探讨在结构无关因果推断中，处理效应估计的性能如何依赖于处理噪声的分布，并解决DML在高斯和非高斯噪声下的表现差异问题。

Method: 基于部分线性模型，分析DML在高斯噪声下的最优性，并提出新的ACE方法，利用高阶累积量估计器实现对非高斯噪声的稳健性。

Result: DML在高斯噪声下是最优的，但在非高斯噪声下表现不佳；ACE方法通过高阶累积量估计器实现了更好的性能。

Conclusion: 处理噪声的分布对因果推断的性能有重要影响，ACE方法在非高斯噪声下提供了更优的估计效果。

Abstract: Structure-agnostic causal inference studies how well one can estimate a
treatment effect given black-box machine learning estimates of nuisance
functions (like the impact of confounders on treatment and outcomes). Here, we
find that the answer depends in a surprising way on the distribution of the
treatment noise. Focusing on the partially linear model of
\citet{robinson1988root}, we first show that the widely adopted double machine
learning (DML) estimator is minimax rate-optimal for Gaussian treatment noise,
resolving an open problem of \citet{mackey2018orthogonal}. Meanwhile, for
independent non-Gaussian treatment noise, we show that DML is always suboptimal
by constructing new practical procedures with higher-order robustness to
nuisance errors. These \emph{ACE} procedures use structure-agnostic cumulant
estimators to achieve $r$-th order insensitivity to nuisance errors whenever
the $(r+1)$-st treatment cumulant is non-zero. We complement these core results
with novel minimax guarantees for binary treatments in the partially linear
model. Finally, using synthetic demand estimation experiments, we demonstrate
the practical benefits of our higher-order robust estimators.

</details>


### [125] [Sparse Gaussian Processes: Structured Approximations and Power-EP Revisited](https://arxiv.org/abs/2507.02377)
*Thang D. Bui,Michalis K. Titsias*

Main category: stat.ML

TL;DR: 论文提出了一种基于块对角结构的稀疏变分高斯过程改进方法，并通过实验验证其性能优于现有对角近似方法，同时计算成本相当。


<details>
  <summary>Details</summary>
Motivation: 稀疏变分高斯过程是扩展GP模型的标准方法，但现有对角缩放矩阵方法仍有改进空间。

Method: 引入块对角结构的缩放矩阵，并基于Power Expectation Propagation（PEP）框架进行优化。

Result: 块对角近似方法在回归实验中表现优于或与现有对角近似方法相当，且计算成本相近。

Conclusion: 提出的块对角近似方法和PEP框架为稀疏高斯过程提供了更灵活且性能优越的替代方案。

Abstract: Inducing-point-based sparse variational Gaussian processes have become the
standard workhorse for scaling up GP models. Recent advances show that these
methods can be improved by introducing a diagonal scaling matrix to the
conditional posterior density given the inducing points. This paper first
considers an extension that employs a block-diagonal structure for the scaling
matrix, provably tightening the variational lower bound. We then revisit the
unifying framework of sparse GPs based on Power Expectation Propagation (PEP)
and show that it can leverage and benefit from the new structured approximate
posteriors. Through extensive regression experiments, we show that the proposed
block-diagonal approximation consistently performs similarly to or better than
existing diagonal approximations while maintaining comparable computational
costs. Furthermore, the new PEP framework with structured posteriors provides
competitive performance across various power hyperparameter settings, offering
practitioners flexible alternatives to standard variational approaches.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [126] [ManifoldMind: Dynamic Hyperbolic Reasoning for Trustworthy Recommendations](https://arxiv.org/abs/2507.02014)
*Anoushka Harit,Zhongtian Sun,Suncica Hadzidedic*

Main category: cs.IR

TL;DR: ManifoldMind是一种基于双曲空间的概率几何推荐系统，支持个性化不确定性建模和语义探索。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在固定曲率和刚性嵌入上的局限性，支持更灵活的语义推理。

Method: 使用自适应曲率概率球表示用户、物品和标签，结合曲率感知语义核进行多跳推理。

Result: 在四个公开基准测试中，NDCG、校准和多样性表现优于基线。

Conclusion: ManifoldMind提供透明、可信的推荐，适用于稀疏或抽象领域。

Abstract: We introduce ManifoldMind, a probabilistic geometric recommender system for
exploratory reasoning over semantic hierarchies in hyperbolic space. Unlike
prior methods with fixed curvature and rigid embeddings, ManifoldMind
represents users, items, and tags as adaptive-curvature probabilistic spheres,
enabling personalised uncertainty modeling and geometry-aware semantic
exploration. A curvature-aware semantic kernel supports soft, multi-hop
inference, allowing the model to explore diverse conceptual paths instead of
overfitting to shallow or direct interactions. Experiments on four public
benchmarks show superior NDCG, calibration, and diversity compared to strong
baselines. ManifoldMind produces explicit reasoning traces, enabling
transparent, trustworthy, and exploration-driven recommendations in sparse or
abstract domains.

</details>


### [127] [Listwise Preference Alignment Optimization for Tail Item Recommendation](https://arxiv.org/abs/2507.02255)
*Zihao Li,Chao Yang,Tong Zhang,Yakun Chen,Xianzhi Wang,Guandong Xu,Daoyi Dong*

Main category: cs.IR

TL;DR: 论文提出LPO4Rec方法，通过列表比较优化推荐系统中的偏好对齐，提升训练效率并减少计算成本，特别关注尾部物品推荐。


<details>
  <summary>Details</summary>
Motivation: 现有偏好对齐方法在推荐系统中存在计算成本高或训练效率低的问题，且缺乏对尾部物品推荐的解决方案。

Method: 扩展Bradley-Terry模型至列表比较，提出封闭形式最优策略，结合自适应负采样和重加权策略优化尾部物品推荐。

Result: 在三个公开数据集上显著优于10个基线方法，性能提升达50%，GPU内存使用减少17.9%。

Conclusion: LPO4Rec通过列表偏好优化有效提升推荐系统性能，尤其在尾部物品推荐中表现突出。

Abstract: Preference alignment has achieved greater success on Large Language Models
(LLMs) and drawn broad interest in recommendation research. Existing preference
alignment methods for recommendation either require explicit reward modeling or
only support pairwise preference comparison. The former directly increases
substantial computational costs, while the latter hinders training efficiency
on negative samples. Moreover, no existing effort has explored preference
alignment solutions for tail-item recommendation. To bridge the above gaps, we
propose LPO4Rec, which extends the Bradley-Terry model from pairwise comparison
to listwise comparison, to improve the efficiency of model training.
Specifically, we derive a closed form optimal policy to enable more efficient
and effective training without explicit reward modeling. We also present an
adaptive negative sampling and reweighting strategy to prioritize tail items
during optimization and enhance performance in tail-item recommendations.
Besides, we theoretically prove that optimizing the listwise preference
optimization (LPO) loss is equivalent to maximizing the upper bound of the
optimal reward. Our experiments on three public datasets show that our method
outperforms 10 baselines by a large margin, achieving up to 50% performance
improvement while reducing 17.9% GPU memory usage when compared with direct
preference optimization (DPO) in tail-item recommendation. Our code is
available at https://github.com/Yuhanleeee/LPO4Rec.

</details>


### [128] [Content filtering methods for music recommendation: A review](https://arxiv.org/abs/2507.02282)
*Terence Zeng,Abhishek K. Umrawal*

Main category: cs.IR

TL;DR: 论文探讨了音乐推荐系统中协作过滤的局限性，并研究了内容过滤方法（如歌词分析和音频信号处理）在解决稀疏性问题中的作用。


<details>
  <summary>Details</summary>
Motivation: 音乐流媒体平台中用户与歌曲的交互稀疏，协作过滤方法效果有限，需探索其他方法。

Method: 研究了内容过滤方法，包括基于大型语言模型（LLMs）的歌词分析和音频信号处理技术。

Result: 内容过滤可以缓解协作过滤的偏差，但不同分析方法间可能存在冲突。

Conclusion: 提出了解决分析方法间差异的途径，强调了内容过滤在音乐推荐系统中的潜力。

Abstract: Recommendation systems have become essential in modern music streaming
platforms, shaping how users discover and engage with songs. One common
approach in recommendation systems is collaborative filtering, which suggests
content based on the preferences of users with similar listening patterns to
the target user. However, this method is less effective on media where
interactions are sparse. Music is one such medium, since the average user of a
music streaming service will never listen to the vast majority of tracks. Due
to this sparsity, there are several challenges that have to be addressed with
other methods. This review examines the current state of research in addressing
these challenges, with an emphasis on the role of content filtering in
mitigating biases inherent in collaborative filtering approaches. We explore
various methods of song classification for content filtering, including lyrical
analysis using Large Language Models (LLMs) and audio signal processing
techniques. Additionally, we discuss the potential conflicts between these
different analysis methods and propose avenues for resolving such
discrepancies.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [129] [NLP4Neuro: Sequence-to-sequence learning for neural population decoding](https://arxiv.org/abs/2507.02264)
*Jacob J. Morra,Kaitlyn E. Fouke,Kexin Hang,Zichen He,Owen Traubert,Timothy W. Dunn,Eva A. Naumann*

Main category: q-bio.NC

TL;DR: 论文提出NLP4Neuro方法，利用预训练的大型语言模型（LLMs）解码斑马鱼全脑神经活动与行为的关系，发现预训练权重和混合专家模型（如DeepSeek Coder-7b）显著提升了解码性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决哺乳动物大脑中大规模神经活动与行为关系解码的挑战，利用现代LLMs的序列学习能力探索其在神经解码中的潜力。

Method: 方法包括开发NLP4Neuro框架，测试预训练LLMs在斑马鱼全脑钙成像和行为记录数据上的解码能力，并评估不同模型的性能。

Result: 结果显示，预训练权重和混合专家模型（如DeepSeek Coder-7b）显著提高了行为解码的准确性，并能预测长时间尺度的尾部运动。

Conclusion: 结论表明，LLMs在全脑神经环路解析中具有重要潜力，为神经解码提供了新工具。

Abstract: Delineating how animal behavior arises from neural activity is a foundational
goal of neuroscience. However, as the computations underlying behavior unfold
in networks of thousands of individual neurons across the entire brain, this
presents challenges for investigating neural roles and computational mechanisms
in large, densely wired mammalian brains during behavior. Transformers, the
backbones of modern large language models (LLMs), have become powerful tools
for neural decoding from smaller neural populations. These modern LLMs have
benefited from extensive pre-training, and their sequence-to-sequence learning
has been shown to generalize to novel tasks and data modalities, which may also
confer advantages for neural decoding from larger, brain-wide activity
recordings. Here, we present a systematic evaluation of off-the-shelf LLMs to
decode behavior from brain-wide populations, termed NLP4Neuro, which we used to
test LLMs on simultaneous calcium imaging and behavior recordings in larval
zebrafish exposed to visual motion stimuli. Through NLP4Neuro, we found that
LLMs become better at neural decoding when they use pre-trained weights learned
from textual natural language data. Moreover, we found that a recent
mixture-of-experts LLM, DeepSeek Coder-7b, significantly improved behavioral
decoding accuracy, predicted tail movements over long timescales, and provided
anatomically consistent highly interpretable readouts of neuron salience.
NLP4Neuro demonstrates that LLMs are highly capable of informing brain-wide
neural circuit dissection.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [130] [Acoustic evaluation of a neural network dedicated to the detection of animal vocalisations](https://arxiv.org/abs/2507.01974)
*Jérémy Rouch,M Ducrettet,S Haupert,R Emonet,F Sèbe*

Main category: cs.SD

TL;DR: 本文提出了一种基于信号噪声比与检测概率关系的简单方法，用于评估自动信号检测系统的性能，并优化其训练。


<details>
  <summary>Details</summary>
Motivation: 当前自动信号检测方法的性能评估多依赖机器学习指标，缺乏声学分析，因此需要一种更全面的评估方法。

Method: 通过合成信号的信噪比与其检测概率的关系，提出了一种声学分析方法。

Result: 该方法提供了系统性能信息，优化了训练，并实现了检测距离的建模，从而评估声学环境动态和估计呼叫空间密度。

Conclusion: 该方法为自动信号检测系统的性能评估和优化提供了一种有效的声学分析工具。

Abstract: The accessibility of long-duration recorders, adapted to sometimes demanding
field conditions, has enabled the deployment of extensive animal population
monitoring campaigns through ecoacoustics. The effectiveness of automatic
signal detection methods, increasingly based on neural approaches, is
frequently evaluated solely through machine learning metrics, while acoustic
analysis of performance remains rare. As part of the acoustic monitoring of
Rock Ptarmigan populations, we propose here a simple method for acoustic
analysis of the detection system's performance. The proposed measure is based
on relating the signal-to-noise ratio of synthetic signals to their probability
of detection. We show how this measure provides information about the system
and allows optimisation of its training. We also show how it enables modelling
of the detection distance, thus offering the possibility of evaluating its
dynamics according to the sound environment and accessing an estimation of the
spatial density of calls.

</details>


### [131] [Analyzing and Improving Speaker Similarity Assessment for Speech Synthesis](https://arxiv.org/abs/2507.02176)
*Marc-André Carbonneau,Benjamin van Niekerk,Hugo Seuté,Jean-Philippe Letendre,Herman Kamper,Julian Zaïdi*

Main category: cs.SD

TL;DR: 论文研究了ASV嵌入在捕捉声音身份时的局限性，提出U3D指标以评估动态节奏模式。


<details>
  <summary>Details</summary>
Motivation: 研究ASV嵌入在表征声音身份时的不足，尤其是对动态特征的忽略。

Method: 分析ASV嵌入的局限性，提出U3D指标评估动态节奏。

Result: 发现ASV嵌入主要关注静态特征（如音色和音高范围），而忽略动态元素（如节奏）。

Conclusion: U3D指标弥补了ASV嵌入的不足，为声音克隆系统中的身份一致性评估提供了新方法。

Abstract: Modeling voice identity is challenging due to its multifaceted nature. In
generative speech systems, identity is often assessed using automatic speaker
verification (ASV) embeddings, designed for discrimination rather than
characterizing identity. This paper investigates which aspects of a voice are
captured in such representations. We find that widely used ASV embeddings focus
mainly on static features like timbre and pitch range, while neglecting dynamic
elements such as rhythm. We also identify confounding factors that compromise
speaker similarity measurements and suggest mitigation strategies. To address
these gaps, we propose U3D, a metric that evaluates speakers' dynamic rhythm
patterns. This work contributes to the ongoing challenge of assessing speaker
identity consistency in the context of ever-better voice cloning systems. We
publicly release our code.

</details>


### [132] [Posterior Transition Modeling for Unsupervised Diffusion-Based Speech Enhancement](https://arxiv.org/abs/2507.02391)
*Mostafa Sadeghi,Jean-Eudes Ayilo,Romain Serizel,Xavier Alameda-Pineda*

Main category: cs.SD

TL;DR: 论文提出两种基于扩散模型的非监督语音增强算法，无需超参数调整且性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过近似噪声扰动似然分数引导反向扩散过程，需要超参数调整，性能受限。

Method: 提出两种算法：1）将扩散先验与观测模型结合；2）在噪声语音上定义扩散过程，获得精确似然分数。

Result: 在WSJ0-QUT和VoiceBank-DEMAND数据集上，性能优于监督和非监督基线方法，且对领域转移更鲁棒。

Conclusion: 新方法在非监督语音增强中表现优异，解决了现有方法的局限性。

Abstract: We explore unsupervised speech enhancement using diffusion models as
expressive generative priors for clean speech. Existing approaches guide the
reverse diffusion process using noisy speech through an approximate,
noise-perturbed likelihood score, combined with the unconditional score via a
trade-off hyperparameter. In this work, we propose two alternative algorithms
that directly model the conditional reverse transition distribution of
diffusion states. The first method integrates the diffusion prior with the
observation model in a principled way, removing the need for hyperparameter
tuning. The second defines a diffusion process over the noisy speech itself,
yielding a fully tractable and exact likelihood score. Experiments on the
WSJ0-QUT and VoiceBank-DEMAND datasets demonstrate improved enhancement metrics
and greater robustness to domain shifts compared to both supervised and
unsupervised baselines.

</details>


### [133] [De-AntiFake: Rethinking the Protective Perturbations Against Voice Cloning Attacks](https://arxiv.org/abs/2507.02606)
*Wei Fan,Kejiang Chen,Chang Liu,Weiming Zhang,Nenghai Yu*

Main category: cs.SD

TL;DR: 本文首次系统评估了对抗性扰动在真实威胁模型下对语音克隆（VC）的保护效果，并提出了一种新的两阶段净化方法，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 语音克隆技术的快速发展引发了隐私和安全问题，现有对抗性扰动保护方法易被攻击者绕过，亟需更鲁棒的解决方案。

Method: 提出两阶段净化方法：先净化扰动语音，再通过音素引导对齐干净语音分布。

Result: 实验表明，新方法在破坏VC防御方面优于现有净化方法，揭示了对抗性扰动防御的局限性。

Conclusion: 研究强调了开发更鲁棒解决方案的紧迫性，以应对语音克隆带来的安全和隐私风险。

Abstract: The rapid advancement of speech generation models has heightened privacy and
security concerns related to voice cloning (VC). Recent studies have
investigated disrupting unauthorized voice cloning by introducing adversarial
perturbations. However, determined attackers can mitigate these protective
perturbations and successfully execute VC. In this study, we conduct the first
systematic evaluation of these protective perturbations against VC under
realistic threat models that include perturbation purification. Our findings
reveal that while existing purification methods can neutralize a considerable
portion of the protective perturbations, they still lead to distortions in the
feature space of VC models, which degrades the performance of VC. From this
perspective, we propose a novel two-stage purification method: (1) Purify the
perturbed speech; (2) Refine it using phoneme guidance to align it with the
clean speech distribution. Experimental results demonstrate that our method
outperforms state-of-the-art purification methods in disrupting VC defenses.
Our study reveals the limitations of adversarial perturbation-based VC defenses
and underscores the urgent need for more robust solutions to mitigate the
security and privacy risks posed by VC. The code and audio samples are
available at https://de-antifake.github.io.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [134] [DecoRTL: A Run-time Decoding Framework for RTL Code Generation with LLMs](https://arxiv.org/abs/2507.02226)
*Mohammad Akyash,Kimia Azar,Hadi Kamali*

Main category: cs.PL

TL;DR: 论文提出了一种名为DecoRTL的新型解码策略，用于改进大型语言模型在RTL代码生成中的表现，解决了传统解码策略在结构性和语义需求上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统LLM解码策略在RTL代码生成中常产生无效或重复的代码，无法满足结构性和语义需求。

Method: DecoRTL结合了自一致性采样和语法感知温度调整，生成多样且正确的代码。

Result: 在VerilogEval基准测试中，DecoRTL显著提升了语法有效性、功能正确性和输出多样性。

Conclusion: DecoRTL无需额外微调即可在推理时显著提升RTL代码生成质量。

Abstract: As one of their many applications, large language models (LLMs) have recently
shown promise in automating register transfer level (RTL) code generation.
However, conventional LLM decoding strategies, originally designed for natural
language, often fail to meet the structural and semantic demands of RTL,
leading to hallucinated, repetitive, or invalid code outputs. In this paper, we
first investigate the root causes of these decoding failures through an
empirical analysis of token-level entropy during RTL generation. Our findings
reveal that LLMs exhibit low confidence in regions of structural ambiguity or
semantic complexity, showing that standard decoding strategies fail to
differentiate between regions requiring determinism (syntax-critical regions)
and those that benefit from creative exploratory variability (design-critical
regions). Then, to overcome this, we introduce DecoRTL, a novel run-time
decoding strategy, that is both syntax-aware and contrastive for RTL code
generation. DecoRTL integrates two complementary components: (i)
self-consistency sampling, which generates multiple candidates and re-ranks
them based on token-level agreement to promote correctness while maintaining
diversity; and (ii) syntax-aware temperature adaptation, which classifies
tokens by their syntactical and functional roles and adjusts the sampling
temperature accordingly, enforcing low temperature for syntax-critical tokens
and higher temperature for exploratory ones. Our approach operates entirely at
inference time without requiring any additional model fine-tuning. Through
evaluations on multiple open-source LLMs using the VerilogEval benchmark, we
demonstrate significant improvements in syntactic validity, functional
correctness, and output diversity, while the execution overhead (performance
overhead) is imperceptible.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [135] [Machine Learning Based Stress Testing Framework for Indian Financial Market Portfolios](https://arxiv.org/abs/2507.02011)
*Vidya Sagar G,Shifat Ali,Siddhartha P. Chakrabarty*

Main category: q-fin.RM

TL;DR: 提出了一种基于机器学习的框架，用于印度金融市场的行业压力测试，改进了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统压力测试方法存在局限性，无法捕捉复杂的非线性依赖关系，因此需要更灵活和稳健的方法。

Method: 通过主成分分析和自编码器进行降维和潜在因子建模，进一步使用变分自编码器引入概率结构，支持蒙特卡洛情景生成。

Result: 框架能够捕捉非线性依赖关系，并通过风险价值（VaR）和预期短缺（ES）进行风险估计。

Conclusion: 机器学习方法提高了金融压力测试的灵活性、稳健性和现实性。

Abstract: This paper presents a machine learning driven framework for sectoral stress
testing in the Indian financial market, focusing on financial services,
information technology, energy, consumer goods, and pharmaceuticals. Initially,
we address the limitations observed in conventional stress testing through
dimensionality reduction and latent factor modeling via Principal Component
Analysis and Autoencoders. Building on this, we extend the methodology using
Variational Autoencoders, which introduces a probabilistic structure to the
latent space. This enables Monte Carlo-based scenario generation, allowing for
more nuanced, distribution-aware simulation of stressed market conditions. The
proposed framework captures complex non-linear dependencies and supports risk
estimation through Value-at-Risk and Expected Shortfall. Together, these
pipelines demonstrate the potential of Machine Learning approaches to improve
the flexibility, robustness, and realism of financial stress testing.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [136] [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)
*Wenquan Lu,Yuechuan Yang,Kyle Lee,Yanshu Li,Enqi Liu*

Main category: cs.CL

TL;DR: 论文研究了Huginn-3.5B模型中是否存在潜在推理结构，发现其内部行为在算术任务中表现出有限的潜在推理证据，且增加递归深度仅带来边际收益。


<details>
  <summary>Details</summary>
Motivation: 探索深度递归Transformer模型（如Huginn-3.5B）是否能在潜在空间中实现推理，以弥补传统解码器架构在效率上的不足。

Method: 使用Logit Lens和Coda Lens等探测技术，分析模型在算术任务中的内部行为，并跟踪结果令牌的排名轨迹。

Result: 发现潜在推理的证据有限，且隐藏状态的可解释性依赖于层索引和解码方法；增加递归深度效果不明显。

Conclusion: Huginn-3.5B的潜在推理能力有限，显式外部化推理步骤的模型表现更优。

Abstract: Chain-of-thought (CoT) reasoning has enabled transformer-based language
models to excel at complex mathematics and multi-step planning. However, in
standard decoder-only architectures, these reasoning steps are externalized in
natural language, improving interpretability at the cost of efficiency. To
capture reasoning that is not easily represented in words, many works have
explored recurrent architectures that aim to internalize reasoning in latent
space, potentially supporting latent CoT. In this paper, we investigate whether
such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer
that reuses layers at inference time without increasing parameter count. We
examine the model's internal behavior on arithmetic tasks using a suite of
probing techniques including the Logit Lens and Coda Lens. Our findings reveal
limited evidence of interpretable latent CoT by tracking rank trajectories of
final and intermediate result tokens. Furthermore, we uncover significant
probing inconsistencies across recurrent blocks, where the interpretability of
hidden states depends heavily on both the layer index and the decoding method.
Finally, we empirically show that increasing recurrence depth yields only
marginal gains and falls well short of models that explicitly externalize
reasoning steps. The code is available at
https://github.com/wenquanlu/huginn-latent-cot.

</details>


### [137] [MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)
*Hongli Yu,Tinghong Chen,Jiangtao Feng,Jiangjie Chen,Weinan Dai,Qiying Yu,Ya-Qin Zhang,Wei-Ying Ma,Jingjing Liu,Mingxuan Wang,Hao Zhou*

Main category: cs.CL

TL;DR: MemAgent通过分段读取和覆盖策略优化长文本处理，在32K训练下支持8K到3.5M的上下文扩展，性能损失低于5%。


<details>
  <summary>Details</summary>
Motivation: 解决长文本处理中无限长文档的线性复杂度问题，避免性能下降。

Method: 引入MemAgent工作流，分段读取文本并更新内存，扩展DAPO算法进行多对话生成训练。

Result: 在32K训练下支持8K到3.5M的QA任务，性能损失<5%，512K RULER测试达到95%+。

Conclusion: MemAgent在长文本处理中表现出色，支持高效扩展且性能稳定。

Abstract: Despite improvements by length extrapolation, efficient attention and memory
modules, handling infinitely long documents with linear complexity without
performance degradation during extrapolation remains the ultimate challenge in
long-text processing. We directly optimize for long-text tasks in an end-to-end
fashion and introduce a novel agent workflow, MemAgent, which reads text in
segments and updates the memory using an overwrite strategy. We extend the DAPO
algorithm to facilitate training via independent-context multi-conversation
generation. MemAgent has demonstrated superb long-context capabilities, being
able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task
with performance loss < 5% and achieves 95%+ in 512K RULER test.

</details>


### [138] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

TL;DR: DoMIX提出了一种基于LoRA模块的高效、并行且对领域顺序鲁棒的领域自适应预训练方法，解决了现有方法的计算成本高、对数据顺序敏感等问题。


<details>
  <summary>Details</summary>
Motivation: 现有持续领域自适应预训练方法存在高计算成本、对数据顺序敏感以及无法为特定任务提供定制化模型的问题。

Method: 利用LoRA模块（一种参数高效微调方法），实现高效、并行且对领域顺序鲁棒的预训练。

Result: DoMIX能够高效利用累积知识，为特定任务提供定制化预训练模型，并可扩展至标准LLM微调场景。

Conclusion: DoMIX通过LoRA模块解决了持续领域自适应预训练的关键挑战，具有高效性和灵活性。

Abstract: Domain-Adaptive Pre-training (DAP) has recently gained attention for its
effectiveness in fine-tuning pre-trained models. Building on this, continual
DAP has been explored to develop pre-trained models capable of incrementally
incorporating different domain datasets. However, existing continual DAP
methods face several limitations: (1) high computational cost and GPU memory
usage during training; (2) sensitivity to incremental data order; and (3)
providing a single, generalized model for all end tasks, which contradicts the
essence of DAP. In this paper, we propose DoMIX, a novel approach that
addresses these challenges by leveraging LoRA modules, a representative
parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient
and parallel domain-adaptive pre-training that is robust to domain order and
effectively utilizes accumulated knowledge to provide tailored pre-trained
models for specific tasks. We also demonstrate that our method can be extended
beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available
at https://github.com/dohoonkim-ai/DoMIX.

</details>


### [139] [Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability](https://arxiv.org/abs/2507.02407)
*Mark Atta Mensah,Isaac Wiafe,Akon Ekpezu,Justice Kwame Appati,Jamal-Deen Abdulai,Akosua Nyarkoa Wiafe-Akenten,Frank Ernest Yeboah,Gifty Odame*

Main category: cs.CL

TL;DR: 本研究评估了七种基于Transformer架构的Akan ASR模型（如Whisper和Wav2Vec2）在四种不同领域的Akan语音数据集上的表现，发现模型性能高度依赖训练领域，并揭示了Whisper和Wav2Vec2在错误行为上的差异。


<details>
  <summary>Details</summary>
Motivation: 现有ASR研究多局限于单一领域评估，缺乏对跨领域泛化能力的考察，本研究填补了这一空白。

Method: 使用四种Akan语音数据集（涵盖文化描述、非正式对话、圣经朗读和金融对话）对七种Transformer架构的ASR模型进行基准测试，比较词错误率和字符错误率。

Result: 模型在训练领域表现最佳，跨领域性能显著下降；Whisper生成流畅但可能误导的错误，Wav2Vec2错误更明显但不易解释。

Conclusion: 研究强调在低资源语言应用中需考虑架构选择（可读性与透明性权衡），并提出领域适应、自适应路由和多语言训练的需求。

Abstract: Most existing automatic speech recognition (ASR) research evaluate models
using in-domain datasets. However, they seldom evaluate how they generalize
across diverse speech contexts. This study addresses this gap by benchmarking
seven Akan ASR models built on transformer architectures, such as Whisper and
Wav2Vec2, using four Akan speech corpora to determine their performance. These
datasets encompass various domains, including culturally relevant image
descriptions, informal conversations, biblical scripture readings, and
spontaneous financial dialogues. A comparison of the word error rate and
character error rate highlighted domain dependency, with models performing
optimally only within their training domains while showing marked accuracy
degradation in mismatched scenarios. This study also identified distinct error
behaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned
Whisper Akan models led to more fluent but potentially misleading transcription
errors, Wav2Vec2 produced more obvious yet less interpretable outputs when
encountering unfamiliar inputs. This trade-off between readability and
transparency in ASR errors should be considered when selecting architectures
for low-resource language (LRL) applications. These findings highlight the need
for targeted domain adaptation techniques, adaptive routing strategies, and
multilingual training frameworks for Akan and other LRLs.

</details>


### [140] [IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders](https://arxiv.org/abs/2507.02506)
*Sneha Deshmukh,Prathmesh Kamble*

Main category: cs.CL

TL;DR: 介绍了印度首个公开的保释判决数据集IndianBailJudgments-1200，包含1200条标注数据，支持多种法律NLP任务。


<details>
  <summary>Details</summary>
Motivation: 解决印度法律NLP领域因缺乏结构化数据集而发展不足的问题。

Method: 使用GPT-4o提示工程生成标注，并进行一致性验证。

Result: 创建了包含20+属性的数据集，支持保释结果预测、摘要生成和公平性分析等任务。

Conclusion: 该数据集填补了印度保释法领域的空白，为法律NLP研究提供了重要资源。

Abstract: Legal NLP remains underdeveloped in regions like India due to the scarcity of
structured datasets. We introduce IndianBailJudgments-1200, a new benchmark
dataset comprising 1200 Indian court judgments on bail decisions, annotated
across 20+ attributes including bail outcome, IPC sections, crime type, and
legal reasoning. Annotations were generated using a prompt-engineered GPT-4o
pipeline and verified for consistency. This resource supports a wide range of
legal NLP tasks such as outcome prediction, summarization, and fairness
analysis, and is the first publicly available dataset focused specifically on
Indian bail jurisprudence.

</details>


### [141] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)
*Cornelia Gruber,Helen Alber,Bernd Bischl,Göran Kauermann,Barbara Plank,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 论文探讨了标签变异（LV）和人类标签变异（HLV）在监督学习中的重要性，提出了一种将HLV融入主动学习（AL）循环的概念框架。


<details>
  <summary>Details</summary>
Motivation: 高质量标注数据有限，且现有标注框架常假设单一真实标签，忽略了HLV作为信息信号的价值。主动学习中的简化假设在实际中难以成立。

Method: 分解标签变异为信号（如HLV）和噪声（如标注错误），提出概念框架将HLV融入AL循环（实例选择、标注者选择、标签表示），并探讨大型语言模型（LLM）作为标注者的整合。

Result: 提出了一个更贴近真实世界标注复杂性的HLV感知主动学习框架。

Conclusion: 论文为HLV感知的主动学习奠定了概念基础，强调了在AL中考虑HLV的重要性。

Abstract: Access to high-quality labeled data remains a limiting factor in applied
supervised learning. While label variation (LV), i.e., differing labels for the
same instance, is common, especially in natural language processing, annotation
frameworks often still rest on the assumption of a single ground truth. This
overlooks human label variation (HLV), the occurrence of plausible differences
in annotations, as an informative signal. Similarly, active learning (AL), a
popular approach to optimizing the use of limited annotation budgets in
training ML models, often relies on at least one of several simplifying
assumptions, which rarely hold in practice when acknowledging HLV. In this
paper, we examine foundational assumptions about truth and label nature,
highlighting the need to decompose observed LV into signal (e.g., HLV) and
noise (e.g., annotation error). We survey how the AL and (H)LV communities have
addressed -- or neglected -- these distinctions and propose a conceptual
framework for incorporating HLV throughout the AL loop, including instance
selection, annotator choice, and label representation. We further discuss the
integration of large language models (LLM) as annotators. Our work aims to lay
a conceptual foundation for HLV-aware active learning, better reflecting the
complexities of real-world annotation.

</details>


### [142] [Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs](https://arxiv.org/abs/2507.02778)
*Ken Tsui*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）存在自我修正盲区，提出了一种系统框架Self-Correction Bench进行测量，发现平均盲区率为64.5%。通过添加简单提示（如“Wait”）可显著减少盲区。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在自我修正能力上的局限性，尤其是对自身输出错误的修正盲区，以提高其可靠性和可信度。

Method: 引入Self-Correction Bench框架，通过控制性错误注入在三个复杂度级别上测试14个模型。

Result: 平均盲区率为64.5%，发现训练数据组成是主要原因。简单提示（如“Wait”）可减少盲区89.3%。

Conclusion: 揭示了当前LLMs的关键局限性，并提出了改进其可靠性的潜在途径。

Abstract: Although large language models (LLMs) have become transformative, they still
make mistakes and can explore unproductive reasoning paths. Self-correction is
an important capability for a trustworthy LLM, particularly an autoregressive
LLM. While LLMs can identify error in user input, they exhibit a systematic
'Self-Correction Blind Spot' - failing to correct identical error in their own
outputs. To systematically study this phenomenon, we introduce Self-Correction
Bench, a systematic framework to measure this phenomenon through controlled
error injection at three complexity levels. Testing 14 models, we find an
average 64.5% blind spot rate. We find multiple evidences that this limitation
relates to training data composition: human training demonstrations
predominantly show error-free responses rather than error-correction sequences,
unlike RL-trained models that learn error correction through outcome feedback.
Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting
that the capability exists but requires activation. Our work highlights a
critical limitation in current LLMs and offers potential avenues for improving
their reliability and trustworthiness.

</details>


### [143] [SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model](https://arxiv.org/abs/2507.02822)
*Wencheng Zhang,Shiqin Qiao,Lingjie Luo,Yinfeng Li,Chuanyang Zheng,Qian Xu,Meng Li,Yong Gui,Yijun He,Jianing Qiu,Jindong Hong,Jiankai Sun*

Main category: cs.CL

TL;DR: 论文提出SynapseRoute框架，动态分配查询到高推理或低成本模式，优化医疗问答的准确性和成本效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）应用中需平衡性能和成本，高推理与低成本模式存在明显成本差距，动态路由可优化资源分配。

Method: 提出SynapseRoute框架，基于机器学习动态路由查询到高推理或低成本模式，并引入AIT指数评估准确性、延迟和成本。

Result: 实验显示SynapseRoute提高准确性（0.8390 vs. 0.8272），减少推理时间36.8%和token消耗39.66%。

Conclusion: 动态路由可避免过度推理，优化资源使用，AIT指数为模型选择提供全面评估标准。

Abstract: With the widespread adoption of large language models (LLMs) in practical
applications, selecting an appropriate model requires balancing not only
performance but also operational cost. The emergence of reasoning-capable
models has further widened the cost gap between "thinking" (high reasoning) and
"non-thinking" (fast, low-cost) modes. In this work, we reveal that
approximately 58% of medical questions can be accurately answered by the
non-thinking mode alone, without requiring the high-cost reasoning process.
This highlights a clear dichotomy in problem complexity and suggests that
dynamically routing queries to the appropriate mode based on complexity could
optimize accuracy, cost-efficiency, and overall user experience. Based on this,
we further propose SynapseRoute, a machine learning-based dynamic routing
framework that intelligently assigns input queries to either thinking or
non-thinking modes. Experimental results on several medical datasets
demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.
0.8272) compared to the thinking mode alone but also reduces inference time by
36.8% and token consumption by 39.66%. Importantly, qualitative analysis
indicates that over-reasoning on simpler queries can lead to unnecessary delays
and even decreased accuracy, a pitfall avoided by our adaptive routing.
Finally, this work further introduces the Accuracy-Inference-Token (AIT) index
to comprehensively evaluate the trade-offs among accuracy, latency, and token
cost.

</details>


### [144] [LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users](https://arxiv.org/abs/2507.02850)
*Almog Hilel,Idan Shenfeld,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 论文揭示了语言模型（LM）在用户反馈训练中的漏洞，单个用户可通过提供提示和反馈（点赞/点踩）持久改变模型知识和行为。攻击者通过操纵反馈信号，使模型在后续偏好调整中更倾向于输出恶意内容。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示语言模型在用户反馈训练中的潜在安全漏洞，以及这种漏洞如何被利用来操纵模型行为。

Method: 攻击方法包括：1）让模型随机输出“有毒”或良性响应；2）通过点赞有毒响应或点踩良性响应操纵反馈信号；3）利用反馈信号在偏好调整中增加模型输出有毒内容的概率。

Result: 结果显示，攻击可用于：1）插入模型原本不具备的事实知识；2）修改代码生成模式以引入安全漏洞；3）注入虚假金融新闻。

Conclusion: 结论指出，该研究不仅揭示了偏好调整的新特性（即有限反馈数据可精细控制模型行为），还扩展了对预训练数据投毒和部署时提示注入攻击的理解。

Abstract: We describe a vulnerability in language models (LMs) trained with user
feedback, whereby a single user can persistently alter LM knowledge and
behavior given only the ability to provide prompts and upvote / downvote
feedback on LM outputs. To implement the attack, the attacker prompts the LM to
stochastically output either a "poisoned" or benign response, then upvotes the
poisoned response or downvotes the benign one. When feedback signals are used
in a subsequent preference tuning behavior, LMs exhibit increased probability
of producing poisoned responses even in contexts without malicious prompts. We
show that this attack can be used to (1) insert factual knowledge the model did
not previously possess, (2) modify code generation patterns in ways that
introduce exploitable security flaws, and (3) inject fake financial news. Our
finding both identifies a new qualitative feature of language model preference
tuning (showing that it even highly restricted forms of preference data can be
used to exert fine-grained control over behavior), and a new attack mechanism
for LMs trained with user feedback (extending work on pretraining-time data
poisoning and deployment-time prompt injection).

</details>


### [145] [MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs](https://arxiv.org/abs/2507.02851)
*Purbesh Mitra,Sennur Ulukus*

Main category: cs.CL

TL;DR: 论文提出MOTIF方法，通过多轮思考增强LLM的推理能力，在有限上下文下实现更高性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM的上下文大小限制了其推理能力，需多轮思考策略突破这一瓶颈。

Method: 采用强化学习微调方法（MOTIF），生成多轮思考令牌，扩展上下文容量。

Result: 在MATH500和AIME2024基准测试中，性能分别提升3.8%和3.3%，且样本效率高。

Conclusion: MOTIF方法有效提升LLM推理能力，且具有样本高效性。

Abstract: Recent advancements in the reasoning capabilities of large language models
(LLMs) show that employing group relative policy optimization (GRPO) algorithm
for reinforcement learning (RL) training allows the models to use more
thinking/reasoning tokens for generating better responses. However, LLMs can
generate only a finite amount of tokens while maintaining attention to the
previously generated tokens. This limit, also known as the context size of an
LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.
To think beyond the limit of context size, an LLM must employ a modular
thinking strategy to reason over multiple rounds. In this work, we propose
$\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL
training method for generating thinking tokens in multiple rounds, effectively
allowing the model to think with additional context size. We trained the
open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient
fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our
experiments show 3.8\% and 3.3\% improvements over vanilla GRPO based training
in the respective benchmarks. Furthermore, this improvement was achieved with
only 15\% of samples, thus demonstrating sample efficiency of MOTIF. Our code
and models are available at https://github.com/purbeshmitra/MOTIF and
https://huggingface.co/purbeshmitra/MOTIF, respectively.

</details>


### [146] [Answer Matching Outperforms Multiple Choice for Language Model Evaluation](https://arxiv.org/abs/2507.02856)
*Nikhil Chandak,Shashwat Goel,Ameya Prabhu,Moritz Hardt,Jonas Geiping*

Main category: cs.CL

TL;DR: 论文指出多项选择题评测的局限性，提出生成式评测方法（答案匹配），并证明其与人类评分一致性高。


<details>
  <summary>Details</summary>
Motivation: 多项选择题评测存在缺陷，可能无需问题即可回答，而生成式评测能更准确反映模型能力。

Method: 采用答案匹配方法：模型生成自由回答，再用现代语言模型与参考答案匹配。

Result: 答案匹配方法（即使是小模型）与人类评分一致性接近完美，优于多项选择题评测。

Conclusion: 建议从多项选择题评测转向答案匹配方法，以提升评测准确性。

Abstract: Multiple choice benchmarks have long been the workhorse of language model
evaluation because grading multiple choice is objective and easy to automate.
However, we show multiple choice questions from popular benchmarks can often be
answered without even seeing the question. These shortcuts arise from a
fundamental limitation of discriminative evaluation not shared by evaluations
of the model's free-form, generative answers. Until recently, there appeared to
be no viable, scalable alternative to multiple choice--but, we show that this
has changed. We consider generative evaluation via what we call answer
matching: Give the candidate model the question without the options, have it
generate a free-form response, then use a modern language model with the
reference answer to determine if the response matches the reference. To compare
the validity of different evaluation strategies, we annotate MMLU-Pro and
GPQA-Diamond to obtain human grading data, and measure the agreement of each
evaluation approach. We find answer matching using recent models--even small
ones--achieves near-perfect agreement, in the range of inter-annotator
agreement. In contrast, both multiple choice evaluation and using
LLM-as-a-judge without reference answers aligns poorly with human grading.
Improving evaluations via answer matching is not merely a conceptual concern:
the rankings of several models change significantly when evaluating their
free-form responses with answer matching. In light of these findings, we
discuss how to move the evaluation ecosystem from multiple choice to answer
matching.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [147] [SAT-BO: Verification Rule Learning and Optimization for FraudTransaction Detection](https://arxiv.org/abs/2507.02635)
*Mao Luo,Zhi Wang,Yiwen Huang,Qingyun Zhang,Zhouxing Su,Zhipeng Lv,Wen Hu,Jianguo Li*

Main category: cs.CR

TL;DR: 论文探讨了电子支付平台中验证规则的漏洞问题，提出需要系统性方法来确保规则的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 电子支付平台的高交易量和潜在金融风险促使研究如何系统性识别和修复验证规则的缺陷。

Method: 通过分析手动构建的验证规则，研究其漏洞并提出系统性改进方法。

Result: 发现现有验证规则易受攻击，需系统性方法提升其鲁棒性。

Conclusion: 系统性方法对提升电子支付平台验证规则的安全性至关重要。

Abstract: Electronic payment platforms are estimated to process billions oftransactions
daily, with the cumulative value of these transactionspotentially reaching into
the trillions. Even a minor error within thishigh-volume environment could
precipitate substantial financiallosses. To mitigate this risk, manually
constructed verification rules,developed by domain experts, are typically
employed to identifyand scrutinize transactions in production environments.
However,due to the absence of a systematic approach to ensure the robust-ness
of these verification rules against vulnerabilities, they remainsusceptible to
exploitation.To mitigate this risk, manually constructed verification rules,
de-veloped by domain experts, are typically employed to identify andscrutinize
transactions in production environments. However, dueto the absence of a
systematic approach to ensure the robustness ofthese verification rules against
vulnerabilities, they remain suscep-tible to exploitation. To ensure data
security, database maintainersusually compose complex verification rules to
check whether aquery/update request is valid. However, the rules written by
ex-perts are usually imperfect, and malicious requests may bypassthese rules.
As a result, the demand for identifying the defects ofthe rules systematically
emerges.

</details>


### [148] [Alleviating Attack Data Scarcity: SCANIA's Experience Towards Enhancing In-Vehicle Cyber Security Measures](https://arxiv.org/abs/2507.02607)
*Frida Sundfeldt,Bianca Widstam,Mahshid Helali Moghadam,Kuo-Yun Liang,Anders Vesterberg*

Main category: cs.CR

TL;DR: 本文提出了一种上下文感知的攻击数据生成器，用于生成高质量的攻击数据，以解决车载网络安全中数据稀缺的问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着联网车辆的数字化发展，车载网络安全需求日益突出，但受限于安全、成本和伦理问题，攻击场景数据稀缺，亟需高效的数据生成方法。

Method: 开发了一种上下文感知的攻击数据生成器，利用参数化攻击模型和CAN消息解码技术，生成多种攻击类型的数据，并通过深度学习模型进行验证。

Result: 生成的攻击数据在入侵检测系统（IDS）中表现出高效性和可扩展性，深度学习模型具有高检测和分类能力。

Conclusion: 该方法能有效生成高质量攻击数据，为车载网络安全研究提供了实用工具，并验证了其在实际应用中的一致性。

Abstract: The digital evolution of connected vehicles and the subsequent security risks
emphasize the critical need for implementing in-vehicle cyber security measures
such as intrusion detection and response systems. The continuous advancement of
attack scenarios further highlights the need for adaptive detection mechanisms
that can detect evolving, unknown, and complex threats. The effective use of
ML-driven techniques can help address this challenge. However, constraints on
implementing diverse attack scenarios on test vehicles due to safety, cost, and
ethical considerations result in a scarcity of data representing attack
scenarios. This limitation necessitates alternative efficient and effective
methods for generating high-quality attack-representing data. This paper
presents a context-aware attack data generator that generates attack inputs and
corresponding in-vehicle network log, i.e., controller area network (CAN) log,
representing various types of attack including denial of service (DoS), fuzzy,
spoofing, suspension, and replay attacks. It utilizes parameterized attack
models augmented with CAN message decoding and attack intensity adjustments to
configure the attack scenarios with high similarity to real-world scenarios and
promote variability. We evaluate the practicality of the generated
attack-representing data within an intrusion detection system (IDS) case study,
in which we develop and perform an empirical evaluation of two deep neural
network IDS models using the generated data. In addition to the efficiency and
scalability of the approach, the performance results of IDS models, high
detection and classification capabilities, validate the consistency and
effectiveness of the generated data as well. In this experience study, we also
elaborate on the aspects influencing the fidelity of the data to real-world
scenarios and provide insights into its application.

</details>


### [149] [Can Artificial Intelligence solve the blockchain oracle problem? Unpacking the Challenges and Possibilities](https://arxiv.org/abs/2507.02125)
*Giulio Caldarelli*

Main category: cs.CR

TL;DR: 本文探讨了AI如何作为补充层解决区块链预言机问题，提升数据质量和系统韧性，但无法完全消除对链外数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 区块链预言机问题限制了去中心化应用的发展，现有方法未能彻底解决链外数据可靠性问题。

Method: 通过学术和实践案例，分析AI技术（如异常检测、语言事实提取等）在预言机系统中的应用。

Result: AI能提升数据质量、源选择和系统韧性，但仍需依赖不可验证的链外输入。

Conclusion: AI应作为预言机设计的补充层，而非替代信任假设。

Abstract: The blockchain oracle problem, which refers to the challenge of injecting
reliable external data into decentralized systems, remains a fundamental
limitation to the development of trustless applications. While recent years
have seen a proliferation of architectural, cryptographic, and economic
strategies to mitigate this issue, no one has yet fully resolved the
fundamental question of how a blockchain can gain knowledge about the off-chain
world. In this position paper, we critically assess the role artificial
intelligence (AI) can play in tackling the oracle problem. Drawing from both
academic literature and practitioner implementations, we examine how AI
techniques such as anomaly detection, language-based fact extraction, dynamic
reputation modeling, and adversarial resistance can enhance oracle systems. We
observe that while AI introduces powerful tools for improving data quality,
source selection, and system resilience, it cannot eliminate the reliance on
unverifiable off-chain inputs. Therefore, this study supports the idea that AI
should be understood as a complementary layer of inference and filtering within
a broader oracle design, not a substitute for trust assumptions.

</details>


### [150] [Early Signs of Steganographic Capabilities in Frontier LLMs](https://arxiv.org/abs/2507.02737)
*Artur Zolkowski,Kei Nishimura-Gasparian,Robert McCarthy,Roland S. Zimmermann,David Lindner*

Main category: cs.CR

TL;DR: 评估前沿大语言模型（LLM）的隐写能力，发现当前模型在标准条件下难以隐藏短消息，但通过额外条件（如未监控的草稿）可能成功。初步迹象显示模型能进行基本编码推理，但尚无法隐蔽欺骗监控。


<details>
  <summary>Details</summary>
Motivation: 研究LLM的隐写能力以理解其潜在风险，特别是通过隐藏信息或编码推理规避监控。

Method: 评估两种隐写方式：传递编码消息和进行编码推理，测试模型在不同条件下的表现。

Result: 当前模型在标准条件下难以隐藏短消息，但通过额外条件可能成功；初步显示模型能进行基本编码推理。

Conclusion: 当前LLM的隐写能力尚不成熟，但未来可能发展出更强的规避监控能力。

Abstract: Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks
from misuse and misalignment. However, LLMs could evade monitoring through
steganography: Encoding hidden information within seemingly benign generations.
In this paper, we evaluate the steganography capabilities in frontier LLMs to
better understand the risk they pose. We focus on two types of steganography:
passing encoded messages and performing encoded reasoning. We find that current
models are unable to encode short messages in their outputs without a monitor
noticing under standard affordances. They can succeed, however, if given
additional affordances such as using an unmonitored scratchpad and coordinating
on what encoding scheme to use. We additionally find early signs that models
can perform basic encoded reasoning in a simple state-tracking problem. This
includes some ability to reason with their own and pre-defined schemes,
including encoding schemes such as Hexadecimal. Despite this, they can rarely
hide reasoning subtly within a cover task to fool a monitor. Overall, our
results indicate that current LLMs exhibit nascent steganographic capabilities.
While these capabilities are likely insufficient to bypass well-designed
monitors at present, this could change in the future.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [151] [Red grape detection with accelerated artificial neural networks in the FPGA's programmable logic](https://arxiv.org/abs/2507.02443)
*Sandro Costa Magalhães,Marco Almeida,Filipe Neves dos Santos,António Paulo Moreira,Jorge Dias*

Main category: cs.CV

TL;DR: 论文提出使用FPGA加速ANN，提升机器人检测速度，减少任务执行时间。


<details>
  <summary>Details</summary>
Motivation: 机器人因检测算法速度慢而降低移动速度，影响任务执行效率。

Method: 使用FINN架构在FPGA的PL中部署三种量化ANN模型（MobileNet v1、CNV 2-bit和CNV 1-bit），并在RG2C数据集上训练。

Result: MobileNet v1表现最佳，成功率达98%，推理速度达6611 FPS。

Conclusion: FPGA可有效加速ANN，适用于注意力机制，提升机器人性能。

Abstract: Robots usually slow down for canning to detect objects while moving.
Additionally, the robot's camera is configured with a low framerate to track
the velocity of the detection algorithms. This would be constrained while
executing tasks and exploring, making robots increase the task execution time.
AMD has developed the Vitis-AI framework to deploy detection algorithms into
FPGAs. However, this tool does not fully use the FPGAs' PL. In this work, we
use the FINN architecture to deploy three ANNs, MobileNet v1 with 4-bit
quantisation, CNV with 2-bit quantisation, and CNV with 1-bit quantisation
(BNN), inside an FPGA's PL. The models were trained on the RG2C dataset. This
is a self-acquired dataset released in open access. MobileNet v1 performed
better, reaching a success rate of 98 % and an inference speed of 6611 FPS. In
this work, we proved that we can use FPGAs to speed up ANNs and make them
suitable for attention mechanisms.

</details>


### [152] [SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers](https://arxiv.org/abs/2507.02212)
*Takuro Kawada,Shunsuke Kitada,Sota Nemoto,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: 论文介绍了SciGA-145k数据集，用于支持图形摘要（GA）的选择和推荐，并提出了两种任务和一种新评价指标CAR。


<details>
  <summary>Details</summary>
Motivation: 图形摘要在科学传播中很重要，但设计有效的GA需要高级可视化技能，限制了其广泛应用。

Method: 提出SciGA-145k数据集，定义Intra-GA和Inter-GA推荐任务，并开发CAR评价指标。

Result: 建立了大规模数据集和基线模型，为GA设计和研究提供了基础。

Conclusion: SciGA-145k为提升科学视觉传播和AI在科学中的应用奠定了基础。

Abstract: Graphical Abstracts (GAs) play a crucial role in visually conveying the key
findings of scientific papers. While recent research has increasingly
incorporated visual materials such as Figure 1 as de facto GAs, their potential
to enhance scientific communication remains largely unexplored. Moreover,
designing effective GAs requires advanced visualization skills, creating a
barrier to their widespread adoption. To tackle these challenges, we introduce
SciGA-145k, a large-scale dataset comprising approximately 145,000 scientific
papers and 1.14 million figures, explicitly designed for supporting GA
selection and recommendation as well as facilitating research in automated GA
generation. As a preliminary step toward GA design support, we define two
tasks: 1) Intra-GA recommendation, which identifies figures within a given
paper that are well-suited to serve as GAs, and 2) Inter-GA recommendation,
which retrieves GAs from other papers to inspire the creation of new GAs. We
provide reasonable baseline models for these tasks. Furthermore, we propose
Confidence Adjusted top-1 ground truth Ratio (CAR), a novel recommendation
metric that offers a fine-grained analysis of model behavior. CAR addresses
limitations in traditional ranking-based metrics by considering cases where
multiple figures within a paper, beyond the explicitly labeled GA, may also
serve as GAs. By unifying these tasks and metrics, our SciGA-145k establishes a
foundation for advancing visual scientific communication while contributing to
the development of AI for Science.

</details>


### [153] [Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization](https://arxiv.org/abs/2507.02288)
*De Cheng,Zhipeng Xu,Xinyang Jiang,Dongsheng Li,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: 本文提出了一种基于文本特征引导的视觉提示调优框架，结合大语言模型（LLM）解耦文本提示，并通过WERA方法增强视觉特征的解耦能力，显著提升了领域泛化（DG）性能。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练视觉基础模型（VFMs）在领域泛化中表现出潜力，但如何设计能够解耦跨领域不变特征的提示仍是一个关键挑战。本文旨在利用VFMs的可控语言提示来解决这一问题。

Method: 提出文本特征引导的视觉提示调优框架，使用LLM自动解耦文本提示，并通过WERA方法引入抽象提示以增强视觉特征的解耦能力。

Result: 在PACS、VLCS等主要DG数据集上的实验表明，该方法优于现有最先进的DG方法。

Conclusion: 通过结合文本引导和视觉特征解耦，本文方法显著提升了领域泛化性能，为跨领域模型设计提供了新思路。

Abstract: Domain Generalization (DG) seeks to develop a versatile model capable of
performing effectively on unseen target domains. Notably, recent advances in
pre-trained Visual Foundation Models (VFMs), such as CLIP, have demonstrated
considerable potential in enhancing the generalization capabilities of deep
learning models. Despite the increasing attention toward VFM-based domain
prompt tuning within DG, the effective design of prompts capable of
disentangling invariant features across diverse domains remains a critical
challenge. In this paper, we propose addressing this challenge by leveraging
the controllable and flexible language prompt of the VFM. Noting that the text
modality of VFMs is naturally easier to disentangle, we introduce a novel
framework for text feature-guided visual prompt tuning. This framework first
automatically disentangles the text prompt using a large language model (LLM)
and then learns domain-invariant visual representation guided by the
disentangled text feature. However, relying solely on language to guide visual
feature disentanglement has limitations, as visual features can sometimes be
too complex or nuanced to be fully captured by descriptive text. To address
this, we introduce Worst Explicit Representation Alignment (WERA), which
extends text-guided visual prompts by incorporating an additional set of
abstract prompts. These prompts enhance source domain diversity through
stylized image augmentations, while alignment constraints ensure that visual
representations remain consistent across both the original and augmented
distributions. Experiments conducted on major DG datasets, including PACS,
VLCS, OfficeHome, DomainNet, and TerraInc, demonstrate that our proposed method
outperforms state-of-the-art DG methods.

</details>


### [154] [TABNet: A Triplet Augmentation Self-Recovery Framework with Boundary-Aware Pseudo-Labels for Medical Image Segmentation](https://arxiv.org/abs/2507.02399)
*Peilin Zhang,Shaouxan Wua,Jun Feng,Zhuo Jin,Zhizezhang Gao,Jingkun Chen,Yaqiong Xing,Xiao Zhang*

Main category: cs.CV

TL;DR: TAB Net提出了一种基于涂鸦注释的弱监督医学图像分割框架，通过三重增强自恢复模块和边界感知伪标签监督模块，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 大规模完全注释的医学图像数据集获取成本高且耗时，涂鸦注释作为一种稀疏标注方式提供了高效替代方案，但其稀疏性限制了特征学习和边界监督。

Method: TAB Net包含三重增强自恢复模块（TAS）和边界感知伪标签监督模块（BAP），分别通过多种增强策略和边界感知损失提升分割性能。

Result: 在ACDC和MSCMR seg数据集上，TAB Net显著优于现有弱监督方法，性能接近全监督方法。

Conclusion: TAB Net为稀疏标注下的医学图像分割提供了高效解决方案，性能接近全监督方法。

Abstract: Background and objective: Medical image segmentation is a core task in
various clinical applications. However, acquiring large-scale, fully annotated
medical image datasets is both time-consuming and costly. Scribble annotations,
as a form of sparse labeling, provide an efficient and cost-effective
alternative for medical image segmentation. However, the sparsity of scribble
annotations limits the feature learning of the target region and lacks
sufficient boundary supervision, which poses significant challenges for
training segmentation networks. Methods: We propose TAB Net, a novel
weakly-supervised medical image segmentation framework, consisting of two key
components: the triplet augmentation self-recovery (TAS) module and the
boundary-aware pseudo-label supervision (BAP) module. The TAS module enhances
feature learning through three complementary augmentation strategies: intensity
transformation improves the model's sensitivity to texture and contrast
variations, cutout forces the network to capture local anatomical structures by
masking key regions, and jigsaw augmentation strengthens the modeling of global
anatomical layout by disrupting spatial continuity. By guiding the network to
recover complete masks from diverse augmented inputs, TAS promotes a deeper
semantic understanding of medical images under sparse supervision. The BAP
module enhances pseudo-supervision accuracy and boundary modeling by fusing
dual-branch predictions into a loss-weighted pseudo-label and introducing a
boundary-aware loss for fine-grained contour refinement. Results: Experimental
evaluations on two public datasets, ACDC and MSCMR seg, demonstrate that TAB
Net significantly outperforms state-of-the-art methods for scribble-based
weakly supervised segmentation. Moreover, it achieves performance comparable to
that of fully supervised methods.

</details>


### [155] [Wildlife Target Re-Identification Using Self-supervised Learning in Non-Urban Settings](https://arxiv.org/abs/2507.02403)
*Mufhumudzi Muthivhi,Terence L. van Zyl*

Main category: cs.CV

TL;DR: 该研究探讨了自监督学习在野生动物重识别中的应用，通过无监督方式从相机陷阱数据中提取图像对训练模型，实验表明自监督模型在数据有限时更鲁棒，且在下游任务中表现优于监督方法。


<details>
  <summary>Details</summary>
Motivation: 当前野生动物重识别依赖标注数据训练监督模型，这促使了大规模数据集的构建。本研究旨在探索自监督学习是否能减少对标注数据的依赖，并提升模型性能。

Method: 利用相机陷阱数据中的时序图像对自动提取个体视图，训练自监督模型，并在开放世界场景和下游任务中评估其性能。

Result: 自监督模型在数据有限时表现更鲁棒，且在所有下游任务中性能优于监督模型。

Conclusion: 自监督学习为野生动物重识别提供了一种高效且减少标注依赖的解决方案，具有广泛应用潜力。

Abstract: Wildlife re-identification aims to match individuals of the same species
across different observations. Current state-of-the-art (SOTA) models rely on
class labels to train supervised models for individual classification. This
dependence on annotated data has driven the curation of numerous large-scale
wildlife datasets. This study investigates self-supervised learning
Self-Supervised Learning (SSL) for wildlife re-identification. We automatically
extract two distinct views of an individual using temporal image pairs from
camera trap data without supervision. The image pairs train a self-supervised
model from a potentially endless stream of video data. We evaluate the learnt
representations against supervised features on open-world scenarios and
transfer learning in various wildlife downstream tasks. The analysis of the
experimental results shows that self-supervised models are more robust even
with limited data. Moreover, self-supervised features outperform supervision
across all downstream tasks. The code is available here
https://github.com/pxpana/SSLWildlife.

</details>


### [156] [Determination Of Structural Cracks Using Deep Learning Frameworks](https://arxiv.org/abs/2507.02416)
*Subhasis Dasgupta,Jaydip Sen,Tuhina Halder*

Main category: cs.CV

TL;DR: 论文提出了一种新型深度学习架构，通过集成残差U-Net模型和元模型，显著提升了结构裂缝检测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 手动检测结构裂缝存在速度慢、不一致和易出错的问题，影响评估可靠性。研究旨在通过深度学习解决这些问题。

Method: 采用多种残差U-Net模型配置，并将其集成到包含卷积块的元模型中，形成集成模型。

Result: 集成模型在IoU和DICE系数上表现最佳，尤其在低分辨率图像中优于SegNet和传统U-Net。

Conclusion: 该研究为结构缺陷监测任务提供了更可靠的自动化系统解决方案。

Abstract: Structural crack detection is a critical task for public safety as it helps
in preventing potential structural failures that could endanger lives. Manual
detection by inexperienced personnel can be slow, inconsistent, and prone to
human error, which may compromise the reliability of assessments. The current
study addresses these challenges by introducing a novel deep-learning
architecture designed to enhance the accuracy and efficiency of structural
crack detection. In this research, various configurations of residual U-Net
models were utilized. These models, due to their robustness in capturing fine
details, were further integrated into an ensemble with a meta-model comprising
convolutional blocks. This unique combination aimed to boost prediction
efficiency beyond what individual models could achieve. The ensemble's
performance was evaluated against well-established architectures such as SegNet
and the traditional U-Net. Results demonstrated that the residual U-Net models
outperformed their predecessors, particularly with low-resolution imagery, and
the ensemble model exceeded the performance of individual models, proving it as
the most effective. The assessment was based on the Intersection over Union
(IoU) metric and DICE coefficient. The ensemble model achieved the highest
scores, signifying superior accuracy. This advancement suggests way for more
reliable automated systems in structural defects monitoring tasks.

</details>


### [157] [MC-INR: Efficient Encoding of Multivariate Scientific Simulation Data using Meta-Learning and Clustered Implicit Neural Representations](https://arxiv.org/abs/2507.02494)
*Hyunsoo Son,Jeonghyun Noh,Suemin Jeon,Chaoli Wang,Won-Ki Jeong*

Main category: cs.CV

TL;DR: MC-INR是一种新型神经网络框架，通过结合元学习和聚类，解决了现有INR方法在复杂结构、多变量数据和非结构化网格上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有INR方法在复杂结构、多变量数据和非结构化网格上表现不佳，限制了其在实际应用中的效果。

Method: 提出MC-INR框架，结合元学习和聚类，引入残差动态重聚类机制和分支层，以灵活编码复杂结构和多变量数据。

Result: 实验表明，MC-INR在科学数据编码任务中优于现有方法。

Conclusion: MC-INR通过创新设计，显著提升了INR在复杂和多变量数据上的性能。

Abstract: Implicit Neural Representations (INRs) are widely used to encode data as
continuous functions, enabling the visualization of large-scale multivariate
scientific simulation data with reduced memory usage. However, existing
INR-based methods face three main limitations: (1) inflexible representation of
complex structures, (2) primarily focusing on single-variable data, and (3)
dependence on structured grids. Thus, their performance degrades when applied
to complex real-world datasets. To address these limitations, we propose a
novel neural network-based framework, MC-INR, which handles multivariate data
on unstructured grids. It combines meta-learning and clustering to enable
flexible encoding of complex structures. To further improve performance, we
introduce a residual-based dynamic re-clustering mechanism that adaptively
partitions clusters based on local error. We also propose a branched layer to
leverage multivariate data through independent branches simultaneously.
Experimental results demonstrate that MC-INR outperforms existing methods on
scientific data encoding tasks.

</details>


### [158] [Learning few-step posterior samplers by unfolding and distillation of diffusion models](https://arxiv.org/abs/2507.02686)
*Charlesquin Kemajou Mbakam,Jonathan Spence,Marcelo Pereyra*

Main category: cs.CV

TL;DR: 本文提出了一种新框架，通过深度展开和模型蒸馏将扩散模型（DM）转化为快速后验采样的条件模型，首次将深度展开应用于蒙特卡洛采样方案。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在贝叶斯计算成像中表现出强大的图像先验能力，但现有方法存在灵活性或精度不足的问题。

Method: 结合深度展开和模型蒸馏，将DM转化为快速采样的条件模型，并首次将深度展开应用于LATINO Langevin采样器。

Result: 实验表明，该方法在准确性和计算效率上优于现有技术，同时保持了推理时对前向模型变化的适应性。

Conclusion: 该框架为扩散模型在计算成像中的应用提供了高效且灵活的解决方案。

Abstract: Diffusion models (DMs) have emerged as powerful image priors in Bayesian
computational imaging. Two primary strategies have been proposed for leveraging
DMs in this context: Plug-and-Play methods, which are zero-shot and highly
flexible but rely on approximations; and specialized conditional DMs, which
achieve higher accuracy and faster inference for specific tasks through
supervised training. In this work, we introduce a novel framework that
integrates deep unfolding and model distillation to transform a DM image prior
into a few-step conditional model for posterior sampling. A central innovation
of our approach is the unfolding of a Markov chain Monte Carlo (MCMC) algorithm
- specifically, the recently proposed LATINO Langevin sampler (Spagnoletti et
al., 2025) - representing the first known instance of deep unfolding applied to
a Monte Carlo sampling scheme. We demonstrate our proposed unfolded and
distilled samplers through extensive experiments and comparisons with the state
of the art, where they achieve excellent accuracy and computational efficiency,
while retaining the flexibility to adapt to variations in the forward model at
inference time.

</details>


### [159] [Linear Attention with Global Context: A Multipole Attention Mechanism for Vision and Physics](https://arxiv.org/abs/2507.02748)
*Alex Colagrande,Paul Caillon,Eva Feillet,Alexandre Allauzen*

Main category: cs.CV

TL;DR: MANO是一种新型的注意力机制，通过多尺度距离计算实现线性复杂度，显著降低计算资源需求，性能媲美主流Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer的二次复杂度限制了其在高分辨率输入上的应用，现有方法往往牺牲细节。

Method: 受n体数值模拟启发，将注意力建模为网格点间的交互问题，提出多极注意力神经算子（MANO）。

Result: 在图像分类和Darcy流任务中，MANO性能与ViT和Swin Transformer相当，但计算资源和内存需求大幅降低。

Conclusion: MANO提供了一种高效且性能优越的Transformer替代方案，适用于大规模数据处理。

Abstract: Transformers have become the de facto standard for a wide range of tasks,
from image classification to physics simulations. Despite their impressive
performance, the quadratic complexity of standard Transformers in both memory
and time with respect to the input length makes them impractical for processing
high-resolution inputs. Therefore, several variants have been proposed, the
most successful relying on patchification, downsampling, or coarsening
techniques, often at the cost of losing the finest-scale details. In this work,
we take a different approach. Inspired by state-of-the-art techniques in
$n$-body numerical simulations, we cast attention as an interaction problem
between grid points. We introduce the Multipole Attention Neural Operator
(MANO), which computes attention in a distance-based multiscale fashion. MANO
maintains, in each attention head, a global receptive field and achieves linear
time and memory complexity with respect to the number of grid points. Empirical
results on image classification and Darcy flows demonstrate that MANO rivals
state-of-the-art models such as ViT and Swin Transformer, while reducing
runtime and peak memory usage by orders of magnitude. We open source our code
for reproducibility at https://github.com/AlexColagrande/MANO.

</details>


### [160] [Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory](https://arxiv.org/abs/2507.02863)
*Yuqi Wu,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: Point3R提出了一种在线框架，用于密集流式3D重建，通过显式空间指针内存解决隐式内存容量有限和信息丢失的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的隐式内存方法在密集3D重建中存在容量限制和早期帧信息丢失的问题，Point3R旨在通过显式空间指针内存解决这些问题。

Method: Point3R维护一个显式空间指针内存，每个指针与当前场景的3D结构直接关联，并通过3D分层位置嵌入和融合机制实现信息的高效整合。

Result: Point3R在多种任务中表现出竞争性或最先进的性能，且训练成本低。

Conclusion: Point3R通过显式空间指针内存和高效融合机制，实现了高质量的密集流式3D重建。

Abstract: Dense 3D scene reconstruction from an ordered sequence or unordered image
collections is a critical step when bringing research in computer vision into
practical scenarios. Following the paradigm introduced by DUSt3R, which unifies
an image pair densely into a shared coordinate system, subsequent methods
maintain an implicit memory to achieve dense 3D reconstruction from more
images. However, such implicit memory is limited in capacity and may suffer
from information loss of earlier frames. We propose Point3R, an online
framework targeting dense streaming 3D reconstruction. To be specific, we
maintain an explicit spatial pointer memory directly associated with the 3D
structure of the current scene. Each pointer in this memory is assigned a
specific 3D position and aggregates scene information nearby in the global
coordinate system into a changing spatial feature. Information extracted from
the latest frame interacts explicitly with this pointer memory, enabling dense
integration of the current observation into the global coordinate system. We
design a 3D hierarchical position embedding to promote this interaction and
design a simple yet effective fusion mechanism to ensure that our pointer
memory is uniform and efficient. Our method achieves competitive or
state-of-the-art performance on various tasks with low training costs. Code is
available at: https://github.com/YkiWu/Point3R.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [161] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar,Rushikesh K. Joshi*

Main category: cs.AI

TL;DR: HCVR是一种轻量级的基于规则的特征选择方法，结合P2P和P2T相关性，通过多数投票规则保留相关特征并消除冗余。


<details>
  <summary>Details</summary>
Motivation: 传统特征选择方法（如CFS、mRMR、MI、RFE、SFS和遗传算法）在性能上存在不足，HCVR旨在通过混合非迭代和迭代过滤方法提升性能。

Method: HCVR采用贪心算法，通过后向消除逐步剔除冗余特征，利用相关性阈值和多数投票规则决定特征保留与否。

Result: 在SPAMBASE数据集上，HCVR相比传统方法表现出更好的性能，分类器性能得到提升。

Conclusion: HCVR是一种有效的特征选择方法，适用于轻量级和高性能需求的场景。

Abstract: In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [162] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani,Yingxue Zhang,Derek Li,Qianyi Sun,Soumyasundar Pal,Zhanguang Zhang,Yaochen Hu,Rohan Deepak Ajwani,Antonios Valkanas,Raika Karimi,Peng Cheng,Yunzhou Wang,Pengyi Liao,Hanrui Huang,Bin Wang,Jianye Hao,Mark Coates*

Main category: cs.AI

TL;DR: 本文综述了大型语言模型（LLMs）在推理效率方面的挑战，并提出了一种两层次分类法（L1可控性和L2适应性）来优化计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在推理时计算效率低下，无法根据任务复杂度动态调整计算资源，导致简单问题过度计算而复杂问题计算不足。

Method: 提出两层次分类法：L1（固定计算预算）和L2（动态调整计算资源），并评估了主流LLMs的性能与计算开销。

Result: 通过实验展示了推理性能与计算开销之间的权衡，强调了TTC方法的实用性、适应性和可扩展性。

Conclusion: 未来研究方向包括混合思维模型，目标是提升LLMs的计算效率、鲁棒性和用户约束响应能力。

Abstract: Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [163] [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/abs/2507.02554)
*Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Rishi Hazra,Nicolas Baldwin,Alexis Audran-Reiss,Michael Kuchnik,Despoina Magka,Minqi Jiang,Alisia Maria Lupidi,Andrei Lupu,Roberta Raileanu,Kelvin Niu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Michael Shvartsman,Shagun Sodhani,Alexander H. Miller,Abhishek Charnalia,Derek Dunfield,Carole-Jean Wu,Pontus Stenetorp,Nicola Cancedda,Jakob Nicolaus Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: AI研究代理通过优化搜索策略和操作符集，在MLE-bench基准测试中显著提升了性能，成功率达到47.7%。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过自动化设计和优化机器学习模型来加速科学进展，特别是在Kaggle竞赛等实际场景中。

Method: 将AI研究代理形式化为搜索策略，通过设计不同的操作符集和搜索策略（贪婪、MCTS、进化算法）并系统比较其性能。

Result: 最佳搜索策略和操作符集的组合在MLE-bench lite上实现了47.7%的成功率，优于之前的39.6%。

Conclusion: 搜索策略、操作符设计和评估方法的联合优化对自动化机器学习的进步至关重要。

Abstract: AI research agents are demonstrating great potential to accelerate scientific
progress by automating the design, implementation, and training of machine
learning models. We focus on methods for improving agents' performance on
MLE-bench, a challenging benchmark where agents compete in Kaggle competitions
to solve real-world machine learning problems. We formalize AI research agents
as search policies that navigate a space of candidate solutions, iteratively
modifying them using operators. By designing and systematically varying
different operator sets and search policies (Greedy, MCTS, Evolutionary), we
show that their interplay is critical for achieving high performance. Our best
pairing of search strategy and operator set achieves a state-of-the-art result
on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from
39.6% to 47.7%. Our investigation underscores the importance of jointly
considering the search strategy, operator design, and evaluation methodology in
advancing automated machine learning.

</details>


### [164] [Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education](https://arxiv.org/abs/2507.02681)
*Behnam Parsaeifard,Christof Imhof,Tansu Pancar,Ioan-Sorin Comsa,Martin Hlosta,Nicole Bergamin,Per Bergamin*

Main category: cs.AI

TL;DR: 论文通过分析非强制性测验的学生日志数据，使用机器学习算法预测学生脱离行为，准确率达91%，并提供了可解释的框架和干预建议。


<details>
  <summary>Details</summary>
Motivation: 远程教育中学生脱离任务可能导致严重后果，如辍学，因此需要有效检测和干预方法。

Method: 从Moodle提取学生日志数据，训练并比较八种机器学习算法，使用SHAP方法解释模型决策。

Result: 模型平衡准确率达91%，85%的脱离学生被正确检测。

Conclusion: 研究提供了高预测性能和可解释框架，并讨论了如何设计及时干预以减少在线学习中学生的脱离行为。

Abstract: Students disengaging from their tasks can have serious long-term
consequences, including academic drop-out. This is particularly relevant for
students in distance education. One way to measure the level of disengagement
in distance education is to observe participation in non-mandatory exercises in
different online courses. In this paper, we detect student disengagement in the
non-mandatory quizzes of 42 courses in four semesters from a distance-based
university. We carefully identified the most informative student log data that
could be extracted and processed from Moodle. Then, eight machine learning
algorithms were trained and compared to obtain the highest possible prediction
accuracy. Using the SHAP method, we developed an explainable machine learning
framework that allows practitioners to better understand the decisions of the
trained algorithm. The experimental results show a balanced accuracy of 91\%,
where about 85\% of disengaged students were correctly detected. On top of the
highly predictive performance and explainable framework, we provide a
discussion on how to design a timely intervention to minimise disengagement
from voluntary tasks in online learning.

</details>


### [165] [Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving](https://arxiv.org/abs/2507.02726)
*Matthieu Zimmer,Xiaotong Ji,Rasul Tutunov,Anthony Bordg,Jun Wang,Haitham Bou Ammar*

Main category: cs.AI

TL;DR: 论文提出了一种自生成目标条件MDPs（sG-MDPs）框架，结合蒙特卡洛树搜索（MCTS）算法，用于解决大型语言模型在自动定理证明中的推理挑战，并在PutnamBench上取得了新突破。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自动定理证明中面临稀疏奖励和证明规模庞大的挑战，尤其是在复杂多步推理的大学级问题中表现不佳。

Method: 引入sG-MDPs框架，使代理能够根据证明状态生成并追求子目标，并应用MCTS算法解决问题。

Result: 在PutnamBench上，Bourbaki（7B）解决了26个问题，达到了该规模模型的新最佳性能。

Conclusion: sG-MDPs框架结合MCTS算法显著提升了大型语言模型在复杂推理任务中的表现。

Abstract: Reasoning remains a challenging task for large language models (LLMs),
especially within the logically constrained environment of automated theorem
proving (ATP), due to sparse rewards and the vast scale of proofs. These
challenges are amplified in benchmarks like PutnamBench, which contains
university-level problems requiring complex, multi-step reasoning. To address
this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new
framework in which agents generate and pursue their subgoals based on the
evolving proof state. Given this more structured generation of goals, the
resulting problem becomes more amenable to search. We then apply Monte Carlo
Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our
approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs
for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)
solves 26 problems, achieving new state-of-the-art results with models at this
scale.

</details>


### [166] [Grounding Intelligence in Movement](https://arxiv.org/abs/2507.02771)
*Melanie Segado,Felipe Parodi,Jordan K. Matelsky,Michael L. Platt,Eva B. Dyer,Konrad P. Kording*

Main category: cs.AI

TL;DR: 论文主张将运动作为AI建模的主要目标，强调其跨领域、结构化及物理基础的重要性。


<details>
  <summary>Details</summary>
Motivation: 运动在生物系统中至关重要，但现有AI模型对其建模不足，且数据收集和建模方法碎片化。

Method: 提出将运动视为独立且结构化的模态，利用其低维表示（如姿态）进行建模。

Result: 强调运动建模能提升生成模型和控制能力，并为理解生物与人工系统的行为提供共同基础。

Conclusion: 运动不仅是行为结果，更是智能系统与世界互动的窗口，应作为AI核心建模目标。

Abstract: Recent advances in machine learning have dramatically improved our ability to
model language, vision, and other high-dimensional data, yet they continue to
struggle with one of the most fundamental aspects of biological systems:
movement. Across neuroscience, medicine, robotics, and ethology, movement is
essential for interpreting behavior, predicting intent, and enabling
interaction. Despite its core significance in our intelligence, movement is
often treated as an afterthought rather than as a rich and structured modality
in its own right. This reflects a deeper fragmentation in how movement data is
collected and modeled, often constrained by task-specific goals and
domain-specific assumptions. But movement is not domain-bound. It reflects
shared physical constraints, conserved morphological structures, and purposeful
dynamics that cut across species and settings. We argue that movement should be
treated as a primary modeling target for AI. It is inherently structured and
grounded in embodiment and physics. This structure, often allowing for compact,
lower-dimensional representations (e.g., pose), makes it more interpretable and
computationally tractable to model than raw, high-dimensional sensory inputs.
Developing models that can learn from and generalize across diverse movement
data will not only advance core capabilities in generative modeling and
control, but also create a shared foundation for understanding behavior across
biological and artificial systems. Movement is not just an outcome, it is a
window into how intelligent systems engage with the world.

</details>


### [167] [KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs](https://arxiv.org/abs/2507.02773)
*Yuzhang Xie,Hejie Cui,Ziyang Zhang,Jiaying Lu,Kai Shu,Fadi Nahab,Xiao Hu,Carl Yang*

Main category: cs.AI

TL;DR: KERAP是一种基于知识图谱的多智能体框架，用于提升LLM在零样本医疗诊断预测中的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型依赖监督训练，泛化能力有限；LLM在医疗诊断中存在幻觉和缺乏结构化推理的问题。

Method: 提出KERAP框架，包含属性映射的链接智能体、结构化知识提取的检索智能体和迭代优化预测的预测智能体。

Result: 实验证明KERAP能高效提升诊断可靠性，适用于零样本医疗诊断预测。

Conclusion: KERAP为医疗诊断预测提供了可扩展且可解释的解决方案。

Abstract: Medical diagnosis prediction plays a critical role in disease detection and
personalized healthcare. While machine learning (ML) models have been widely
adopted for this task, their reliance on supervised training limits their
ability to generalize to unseen cases, particularly given the high cost of
acquiring large, labeled datasets. Large language models (LLMs) have shown
promise in leveraging language abilities and biomedical knowledge for diagnosis
prediction. However, they often suffer from hallucinations, lack structured
medical reasoning, and produce useless outputs. To address these challenges, we
propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves
LLM-based diagnosis prediction through a multi-agent architecture. Our
framework consists of a linkage agent for attribute mapping, a retrieval agent
for structured knowledge extraction, and a prediction agent that iteratively
refines diagnosis predictions. Experimental results demonstrate that KERAP
enhances diagnostic reliability efficiently, offering a scalable and
interpretable solution for zero-shot medical diagnosis prediction.

</details>


### [168] [StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason](https://arxiv.org/abs/2507.02841)
*Kaiyi Zhang,Ang Lv,Jinpeng Li,Yongbo Wang,Feng Wang,Haoyuan Hu,Rui Yan*

Main category: cs.AI

TL;DR: StepHint是一种新的RLVR算法，通过多级逐步提示帮助模型更有效地探索解空间，解决了近失奖励问题和探索停滞问题。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法存在近失奖励问题和探索停滞问题，限制了训练效率和模型能力。

Method: StepHint利用多级逐步提示，从强模型中生成有效推理链并自适应分区，提供不同级别的提示以引导模型探索。

Result: StepHint在六个数学基准测试中优于其他RLVR增强方法，并表现出更好的泛化能力。

Conclusion: StepHint通过多级提示有效解决了RLVR的关键问题，提升了模型推理能力和训练效率。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for improving the complex reasoning abilities of large language models (LLMs).
However, current RLVR methods face two significant challenges: the near-miss
reward problem, where a small mistake can invalidate an otherwise correct
reasoning process, greatly hindering training efficiency; and exploration
stagnation, where models tend to focus on solutions within their ``comfort
zone,'' lacking the motivation to explore potentially more effective
alternatives. To address these challenges, we propose StepHint, a novel RLVR
algorithm that utilizes multi-level stepwise hints to help models explore the
solution space more effectively. StepHint generates valid reasoning chains from
stronger models and partitions these chains into reasoning steps using our
proposed adaptive partitioning method. The initial few steps are used as hints,
and simultaneously, multiple-level hints (each comprising a different number of
steps) are provided to the model. This approach directs the model's exploration
toward a promising solution subspace while preserving its flexibility for
independent exploration. By providing hints, StepHint mitigates the near-miss
reward problem, thereby improving training efficiency. Additionally, the
external reasoning pathways help the model develop better reasoning abilities,
enabling it to move beyond its ``comfort zone'' and mitigate exploration
stagnation. StepHint outperforms competitive RLVR enhancement methods across
six mathematical benchmarks, while also demonstrating superior generalization
and excelling over baselines on out-of-domain benchmarks.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [169] [A Scalable and Quantum-Accurate Foundation Model for Biomolecular Force Field via Linearly Tensorized Quadrangle Attention](https://arxiv.org/abs/2507.00884)
*Qun Su,Kai Zhu,Qiaolin Gou,Jintu Zhang,Renling Hu,Yurong Li,Yongze Wang,Hui Zhang,Ziyi You,Linlong Jiang,Yu Kang,Jike Wang,Chang-Yu Hsieh,Tingjun Hou*

Main category: physics.chem-ph

TL;DR: LiTEN是一种新型的等变神经网络，通过Tensorized Quadrangle Attention（TQA）高效建模三体和四体相互作用，结合LiTEN-FF预训练模型，在生物分子模拟中实现了量子力学级别的精度和高效性。


<details>
  <summary>Details</summary>
Motivation: 现有生物分子模拟方法（经典力场和量子力学方法）在精度和效率上存在局限，AI力场（AIFFs）虽试图解决这一问题，但面临多体建模复杂性、训练数据不足和泛化性验证不足的挑战。

Method: 提出LiTEN模型，利用TQA通过向量操作重新参数化高阶张量特征，避免昂贵的球谐函数计算；LiTEN-FF是基于nablaDFT数据集预训练的AIFF基础模型，并在SPICE数据集上微调。

Result: LiTEN在rMD17、MD22和Chignolin等评估子集上表现优于MACE、NequIP和EquiFormer等领先模型；LiTEN-FF在生物分子建模任务中提供10倍于MACE-OFF的推理速度。

Conclusion: LiTEN框架为复杂生物分子建模提供了物理基础的高效解决方案，适用于药物发现等领域。

Abstract: Accurate atomistic biomolecular simulations are vital for disease mechanism
understanding, drug discovery, and biomaterial design, but existing simulation
methods exhibit significant limitations. Classical force fields are efficient
but lack accuracy for transition states and fine conformational details
critical in many chemical and biological processes. Quantum Mechanics (QM)
methods are highly accurate but computationally infeasible for large-scale or
long-time simulations. AI-based force fields (AIFFs) aim to achieve QM-level
accuracy with efficiency but struggle to balance many-body modeling complexity,
accuracy, and speed, often constrained by limited training data and
insufficient validation for generalizability. To overcome these challenges, we
introduce LiTEN, a novel equivariant neural network with Tensorized Quadrangle
Attention (TQA). TQA efficiently models three- and four-body interactions with
linear complexity by reparameterizing high-order tensor features via vector
operations, avoiding costly spherical harmonics. Building on LiTEN, LiTEN-FF is
a robust AIFF foundation model, pre-trained on the extensive nablaDFT dataset
for broad chemical generalization and fine-tuned on SPICE for accurate solvated
system simulations. LiTEN achieves state-of-the-art (SOTA) performance across
most evaluation subsets of rMD17, MD22, and Chignolin, outperforming leading
models such as MACE, NequIP, and EquiFormer. LiTEN-FF enables the most
comprehensive suite of downstream biomolecular modeling tasks to date,
including QM-level conformer searches, geometry optimization, and free energy
surface construction, while offering 10x faster inference than MACE-OFF for
large biomolecules (~1000 atoms). In summary, we present a physically grounded,
highly efficient framework that advances complex biomolecular modeling,
providing a versatile foundation for drug discovery and related applications.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [170] [Resolving CAP Through Automata-Theoretic Economic Design: A Unified Mathematical Framework for Real-Time Partition-Tolerant Systems](https://arxiv.org/abs/2507.02464)
*Craig S Wright*

Main category: cs.GT

TL;DR: 本文通过自动机理论和经济学框架，将CAP定理的权衡问题转化为约束优化问题，证明在有限误差范围内可同时保持一致性和可用性。


<details>
  <summary>Details</summary>
Motivation: CAP定理提出了分布式系统中一致性、可用性和分区容忍性的三难问题，本文旨在通过经济激励和自动机理论扩展其限制。

Method: 将分布式系统建模为分区感知状态机，嵌入经济激励层以稳定共识行为，并结合博弈论机制定义全局转换语义。

Result: 结果表明，在有限误差范围内可以同时保持可用性和一致性，突破了传统CAP定理的限制。

Conclusion: 通过形式化的经济控制，本文扩展了CAP定理的边界，为分布式系统设计提供了新的理论支持。

Abstract: The CAP theorem asserts a trilemma between consistency, availability, and
partition tolerance. This paper introduces a rigorous automata-theoretic and
economically grounded framework that reframes the CAP trade-off as a constraint
optimization problem. We model distributed systems as partition-aware state
machines and embed economic incentive layers to stabilize consensus behavior
across adversarially partitioned networks. By incorporating game-theoretic
mechanisms into the global transition semantics, we define provable bounds on
convergence, liveness, and correctness. Our results demonstrate that
availability and consistency can be simultaneously preserved within bounded
epsilon margins, effectively extending the classical CAP limits through formal
economic control.

</details>


### [171] [Learning to Coordinate Bidders in Non-Truthful Auctions](https://arxiv.org/abs/2507.02801)
*Hu Fu,Tao Lin*

Main category: cs.GT

TL;DR: 论文研究了在非真实拍卖中学习贝叶斯相关均衡（BCE）的样本复杂度，证明了在一大类拍卖中，包括第一价格和全支付拍卖，可以用多项式样本数学习BCE。


<details>
  <summary>Details</summary>
Motivation: 非真实拍卖中的独立策略行为和贝叶斯纳什均衡难以刻画且可能导致不良结果，而BCE通过协调投标人行为提供了一种替代方案，但需要投标人私有估值的分布信息，而这通常是未知的。

Method: 通过将问题转化为估计投标人期望效用，并结合对单调投标策略类伪维度的分析，证明了BCE的可学习性。

Result: 证明了在一大类非真实拍卖中，包括第一价格和全支付拍卖，可以用多项式样本数学习BCE。

Conclusion: 研究为在缺乏投标人估值分布信息的情况下实现BCE提供了理论基础，扩展了拍卖设计的可能性。

Abstract: In non-truthful auctions such as first-price and all-pay auctions, the
independent strategic behaviors of bidders, with the corresponding equilibrium
notion -- Bayes Nash equilibria -- are notoriously difficult to characterize
and can cause undesirable outcomes. An alternative approach to designing better
auction systems is to coordinate the bidders: let a mediator make
incentive-compatible recommendations of correlated bidding strategies to the
bidders, namely, implementing a Bayes correlated equilibrium (BCE). The
implementation of BCE, however, requires knowledge of the distribution of
bidders' private valuations, which is often unavailable. We initiate the study
of the sample complexity of learning Bayes correlated equilibria in
non-truthful auctions. We prove that the BCEs in a large class of non-truthful
auctions, including first-price and all-pay auctions, can be learned with a
polynomial number $\tilde O(\frac{n}{\varepsilon^2})$ of samples from the
bidders' value distributions. Our technique is a reduction to the problem of
estimating bidders' expected utility from samples, combined with an analysis of
the pseudo-dimension of the class of all monotone bidding strategies of
bidders.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [172] [Selective Feature Re-Encoded Quantum Convolutional Neural Network with Joint Optimization for Image Classification](https://arxiv.org/abs/2507.02086)
*Shaswata Mahernob Sarkar,Sheikh Iftekhar Ahmed,Jishnu Mahmud,Shaikh Anowarul Fattah,Gaurav Sharma*

Main category: quant-ph

TL;DR: 该研究提出了一种选择性特征重编码策略和并行模式QCNN架构，显著提升了量子卷积神经网络在图像分类任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 利用NISQ设备的进步和量子原理（如纠缠和叠加），探索QCNN在图像分类中的潜力，并解决特征处理和模型优化的挑战。

Method: 提出选择性特征重编码策略和并行模式QCNN架构，结合PCA和自动编码器的特征提取方法，进行联合优化训练。

Result: 实验表明，选择性特征重编码提升了量子电路性能，并行QCNN架构在MNIST和Fashion MNIST数据集上表现优于独立模型和传统集成方法。

Conclusion: 该方法在量子机器学习中展示了更高的分类准确性和泛化能力，为QML的实际应用提供了新思路。

Abstract: Quantum Machine Learning (QML) has seen significant advancements, driven by
recent improvements in Noisy Intermediate-Scale Quantum (NISQ) devices.
Leveraging quantum principles such as entanglement and superposition, quantum
convolutional neural networks (QCNNs) have demonstrated promising results in
classifying both quantum and classical data. This study examines QCNNs in the
context of image classification and proposes a novel strategy to enhance
feature processing and a QCNN architecture for improved classification
accuracy. First, a selective feature re-encoding strategy is proposed, which
directs the quantum circuits to prioritize the most informative features,
thereby effectively navigating the crucial regions of the Hilbert space to find
the optimal solution space. Secondly, a novel parallel-mode QCNN architecture
is designed to simultaneously incorporate features extracted by two classical
methods, Principal Component Analysis (PCA) and Autoencoders, within a unified
training scheme. The joint optimization involved in the training process allows
the QCNN to benefit from complementary feature representations, enabling better
mutual readjustment of model parameters. To assess these methodologies,
comprehensive experiments have been performed using the widely used MNIST and
Fashion MNIST datasets for binary classification tasks. Experimental findings
reveal that the selective feature re-encoding method significantly improves the
quantum circuit's feature processing capability and performance. Furthermore,
the jointly optimized parallel QCNN architecture consistently outperforms the
individual QCNN models and the traditional ensemble approach involving
independent learning followed by decision fusion, confirming its superior
accuracy and generalization capabilities.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [173] [Measurement as Bricolage: Examining How Data Scientists Construct Target Variables for Predictive Modeling Tasks](https://arxiv.org/abs/2507.02819)
*Luke Guerdan,Devansh Saxena,Stevie Chancellor,Zhiwei Steven Wu,Kenneth Holstein*

Main category: cs.HC

TL;DR: 数据科学家通过拼凑过程构建目标变量，涉及高层测量目标与低层实际约束的迭代协商，满足有效性、简单性、可预测性、可移植性和资源需求等标准。


<details>
  <summary>Details</summary>
Motivation: 研究数据科学家如何将模糊概念转化为具体的代理目标变量，以填补这一过程的理解空白。

Method: 访谈15位教育（8人）和医疗（7人）领域的数据科学家，分析其目标变量构建过程。

Result: 数据科学家通过拼凑过程构建目标变量，使用问题重构策略（如替换或组合变量）满足五大标准。

Conclusion: 研究为HCI、CSCW和ML领域提供了未来研究方向，以更好地支持目标变量构建的科学与艺术。

Abstract: Data scientists often formulate predictive modeling tasks involving fuzzy,
hard-to-define concepts, such as the "authenticity" of student writing or the
"healthcare need" of a patient. Yet the process by which data scientists
translate fuzzy concepts into a concrete, proxy target variable remains poorly
understood. We interview fifteen data scientists in education (N=8) and
healthcare (N=7) to understand how they construct target variables for
predictive modeling tasks. Our findings suggest that data scientists construct
target variables through a bricolage process, involving iterative negotiation
between high-level measurement objectives and low-level practical constraints.
Data scientists attempt to satisfy five major criteria for a target variable
through bricolage: validity, simplicity, predictability, portability, and
resource requirements. To achieve this, data scientists adaptively use problem
(re)formulation strategies, such as swapping out one candidate target variable
for another when the first fails to meet certain criteria (e.g.,
predictability), or composing multiple outcomes into a single target variable
to capture a more holistic set of modeling objectives. Based on our findings,
we present opportunities for future HCI, CSCW, and ML research to better
support the art and science of target variable construction.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [174] [Self-Steering Deep Non-Linear Spatially Selective Filters for Efficient Extraction of Moving Speakers under Weak Guidance](https://arxiv.org/abs/2507.02791)
*Jakob Kienegger,Alina Mannanova,Huajian Fang,Timo Gerkmann*

Main category: eess.AS

TL;DR: 论文提出了一种低复杂度的粒子滤波跟踪算法，用于动态场景中的实时语音增强，通过时间反馈提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度非线性空间选择性滤波器在静态场景中表现优异，但在动态场景中需要高资源的数据驱动跟踪算法，限制了其在实时应用中的可行性。

Method: 采用低复杂度的粒子滤波跟踪算法，并结合时间反馈机制，利用增强的语音信号补偿粒子滤波的建模限制。

Result: 在合成数据集上，该方法显著提高了跟踪准确性并增强了语音性能；真实录音的听力测试也显示其优于其他方法。

Conclusion: 提出的自导向管道在动态场景中实现了高效且低复杂度的语音增强，优于现有方法。

Abstract: Recent works on deep non-linear spatially selective filters demonstrate
exceptional enhancement performance with computationally lightweight
architectures for stationary speakers of known directions. However, to maintain
this performance in dynamic scenarios, resource-intensive data-driven tracking
algorithms become necessary to provide precise spatial guidance conditioned on
the initial direction of a target speaker. As this additional computational
overhead hinders application in resource-constrained scenarios such as
real-time speech enhancement, we present a novel strategy utilizing a
low-complexity tracking algorithm in the form of a particle filter instead.
Assuming a causal, sequential processing style, we introduce temporal feedback
to leverage the enhanced speech signal of the spatially selective filter to
compensate for the limited modeling capabilities of the particle filter.
Evaluation on a synthetic dataset illustrates how the autoregressive interplay
between both algorithms drastically improves tracking accuracy and leads to
strong enhancement performance. A listening test with real-world recordings
complements these findings by indicating a clear trend towards our proposed
self-steering pipeline as preferred choice over comparable methods.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [175] [Resolving Turbulent Magnetohydrodynamics: A Hybrid Operator-Diffusion Framework](https://arxiv.org/abs/2507.02106)
*Semih Kacmaz,E. A. Huerta,Roland Haas*

Main category: physics.flu-dyn

TL;DR: 提出了一种结合物理信息神经算子（PINO）和基于分数的生成扩散模型的混合机器学习框架，用于模拟二维不可压缩电阻磁流体动力学（MHD）湍流的时空演化。


<details>
  <summary>Details</summary>
Motivation: 传统确定性替代模型在高雷诺数湍流模拟中表现不佳，需要一种能够准确捕捉高频动态的方法。

Method: 结合PINO预测低频动态，扩散模型校正高频残差，训练于多雷诺数的高保真模拟数据。

Result: 在Re=1000和3000时准确重建能谱分布，捕捉非高斯统计和间歇结构；在Re=10000时首次恢复磁场高波数演化。

Conclusion: 该框架在高雷诺数湍流模拟中表现出色，填补了确定性模型的空白。

Abstract: We present a hybrid machine learning framework that combines Physics-Informed
Neural Operators (PINOs) with score-based generative diffusion models to
simulate the full spatio-temporal evolution of two-dimensional, incompressible,
resistive magnetohydrodynamic (MHD) turbulence across a broad range of Reynolds
numbers ($\mathrm{Re}$). The framework leverages the equation-constrained
generalization capabilities of PINOs to predict coherent, low-frequency
dynamics, while a conditional diffusion model stochastically corrects
high-frequency residuals, enabling accurate modeling of fully developed
turbulence. Trained on a comprehensive ensemble of high-fidelity simulations
with $\mathrm{Re} \in \{100, 250, 500, 750, 1000, 3000, 10000\}$, the approach
achieves state-of-the-art accuracy in regimes previously inaccessible to
deterministic surrogates. At $\mathrm{Re}=1000$ and $3000$, the model
faithfully reconstructs the full spectral energy distributions of both velocity
and magnetic fields late into the simulation, capturing non-Gaussian
statistics, intermittent structures, and cross-field correlations with high
fidelity. At extreme turbulence levels ($\mathrm{Re}=10000$), it remains the
first surrogate capable of recovering the high-wavenumber evolution of the
magnetic field, preserving large-scale morphology and enabling statistically
meaningful predictions.

</details>
