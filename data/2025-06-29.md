<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.DS](#cs.DS) [Total: 10]
- [cs.SE](#cs.SE) [Total: 11]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 77]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [cs.CV](#cs.CV) [Total: 12]
- [q-bio.CB](#q-bio.CB) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [math.DS](#math.DS) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [cs.CL](#cs.CL) [Total: 6]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [econ.GN](#econ.GN) [Total: 1]
- [stat.ML](#stat.ML) [Total: 9]
- [math.OC](#math.OC) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Condensed Representation of RDF and its Application on Graph Versioning](https://arxiv.org/abs/2506.21203)
*Jey Puget Gil,Emmanuel Coquery,John Samuel,Gilles Gesquiere*

Main category: cs.DB

TL;DR: 本文提出并形式化了动态知识图的浓缩表示方法，以支持对实体关系演化的理解和未来趋势预测。


<details>
  <summary>Details</summary>
Motivation: 研究动态现象有助于理解实体关系随时间的变化并预测未来趋势，动态知识图能有效建模多源异构数据，但需组织这些数据以便分析。

Method: 提出并形式化了动态知识图的浓缩表示方法。

Result: 该方法使动态知识图更易于访问和利用。

Conclusion: 浓缩表示方法为动态知识图的管理和分析提供了有效工具。

Abstract: The study of the evolving phenomena in a domain helps to understand the
relationships between entities at different points in time and predict future
trends. These phenomena, often complex, can be represented using knowledge
graphs, which have the capability to model heterogeneous data from multiple
sources. Nowadays, a considerable amount of sources delivering periodic updates
to knowledge graphs in various domains is openly available. The evolution of
data is of interest to knowledge graph management systems, and therefore it is
crucial to organize these constantly evolving data to make them easily
accessible and exploitable for analyzes. In this article, we will present and
formalize the condensed representation of these evolving graphs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [ClusterRCA: Network Failure Diagnosis in HPC Systems Using Multimodal Data](https://arxiv.org/abs/2506.20673)
*Yongqian Sun,Xijie Pan,Xiao Xiong,Lei Tao,Jiaju Wang,Shenglin Zhang,Yuan Yuan,Yuqi Li,Kunlin Jian*

Main category: cs.DC

TL;DR: ClusterRCA是一种新型框架，用于高性能计算（HPC）系统中的网络故障诊断，通过多模态数据定位故障节点和类型。


<details>
  <summary>Details</summary>
Motivation: 现有方法因数据异构性和准确性不足，无法直接应用于HPC场景。

Method: ClusterRCA结合分类器和图方法，从拓扑连接的NIC对中提取特征，构建故障图并进行定制随机游走以定位根因。

Result: 实验表明，ClusterRCA在HPC系统中诊断网络故障的准确性高，且在不同应用场景中表现稳健。

Conclusion: ClusterRCA为HPC系统提供了一种高效的网络故障诊断解决方案。

Abstract: Network failure diagnosis is challenging yet critical for high-performance
computing (HPC) systems. Existing methods cannot be directly applied to HPC
scenarios due to data heterogeneity and lack of accuracy. This paper proposes a
novel framework, called ClusterRCA, to localize culprit nodes and determine
failure types by leveraging multimodal data. ClusterRCA extracts features from
topologically connected network interface controller (NIC) pairs to analyze the
diverse, multimodal data in HPC systems. To accurately localize culprit nodes
and determine failure types, ClusterRCA combines classifier-based and
graph-based approaches. A failure graph is constructed based on the output of
the state classifier, and then it performs a customized random walk on the
graph to localize the root cause. Experiments on datasets collected by a
top-tier global HPC device vendor show ClusterRCA achieves high accuracy in
diagnosing network failure for HPC systems. ClusterRCA also maintains robust
performance across different application scenarios.

</details>


### [3] [Scalable GPU Performance Variability Analysis framework](https://arxiv.org/abs/2506.20674)
*Ankur Lahiry,Ayush Pokharel,Seth Ockerman,Amal Gueroudji,Line Pouchard,Tanzima Z. Islam*

Main category: cs.DC

TL;DR: 论文提出了一种分布式数据分析框架，用于高效处理大规模GPU性能日志，解决了现有工具在内存和运行时上的限制。


<details>
  <summary>Details</summary>
Motivation: 处理GPU性能日志时，现有工具需要大量内存和长时间运行，无法满足自动化工作流的需求。

Method: 采用分布式框架，将数据集分片并并行处理，减少单节点内存压力，避免瓶颈。

Result: 成功应用于真实HPC和AI工作负载，诊断性能变异性并揭示内存传输延迟对GPU内核行为的影响。

Conclusion: 该框架显著提升了大规模性能日志的分析效率，适用于复杂和高维数据。

Abstract: Analyzing large-scale performance logs from GPU profilers often requires
terabytes of memory and hours of runtime, even for basic summaries. These
constraints prevent timely insight and hinder the integration of performance
analytics into automated workflows. Existing analysis tools typically process
data sequentially, making them ill-suited for HPC workflows with growing trace
complexity and volume. We introduce a distributed data analysis framework that
scales with dataset size and compute availability. Rather than treating the
dataset as a single entity, our system partitions it into independently
analyzable shards and processes them concurrently across MPI ranks. This design
reduces per-node memory pressure, avoids central bottlenecks, and enables
low-latency exploration of high-dimensional trace data. We apply the framework
to end-to-end Nsight Compute traces from real HPC and AI workloads, demonstrate
its ability to diagnose performance variability, and uncover the impact of
memory transfer latency on GPU kernel behavior.

</details>


### [4] [Utility-Driven Speculative Decoding for Mixture-of-Experts](https://arxiv.org/abs/2506.20675)
*Anish Saxena,Po-An Tsai,Hritvik Taneja,Aamer Jaleel,Moinuddin Qureshi*

Main category: cs.DC

TL;DR: GPU内存带宽是低延迟大型语言模型（LLM）推理的主要瓶颈。推测解码利用空闲GPU计算，通过轻量级草案器提出K个令牌，LLM并行验证以提高令牌吞吐量。传统密集LLM每次迭代都获取所有模型权重，推测不会增加延迟开销。然而，混合专家（MoE）模型每令牌仅激活部分权重，推测在MoE中效果不佳，草案令牌集体激活更多权重，增加数据移动和验证时间2-3倍。推测可能导致1.5倍的减速。Cascade框架通过选择性启用推测和动态调整K值，避免减速并加速MoE服务。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型中推测解码的低效问题，避免推测导致的性能下降。

Method: 提出Cascade框架，通过轻量级指标“推测效用”动态选择是否启用推测并调整K值。

Result: Cascade在vLLM中实现，评估显示其将减速限制在5%（相比1.5倍），吞吐量提高7-14%。

Conclusion: Cascade使推测解码在MoE中变得实用。

Abstract: GPU memory bandwidth is the main bottleneck for low-latency Large Language
Model (LLM) inference. Speculative decoding leverages idle GPU compute by using
a lightweight drafter to propose K tokens, which the LLM verifies in parallel,
boosting token throughput. In conventional dense LLMs, all model weights are
fetched each iteration, so speculation adds no latency overhead. Emerging
Mixture of Experts (MoE) models activate only a subset of weights per token,
greatly reducing data movement. However, we show that speculation is
ineffective for MoEs: draft tokens collectively activate more weights,
increasing data movement and verification time by 2-3x. When token throughput
gains fail to offset this overhead, speculation causes slowdowns up to 1.5x,
making it infeasible. Even when useful, the optimal K varies by task, model,
and even between requests and iterations. Thus, despite widespread use in dense
LLMs, speculation remains impractical in leading MoEs.
  We present Cascade, a utility-driven framework that selectively enables
speculation to avoid slowdowns and dynamically tunes K to accelerate MoE
serving. Cascade uses a lightweight metric, speculation utility, the ratio of
token gains to verification cost, which shows iteration-level locality,
enabling periodic decisions via short test and longer set phases. For each
request, Cascade disables speculation if utility drops below one during
testing, and when utility exceeds one, tests multiple K-values to choose the
utility-maximizing K for the set phase. We implement Cascade in vLLM and
evaluate it on five popular MoEs with workloads spanning code, math,
extraction, and mixed tasks. Cascade limits slowdown to 5% (vs. 1.5x) and
improves throughput by 7-14% over static K, making speculative decoding
practical for MoEs.

</details>


### [5] [ParEval-Repo: A Benchmark Suite for Evaluating LLMs with Repository-level HPC Translation Tasks](https://arxiv.org/abs/2506.20938)
*Joshua H. Davis,Daniel Nichols,Ishan Khillan,Abhinav Bhatele*

Main category: cs.DC

TL;DR: 论文提出了一个名为ParEval-Repo的基准测试框架，用于评估基于LLM的代码翻译方法在跨GPGPU执行模型中的效果。


<details>
  <summary>Details</summary>
Motivation: GPGPU架构的多样性导致需要大量开发者努力来优化不同硬件架构，LLM的进展可以减轻这一负担。

Method: 开发了ParEval-Repo框架，包含多种编程模型的科学计算和AI小应用，评估了开源和商业LLM在代码翻译中的表现。

Result: LLM翻译小型科学应用可行，但在生成功能性构建系统和跨文件依赖方面存在挑战。

Conclusion: LLM在代码翻译中潜力有限，需进一步解决构建系统和依赖问题。

Abstract: GPGPU architectures have become significantly diverse in recent years, which
has led to an emergence of a variety of specialized programming models and
software stacks to support them. While portable execution models exist, they
still require significant developer effort to port to and optimize for
different hardware architectures. Recent advances in large language models
(LLMs) can help us reduce some of this programmer burden. In this paper, we
present a novel benchmark and testing framework, ParEval-Repo, which can be
used to evaluate the efficacy of LLM-based approaches in automatically
translating entire codebases across GPGPU execution models. ParEval-Repo
includes several scientific computing and AI mini-applications in a range of
programming models, and levels of repository complexity. We use ParEval-Repo to
evaluate a range of state-of-the-art open-source and commercial LLMs, with both
a non-agentic and a top-down agentic approach. We assess code generated by the
LLMs and approaches in terms of compilability, functional correctness,
categories of build errors, and the cost of translation in terms of the number
of inference tokens. Our results demonstrate that LLM translation of scientific
applications is feasible for small programs but difficulty with generating
functional build systems and cross-file dependencies pose challenges in scaling
to larger codebases.

</details>


### [6] [Portable High-Performance Kernel Generation for a Computational Fluid Dynamics Code with DaCe](https://arxiv.org/abs/2506.20994)
*Måns I. Andersson,Martin Karp,Niclas Jansson,Stefano Markidis*

Main category: cs.DC

TL;DR: 论文探讨了如何利用DaCe框架自动生成高性能代码，以应对HPC系统中硬件多样性的挑战，并以CFD中的Neko求解器为例展示了其效果。


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算（HPC）加速器的多样化，开发者为不同硬件架构编写特定代码的负担加重，影响了科学应用的可维护性。

Method: 利用DaCe框架的Stateful Dataflow Multigraph（SDFG）表示法，自动生成适用于多核处理器和多种加速器的高性能代码。

Result: 在Nvidia GH200、A100和AMD MI250X GPU等多种平台上，生成的代码表现出良好的可移植性和性能。

Conclusion: 通过自动代码生成，证明了使用便携式解决方案确保大规模科学应用长期可持续性的可行性。

Abstract: With the emergence of new high-performance computing (HPC) accelerators, such
as Nvidia and AMD GPUs, efficiently targeting diverse hardware architectures
has become a major challenge for HPC application developers. The increasing
hardware diversity in HPC systems often necessitates the development of
architecture-specific code, hindering the sustainability of large-scale
scientific applications. In this work, we leverage DaCe, a data-centric
parallel programming framework, to automate the generation of high-performance
kernels. DaCe enables automatic code generation for multicore processors and
various accelerators, reducing the burden on developers who would otherwise
need to rewrite code for each new architecture. Our study demonstrates DaCe's
capabilities by applying its automatic code generation to a critical
computational kernel used in Computational Fluid Dynamics (CFD). Specifically,
we focus on Neko, a Fortran-based solver that employs the spectral-element
method, which relies on small tensor operations. We detail the formulation of
this computational kernel using DaCe's Stateful Dataflow Multigraph (SDFG)
representation and discuss how this approach facilitates high-performance code
generation. Additionally, we outline the workflow for seamlessly integrating
DaCe's generated code into the Neko solver. Our results highlight the
portability and performance of the generated code across multiple platforms,
including Nvidia GH200, Nvidia A100, and AMD MI250X GPUs, with competitive
performance results. By demonstrating the potential of automatic code
generation, we emphasise the feasibility of using portable solutions to ensure
the long-term sustainability of large-scale scientific applications.

</details>


### [7] [BLOCKS: Blockchain-supported Cross-Silo Knowledge Sharing for Efficient LLM Services](https://arxiv.org/abs/2506.21033)
*Zhaojiacheng Zhou,Hongze Liu,Shijing Yuan,Hanning Zhang,Jiong Lou,Chentao Wu,Jie Li*

Main category: cs.DC

TL;DR: 提出基于区块链的外部知识框架，解决LLM幻觉问题，协调分散知识孤岛，确保数据安全与知识质量。


<details>
  <summary>Details</summary>
Motivation: LLM的幻觉问题日益突出，外部知识增强是解决方案，但隐私与安全问题导致知识孤岛难以整合。

Method: 通过区块链技术将本地知识提炼为提示，引入声誉机制和交叉验证确保质量，设计查询生成框架。

Result: 实验证明该框架在区块链环境下实现了高效的LLM知识共享。

Conclusion: 提出的框架有效解决了知识孤岛问题，为LLM提供了可靠的外部知识支持。

Abstract: The hallucination problem of Large Language Models (LLMs) has increasingly
drawn attention. Augmenting LLMs with external knowledge is a promising
solution to address this issue. However, due to privacy and security concerns,
a vast amount of downstream task-related knowledge remains dispersed and
isolated across various "silos," making it difficult to access. To bridge this
knowledge gap, we propose a blockchain-based external knowledge framework that
coordinates multiple knowledge silos to provide reliable foundational knowledge
for large model retrieval while ensuring data security. Technically, we distill
knowledge from local data into prompts and execute transactions and records on
the blockchain. Additionally, we introduce a reputation mechanism and
cross-validation to ensure knowledge quality and provide incentives for
participation. Furthermore, we design a query generation framework that
provides a direct API interface for large model retrieval. To evaluate the
performance of our proposed framework, we conducted extensive experiments on
various knowledge sources. The results demonstrate that the proposed framework
achieves efficient LLM service knowledge sharing in blockchain environments.

</details>


### [8] [Bridding OT and PaaS in Edge-to-Cloud Continuum](https://arxiv.org/abs/2506.21072)
*Carlos J Barrios,Yves Denneulin*

Main category: cs.DC

TL;DR: OTPaaS框架高效管理数据，提升响应时间、安全性、可靠性及能源效率，支持工业转型和数据主权。


<details>
  <summary>Details</summary>
Motivation: 为工业转型和数据主权提供高效、安全的数据管理解决方案。

Method: 采用PaaS模型，支持Edge和Cloud环境，包含部署、应用管理和集成组件。

Result: 成功部署，解决关键挑战，适应多种用例。

Conclusion: OTPaaS为工业数据管理提供了高效、灵活的解决方案。

Abstract: The Operational Technology Platform as a Service (OTPaaS) initiative provides
a structured framework for the efficient management and storage of data. It
ensures excellent response times while improving security, reliability, data
and technology sovereignty, robustness, and energy efficiency, which are
crucial for industrial transformation and data sovereignty. This paper
illustrates successful deployment, adaptable application management, and
various integration components catering to Edge and Cloud environments. It
leverages the advantages of the Platform as a Service model and highlights key
challenges that have been addressed for specific use cases.

</details>


### [9] [Enabling Bitcoin Smart Contracts on the Internet Computer](https://arxiv.org/abs/2506.21327)
*Ryan Croote,Islam El-Ashi,Thomas Locher,Yvonne-Anne Pignolet*

Main category: cs.DC

TL;DR: 本文提出了一种直接在互联网计算机（IC）上执行比特币智能合约的架构，避免了传统桥接机制的安全风险，并展示了其高效性和经济可行性。


<details>
  <summary>Details</summary>
Motivation: 比特币本身的可编程性有限，现有方法多依赖桥接机制，存在安全风险。本文旨在通过直接交互实现比特币智能合约的高效执行。

Method: 提出了一种架构，使IC和比特币节点直接交互，无需桥接，解决了比特币概率性与IC状态不可逆性的矛盾。

Result: 实验结果表明，该架构能在几秒内完成最终确认，执行成本低，支持复杂的比特币去中心化应用。

Conclusion: 该架构为比特币智能合约提供了安全高效的解决方案，具有实际应用价值。

Abstract: There is growing interest in providing programmatic access to the value
locked in Bitcoin, which famously offers limited programmability itself.
Various approaches have been put forth in recent years, with the vast majority
of proposed mechanisms either building new functionality on top of Bitcoin or
leveraging a bridging mechanism to enable smart contracts that make use of
``wrapped'' bitcoins on entirely different platforms.
  In this work, an architecture is presented that follows a different approach.
The architecture enables the execution of Turing-complete Bitcoin smart
contracts on the Internet Computer (IC), a blockchain platform for hosting and
executing decentralized applications. Instead of using a bridge, IC and Bitcoin
nodes interact directly, eliminating potential security risks that the use of a
bridge entails. This integration requires novel concepts, in particular to
reconcile the probabilistic nature of Bitcoin with the irreversibility of
finalized state changes on the IC, which may be of independent interest.
  In addition to the presentation of the architecture, we provide evaluation
results based on measurements of the Bitcoin integration running on mainnet.
The evaluation results demonstrate that, with finalization in a few seconds and
low execution costs, this integration enables complex Bitcoin-based
decentralized applications that were not practically feasible or economically
viable before.

</details>


### [10] [Carbon-Aware Microservice Deployment for Optimal User Experience on a Budget](https://arxiv.org/abs/2506.21422)
*Kevin Kreutz,Philipp Wiesner,Monica Vitali*

Main category: cs.DC

TL;DR: 提出了一种针对微服务的碳感知方法，通过选择最佳版本和横向扩展，在碳预算内优化用户体验和收入。


<details>
  <summary>Details</summary>
Motivation: 数据中心碳足迹问题日益严重，现有碳感知策略不适用于服务导向的云应用。

Method: 选择最合适的微服务版本和横向扩展策略，以在每小时碳预算内运行。

Result: 实验表明，该方法能适应不同工作负载和碳强度变化。

Conclusion: 该方法有效平衡了碳预算与用户体验和收入的关系。

Abstract: The carbon footprint of data centers has recently become a critical concern.
So far, most carbon-aware strategies have focused on leveraging the flexibility
of scheduling decisions for batch processing by shifting the time and location
of workload executions. However, such approaches cannot be applied to
service-oriented cloud applications, since they have to be reachable at every
point in time and often at low latencies. We propose a carbon-aware approach
for operating microservices under hourly carbon budgets. By choosing the most
appropriate version and horizontal scaleout for each microservice, our strategy
maximizes user experience and revenue while staying within budget constraints.
Experiments across various application configurations and carbon budgets
demonstrate that the approach adapts properly to changing workloads and carbon
intensities.

</details>


### [11] [exa-AMD: A Scalable Workflow for Accelerating AI-Assisted Materials Discovery and Design](https://arxiv.org/abs/2506.21449)
*Maxim Moraru,Weiyi Xia,Zhuo Ye,Feng Zhang,Yongxin Yao,Ying Wai Li,Cai-Zhuang Wang*

Main category: cs.DC

TL;DR: exa-AMD是一个基于Python的工具，通过整合AI/ML、材料数据库和量子力学计算，加速功能材料的发现与设计。


<details>
  <summary>Details</summary>
Motivation: 加速功能材料的发现与设计，整合多种工具以提升效率。

Method: 利用Parsl任务并行编程库，实现从笔记本电脑到超级计算机的灵活任务执行。

Result: 通过解耦工作流逻辑与执行配置，研究人员无需为不同系统重新实现工作流即可扩展。

Conclusion: exa-AMD为功能材料研究提供了高效、可扩展的解决方案。

Abstract: exa-AMD is a Python-based application designed to accelerate the discovery
and design of functional materials by integrating AI/ML tools, materials
databases, and quantum mechanical calculations into scalable, high-performance
workflows. The execution model of exa-AMD relies on Parsl, a task-parallel
programming library that enables a flexible execution of tasks on any computing
resource from laptops to supercomputers. By using Parsl, exa-AMD is able to
decouple the workflow logic from execution configuration, thereby empowering
researchers to scale their workflows without having to reimplement them for
each system.

</details>


### [12] [Efficient and Reuseable Cloud Configuration Search Using Discovery Spaces](https://arxiv.org/abs/2506.21467)
*Michael Johnston,Burkhard Ringlein,Christoph Hagleitner,Alessandro Pomponio,Vassilis Vassiliadis,Christian Pinto,Srikumar Venugopal*

Main category: cs.DC

TL;DR: 提出Discovery Space抽象化方法，用于高效搜索云资源配置，支持跨工作负载的知识共享和优化。


<details>
  <summary>Details</summary>
Motivation: 解决云资源配置空间庞大、参数复杂的问题，以最小成本满足服务级别协议。

Method: 提出Discovery Space抽象化框架，支持结构化、分布式搜索，并实现具体应用。

Result: 提高了优化效率，支持知识共享，配置搜索速度提升90%以上。

Conclusion: Discovery Space为大规模云资源配置搜索提供了高效、通用的解决方案。

Abstract: Finding the optimal set of cloud resources to deploy a given workload at
minimal cost while meeting a defined service level agreement is an active area
of research. Combining tens of parameters applicable across a large selection
of compute, storage, and services offered by cloud providers with similar
numbers of application-specific parameters leads to configuration spaces with
millions of deployment options.
  In this paper, we propose Discovery Space, an abstraction that formalizes the
description of workload configuration problems, and exhibits a set of
characteristics required for structured, robust and distributed investigations
of large search spaces. We describe a concrete implementation of the Discovery
Space abstraction and show that it is generalizable across a diverse set of
workloads such as Large Language Model inference and Big Data Analytics.
  We demonstrate that our approach enables safe, transparent sharing of data
between executions of best-of-breed optimizers increasing the efficiency of
optimal configuration detection in large search spaces. We also demonstrate how
Discovery Spaces enable transfer and reuse of knowledge across similar search
spaces, enabling configuration search speed-ups of over 90%.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [13] [Adaptive Hybrid Sort: Dynamic Strategy Selection for Optimal Sorting Across Diverse Data Distributions](https://arxiv.org/abs/2506.20677)
*Shrinivass Arunachalam Balasubramanian*

Main category: cs.DS

TL;DR: 提出了一种自适应混合排序范式，根据输入数据的实时模式自动选择最优排序算法（计数排序、基数排序或快速排序）。


<details>
  <summary>Details</summary>
Motivation: 由于没有排序算法在所有数据分布下都最优，需要一种能动态适应数据特征的排序方法。

Method: 通过特征提取模块分析数据参数（如数据量、值范围和熵），决策引擎（结合有限状态机和XGBoost分类器）选择最优排序策略。

Result: 实验证明，该方法在执行时间、灵活性和效率上显著优于传统静态排序算法。

Conclusion: 该框架具有可扩展性，适用于大数据分析、边缘计算和硬件受限系统等广泛场景。

Abstract: Sorting is an essential operation in computer science with direct
consequences on the performance of large scale data systems, real-time systems,
and embedded computation. However, no sorting algorithm is optimal under all
distributions of data. The new adaptive hybrid sorting paradigm proposed in
this paper is the paradigm that automatically selects the most effective
sorting algorithm Counting Sort, Radix Sort, or QuickSort based on real-time
monitoring of patterns in input data. The architecture begins by having a
feature extraction module to compute significant parameters such as data
volume, value range and entropy. These parameters are sent to a decision engine
involving Finite State Machine and XGBoost classifier to aid smart and
effective in choosing the optimal sorting strategy. It implements Counting Sort
on small key ranges, Radix Sort on large range structured input with
low-entropy keys and QuickSort on general purpose sorting. The experimental
findings of both synthetic and real life dataset confirm that the proposed
solution is actually inclined to excel significantly by comparison in execution
time, flexibility and the efficiency of conventional static sorting algorithms.
The proposed framework provides a scalable, high perhaps and applicable to a
wide range of data processing operations like big data analytics, edge
computing, and systems with hardware limitations.

</details>


### [14] [Practical and Accurate Local Edge Differentially Private Graph Algorithms](https://arxiv.org/abs/2506.20828)
*Pranay Mundra,Charalampos Papamanthou,Julian Shun,Quanquan C. Liu*

Main category: cs.DS

TL;DR: 本文提出基于局部差分隐私（LDP）的新算法，用于图分析中的k-core分解和三角形计数，通过利用图的退化性和最大度数改进理论效用，实验显示显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大规模网络分析涉及敏感数据，传统集中式隐私模型依赖可信第三方，而LDP在个体层面保护隐私，无需信任第三方。

Method: 提出基于LDP的算法，利用图的退化性和最大度数优化误差界限，并首次在分布式模拟中评估。

Result: k-core分解误差仅为精确值的3倍，远优于基线；三角形计数算法将近似误差降低六个数量级，同时保持高效运行时间。

Conclusion: 新算法在隐私保护和准确性上显著优于现有方法，适用于实际大规模图分析。

Abstract: The rise of massive networks across diverse domains necessitates
sophisticated graph analytics, often involving sensitive data and raising
privacy concerns. This paper addresses these challenges using local
differential privacy (LDP), which enforces privacy at the individual level,
where no third-party entity is trusted, unlike centralized models that assume a
trusted curator. We introduce novel LDP algorithms for two fundamental graph
statistics: k-core decomposition and triangle counting. Our approach leverages
input-dependent private graph properties, specifically the degeneracy and
maximum degree of the graph, to improve theoretical utility. Unlike prior
methods, our error bounds are determined by the maximum degree rather than the
total number of edges, resulting in significantly tighter guarantees. For
triangle counting, we improve upon the work of Imola, Murakami, and
Chaudhury~\cite{IMC21locally, IMC21communication}, which bounds error in terms
of edge count. Instead, our algorithm achieves bounds based on graph degeneracy
by leveraging a private out-degree orientation, a refined variant of Eden et
al.'s randomized response technique~\cite{ELRS23, and a novel analysis,
yielding stronger guarantees than prior work. Beyond theoretical gains, we are
the first to evaluate local DP algorithms in a distributed simulation, unlike
prior work tested on a single processor. Experiments on real-world graphs show
substantial accuracy gains: our k-core decomposition achieves errors within 3x
of exact values, far outperforming the 131x error in the baseline of Dhulipala
et al.~\cite{DLRSSY22}. Our triangle counting algorithm reduces multiplicative
approximation errors by up to six orders of magnitude, while maintaining
competitive runtime.

</details>


### [15] [Review of Three Variants of the k-d Tree](https://arxiv.org/abs/2506.20687)
*Russell A. Brown*

Main category: cs.DS

TL;DR: 本文探讨了构建平衡k-d树的三种变体及其性能比较，并提出了一种双线程执行方法。


<details>
  <summary>Details</summary>
Motivation: 由于传统平衡技术（如AVL树或红黑树）不适用于k-d树，因此需要找到一种高效的方法来构建平衡的k-d树。

Method: 研究对比了三种基于不同分区技术的k-d树变体，并分析了它们的计算复杂度。此外，还提出并分析了其中一种变体的双线程执行方法。

Result: 三种变体的性能表现不同，双线程执行方法在特定情况下能提升效率。

Conclusion: 选择合适的k-d树变体和执行方法可以显著影响构建过程的计算复杂度。

Abstract: The original description of the k-d tree recognized that rebalancing
techniques, such as used to build an AVL tree or a red-black tree, are not
applicable to a k-d tree. Hence, in order to build a balanced k-d tree, it is
necessary to find the median of a set of data for each recursive subdivision of
that set. The sort or selection used to find the median, and the technique used
to partition the set about that median, strongly influence the computational
complexity of building a k-d tree. This article describes and contrasts three
variants of the k-d tree that differ in their technique used to partition the
set, and compares the performance of those variants. In addition, dual-threaded
execution is proposed and analyzed for one of the three variants.

</details>


### [16] [A Framework for Building Data Structures from Communication Protocols](https://arxiv.org/abs/2506.20761)
*Alexandr Andoni,Shunhua Jiang,Omri Weinstein*

Main category: cs.DS

TL;DR: 提出了一种基于通信模型的高维模式匹配数据结构设计框架，应用于部分匹配问题，显著提升了查询时间和空间效率。


<details>
  <summary>Details</summary>
Motivation: 解决高维模式匹配问题中现有数据结构查询时间过长的问题，尤其是在部分匹配问题中。

Method: 通过将数据结构问题转化为UAM通信复杂度问题，并开发了一种针对稀疏集不相交性的一侧误差通信协议。

Result: 实现了查询时间为n^(1-1/(c log^2 c))且接近线性空间的数据结构，优于现有方法。

Conclusion: 该框架展示了数据依赖数据结构的能力，为高维模式匹配问题提供了更高效的解决方案。

Abstract: We present a general framework for designing efficient data structures for
high-dimensional pattern-matching problems ($\exists \;? i\in[n], f(x_i,y)=1$)
through communication models in which $f(x,y)$ admits sublinear communication
protocols with exponentially-small error. Specifically, we reduce the data
structure problem to the Unambiguous Arthur-Merlin (UAM) communication
complexity of $f(x,y)$ under product distributions.
  We apply our framework to the Partial Match problem (a.k.a, matching with
wildcards), whose underlying communication problem is sparse set-disjointness.
When the database consists of $n$ points in dimension $d$, and the number of
$\star$'s in the query is at most $w = c\log n \;(\ll d)$, the fastest known
linear-space data structure (Cole, Gottlieb and Lewenstein, STOC'04) had query
time $t \approx 2^w = n^c$, which is nontrivial only when $c<1$. By contrast,
our framework produces a data structure with query time $n^{1-1/(c \log^2 c)}$
and space close to linear.
  To achieve this, we develop a one-sided $\epsilon$-error communication
protocol for Set-Disjointness under product distributions with
$\tilde{\Theta}(\sqrt{d\log(1/\epsilon)})$ complexity, improving on the
classical result of Babai, Frankl and Simon (FOCS'86). Building on this
protocol, we show that the Unambiguous AM communication complexity of
$w$-Sparse Set-Disjointness with $\epsilon$-error under product distributions
is $\tilde{O}(\sqrt{w \log(1/\epsilon)})$, independent of the ambient dimension
$d$, which is crucial for the Partial Match result. Our framework sheds further
light on the power of data-dependent data structures, which is instrumental for
reducing to the (much easier) case of product distributions.

</details>


### [17] [Almost Tight Additive Guarantees for \boldmath $k$-Edge-Connectivity](https://arxiv.org/abs/2506.20906)
*Nikhil Kumar,Chaitanya Swamy*

Main category: cs.DS

TL;DR: 论文提出了一种针对k边连通生成子图（kECSS）问题的近似算法，针对偶数k和奇数k分别提供了不同的连通性和成本保证，并扩展到度约束版本。


<details>
  <summary>Details</summary>
Motivation: 解决kECSS问题的APX-hard性质，改进现有算法的解决方案质量和算法复杂性。

Method: 针对偶数k和奇数k分别设计多项式时间算法，利用LP松弛的最优值作为成本上限，并提供两种不同的连通性保证。

Result: 对于偶数k，获得(k-2)-边连通子图；对于奇数k，获得(k-3)-边连通子图，成本不超过LP最优值。还提供了其他连通性和成本保证。

Conclusion: 算法在成本和连通性上接近最优，并扩展到度约束版本，填补了相关领域的空白。

Abstract: We consider the \emph{$k$-edge connected spanning subgraph} (kECSS) problem,
where we are given an undirected graph $G = (V, E)$ with nonnegative edge costs
$\{c_e\}_{e\in E}$, and we seek a minimum-cost \emph{$k$-edge connected}
subgraph $H$ of $G$. For even $k$, we present a polytime algorithm that
computes a $(k-2)$-edge connected subgraph of cost at most the optimal value
$LP^*$ of the natural LP-relaxation for kECSS; for odd $k$, we obtain a
$(k-3)$-edge connected subgraph of cost at most $LP^*$. Since kECSS is APX-hard
for all $k\geq 2$, our results are nearly optimal. They also significantly
improve upon the recent work of Hershkowitz et al., both in terms of solution
quality and the simplicity of algorithm and its analysis. Our techniques also
yield an alternate guarantee, where we obtain a $(k-1)$-edge connected subgraph
of cost at most $1.5\cdot LP^*$; with unit edge costs, the cost guarantee
improves to $(1+\frac{4}{3k})\cdot LP^*$, which improves upon the
state-of-the-art approximation for unit edge costs, but with a unit loss in
edge connectivity.
  Our kECSS-result also yields results for the \emph{$k$-edge connected
spanning multigraph} (kECSM) problem, where multiple copies of an edge can be
selected: we obtain a $(1+2/k)$-approximation algorithm for even $k$, and a
$(1+3/k)$-approximation algorithm for odd $k$.
  Our techniques extend to the degree-bounded versions of kECSS and kECSM,
wherein we also impose degree lower- and upper- bounds on the nodes. We obtain
the same cost and connectivity guarantees for these degree-bounded versions
with an additive violation of (roughly) $2$ for the degree bounds. These are
the first results for degree-bounded \{kECSS,kECSM\} of the form where the cost
of the solution obtained is at most the optimum, and the connectivity
constraints are violated by an additive constant.

</details>


### [18] [Courcelle's Theorem for Lipschitz Continuity](https://arxiv.org/abs/2506.21118)
*Tatsuya Gima,Soh Kumabe,Yuichi Yoshida*

Main category: cs.DS

TL;DR: 提出了一种Lipschitz连续算法的元定理，适用于有界树宽和团宽图上的优化问题，提供近似解且具有对数级Lipschitz常数。


<details>
  <summary>Details</summary>
Motivation: 解决现有Lipschitz连续算法需要针对每个问题单独设计的问题，提供通用框架。

Method: 基于Courcelle定理的类比，利用MSO逻辑表达约束，设计近似算法。

Result: 在有界树宽图上，算法提供(1±ε)-近似解且Lipschitz常数为对数级；类似结果适用于有界团宽图。

Conclusion: 该元定理在近似性和Lipschitz连续性上优于现有方法，并可扩展至Baker分解。

Abstract: Lipschitz continuity of algorithms, introduced by Kumabe and Yoshida
(FOCS'23), measures the stability of an algorithm against small input
perturbations. Algorithms with small Lipschitz continuity are desirable, as
they ensure reliable decision-making and reproducible scientific research.
Several studies have proposed Lipschitz continuous algorithms for various
combinatorial optimization problems, but these algorithms are problem-specific,
requiring a separate design for each problem.
  To address this issue, we provide the first algorithmic meta-theorem in the
field of Lipschitz continuous algorithms. Our result can be seen as a Lipschitz
continuous analogue of Courcelle's theorem, which offers Lipschitz continuous
algorithms for problems on bounded-treewidth graphs. Specifically, we consider
the problem of finding a vertex set in a graph that maximizes or minimizes the
total weight, subject to constraints expressed in monadic second-order logic
(MSO_2). We show that for any $\varepsilon>0$, there exists a $(1\pm
\varepsilon)$-approximation algorithm for the problem with a polylogarithmic
Lipschitz constant on bounded treewidth graphs. On such graphs, our result
outperforms most existing Lipschitz continuous algorithms in terms of
approximability and/or Lipschitz continuity. Further, we provide similar
results for problems on bounded-clique-width graphs subject to constraints
expressed in MSO_1. Additionally, we construct a Lipschitz continuous version
of Baker's decomposition using our meta-theorem as a subroutine.

</details>


### [19] [On Minimizing Wiggle in Stacked Area Charts](https://arxiv.org/abs/2506.21175)
*Alexander Dobler,Martin Nöllenburg*

Main category: cs.DS

TL;DR: 论文分析了堆叠面积图中最小化边界摆动（wiggle）的计算复杂度，证明其NP难性，并提出精确算法与启发式算法的比较。


<details>
  <summary>Details</summary>
Motivation: 堆叠面积图广泛用于时间序列可视化，但优化其可读性的边界摆动最小化问题尚未被正式分析。

Method: 通过混合整数线性规划提出精确算法，并与启发式算法进行实验比较。

Result: 证明边界摆动最小化问题的NP难性及难以近似性，并展示算法性能。

Conclusion: 研究揭示了边界摆动最小化的复杂性，为未来优化提供了理论基础。

Abstract: Stacked area charts are a widely used visualization technique for numerical
time series. The x-axis represents time, and the time series are displayed as
horizontal, variable-height layers stacked on top of each other. The height of
each layer corresponds to the time series values at each time point. The main
aesthetic criterion for optimizing the readability of stacked area charts is
the amount of vertical change of the borders between the time series in the
visualization, called wiggle. While many heuristic algorithms have been
developed to minimize wiggle, the computational complexity of minimizing wiggle
has not been formally analyzed. In this paper, we show that different variants
of wiggle minimization are NP-hard and even hard to approximate. We also
present an exact mixed-integer linear programming formulation and compare its
performance with a state-of-the-art heuristic in an experimental evaluation.
Lastly, we consider a special case of wiggle minimization that corresponds to
the fundamentally interesting and natural problem of ordering a set of numbers
as to minimize their sum of absolute prefix sums. We show several complexity
results for this problem that imply some of the mentioned hardness results for
wiggle minimization.

</details>


### [20] [Edge Clique Partition and Cover Beyond Independence](https://arxiv.org/abs/2506.21216)
*Fedor V. Fomin,Petr A. Golovach,Danil Sagunov,Kirill Simonov*

Main category: cs.DS

TL;DR: 论文研究了基于最大独立集参数化的边团覆盖和边团划分问题，发现两者在复杂性上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 现实世界中，稀疏图的边团覆盖或划分的最小数量常接近最大独立集大小，因此研究基于最大独立集的参数化问题具有重要意义。

Method: 引入并研究了Edge Clique Cover Above Independent Set (ECC/α)和Edge Clique Partition Above Independent Set (ECP/α)，以最大独立集为基准参数化。

Result: ECP/α是固定参数可解的，而ECC/α在k≥2时是NP完全的，但在k∈{0,1}时可在多项式时间内解决。此外，ECC/α在参数k+ω(G)下是固定参数可解的。

Conclusion: 研究揭示了边团覆盖和划分问题在参数化视角下的复杂性差异，并为稀疏图提供了高效的算法。

Abstract: Covering and partitioning the edges of a graph into cliques are classical
problems at the intersection of combinatorial optimization and graph theory,
having been studied through a range of algorithmic and complexity-theoretic
lenses. Despite the well-known fixed-parameter tractability of these problems
when parameterized by the total number of cliques, such a parameterization
often fails to be meaningful for sparse graphs. In many real-world instances,
on the other hand, the minimum number of cliques in an edge cover or partition
can be very close to the size of a maximum independent set \alpha(G).
  Motivated by this observation, we investigate above \alpha parameterizations
of the edge clique cover and partition problems. Concretely, we introduce and
study Edge Clique Cover Above Independent Set (ECC/\alpha) and Edge Clique
Partition Above Independent Set (ECP/\alpha), where the goal is to cover or
partition all edges of a graph using at most \alpha(G) + k cliques, and k is
the parameter. Our main results reveal a distinct complexity landscape for the
two variants. We show that ECP/\alpha is fixed-parameter tractable, whereas
ECC/\alpha is NP-complete for all k \geq 2, yet can be solved in polynomial
time for k \in {0,1}. These findings highlight intriguing differences between
the two problems when viewed through the lens of parameterization above a
natural lower bound.
  Finally, we demonstrate that ECC/\alpha becomes fixed-parameter tractable
when parameterized by k + \omega(G), where \omega(G) is the size of a maximum
clique of the graph G. This result is particularly relevant for sparse graphs,
in which \omega is typically small. For H-minor free graphs, we design a
subexponential algorithm of running time f(H)^{\sqrt{k}}n^{O(1)}.

</details>


### [21] [Vantage Point Selection Algorithms for Bottleneck Capacity Estimation](https://arxiv.org/abs/2506.21418)
*Vikrant Ashvinkumar,Rezaul Chowdhury,Jie Gao,Mayank Goswami,Joseph S. B. Mitchell,Valentin Polishchuk*

Main category: cs.DS

TL;DR: 研究如何选择最优的观测点以揭示互联网图中的瓶颈容量问题，提出了非自适应和自适应两种设置下的算法。


<details>
  <summary>Details</summary>
Motivation: 解决互联网图中瓶颈容量估计问题，通过选择观测点揭示最大数量的瓶颈边容量。

Method: 在非自适应设置中，提出基于随机排列的1-1/e近似算法；在自适应设置中，分析实例最优近似算法的上下界。

Result: 非自适应设置下获得近似算法；自适应设置下给出树和平面图的上下界。

Conclusion: 为观测点选择问题提供了理论和算法支持，适用于不同场景。

Abstract: Motivated by the problem of estimating bottleneck capacities on the Internet,
we formulate and study the problem of vantage point selection. We are given a
graph $G=(V, E)$ whose edges $E$ have unknown capacity values that are to be
discovered. Probes from a vantage point, i.e, a vertex $v \in V$, along
shortest paths from $v$ to all other vertices, reveal bottleneck edge
capacities along each path. Our goal is to select $k$ vantage points from $V$
that reveal the maximum number of bottleneck edge capacities.
  We consider both a non-adaptive setting where all $k$ vantage points are
selected before any bottleneck capacity is revealed, and an adaptive setting
where each vantage point selection instantly reveals bottleneck capacities
along all shortest paths starting from that point. In the non-adaptive setting,
by considering a relaxed model where edge capacities are drawn from a random
permutation (which still leaves the problem of maximizing the expected number
of revealed edges NP-hard), we are able to give a $1-1/e$ approximate
algorithm. In the adaptive setting we work with the least permissive model
where edge capacities are arbitrarily fixed but unknown. We compare with the
best solution for the particular input instance (i.e. by enumerating all
choices of $k$ tuples), and provide both lower bounds on instance optimal
approximation algorithms and upper bounds for trees and planar graphs.

</details>


### [22] [Succinct Preferential Attachment Graphs](https://arxiv.org/abs/2506.21436)
*Ziad Ismaili Alaoui,Namrata,Sebastian Wild*

Main category: cs.DS

TL;DR: 该论文提出了一种针对图数据的压缩数据结构，其空间利用率随图的压缩性自动优化，同时高效支持导航操作。


<details>
  <summary>Details</summary>
Motivation: 现有图压缩数据结构通常局限于特定图类且空间利用率固定，无法适应不同图的压缩性差异。

Method: 设计了一种自适应压缩数据结构，空间利用率随图的压缩性动态调整，支持高效导航操作。

Result: 在Barabási-Albert模型下，空间利用率接近实例最优；对任意图，空间利用率不超过熵压缩边列表。

Conclusion: 该数据结构在空间利用率和查询效率上实现了平衡，适用于多种图类型。

Abstract: Computing over compressed data combines the space saving of data compression
with efficient support for queries directly on the compressed representation.
Such data structures are widely applied in text indexing and have been
successfully generalised to trees. For graphs, support for computing over
compressed data remains patchy; typical results in the area of succinct data
structures are restricted to a specific class of graphs and use the same,
worst-case amount of space for any graph from this class.
  In this work, we design a data structure whose space usage automatically
improves with the compressibility of the graph at hand, while efficiently
supporting navigational operations (simulating adjacency-list access).
Specifically, we show that the space usage approaches the instance-optimal
space when the graph is drawn according to the classic Barab\'asi-Albert model
of preferential-attachment graphs. Our data-structure techniques also work for
arbitrary graphs, guaranteeing a size asymptotically no larger than an
entropy-compressed edge list. A key technical contribution is the careful
analysis of the instance-optimal space usage.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [23] [Generating Reliable Adverse event Profiles for Health through Automated Integrated Data (GRAPH-AID): A Semi-Automated Ontology Building Approach](https://arxiv.org/abs/2506.20851)
*Srikar Reddy Gadusu,Larry Callahan,Samir Lababidi,Arunasri Nishtala,Sophia Healey,Hande McGinty*

Main category: cs.SE

TL;DR: 本文提出了一种用户友好的方法，利用Python和rdflib库简化Neo4j数据库与OWL的集成，解决了现有方法需要描述逻辑（DL）语法知识的问题。


<details>
  <summary>Details</summary>
Motivation: 随着数据和知识的快速扩展，系统化的本体生成方法变得至关重要。现有的KNARM方法虽然提供了知识图谱创建的框架，但在Neo4j与OWL的无缝集成上存在挑战，且需要用户掌握DL语法。

Method: 通过Python和rdflib库开发了一种自动化脚本，从Neo4j数据库（如FDA的FAERS数据集）中生成OWL所需的类和公理。

Result: 成功实现了Neo4j与OWL的集成，并通过FAERS数据集展示了方法的实用性，支持更高效的药物安全监测。

Conclusion: 该方法为快速增长的药物不良事件数据集的本体生成提供了实用解决方案，有助于提升公共卫生决策能力。

Abstract: As data and knowledge expand rapidly, adopting systematic methodologies for
ontology generation has become crucial. With the daily increases in data
volumes and frequent content changes, the demand for databases to store and
retrieve information for the creation of knowledge graphs has become
increasingly urgent. The previously established Knowledge Acquisition and
Representation Methodology (KNARM) outlines a systematic approach to address
these challenges and create knowledge graphs. However, following this
methodology highlights the existing challenge of seamlessly integrating Neo4j
databases with the Web Ontology Language (OWL). Previous attempts to integrate
data from Neo4j into an ontology have been discussed, but these approaches
often require an understanding of description logics (DL) syntax, which may not
be familiar to many users. Thus, a more accessible method is necessary to
bridge this gap. This paper presents a user-friendly approach that utilizes
Python and its rdflib library to support ontology development. We showcase our
novel approach through a Neo4j database we created by integrating data from the
Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS)
database. Using this dataset, we developed a Python script that automatically
generates the required classes and their axioms, facilitating a smoother
integration process. This approach offers a practical solution to the
challenges of ontology generation in the context of rapidly growing adverse
drug event datasets, supporting improved drug safety monitoring and public
health decision-making.

</details>


### [24] [Domain Knowledge in Requirements Engineering: A Systematic Mapping Study](https://arxiv.org/abs/2506.20754)
*Marina Araújo,Júlia Araújo,Romeu Oliveira,Lucas Romao,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文通过系统映射研究，总结了领域知识在需求工程中的应用现状，包括方法、技术和工具，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 领域知识对需求工程至关重要，但缺乏系统化的整合和应用方法。

Method: 采用混合搜索策略（数据库搜索与雪球抽样）进行系统映射研究。

Result: 分析了75篇论文，总结了领域知识的类型、质量属性和挑战，为研究与实践提供支持。

Conclusion: 研究为知识驱动的需求工程奠定了概念和方法基础，并提出了未来研究方向。

Abstract: [Context] Domain knowledge is recognized as a key component for the success
of Requirements Engineering (RE), as it provides the conceptual support needed
to understand the system context, ensure alignment with stakeholder needs, and
reduce ambiguity in requirements specification. Despite its relevance, the
scientific literature still lacks a systematic consolidation of how domain
knowledge can be effectively used and operationalized in RE. [Goal] This paper
addresses this gap by offering a comprehensive overview of existing
contributions, including methods, techniques, and tools to incorporate domain
knowledge into RE practices. [Method] We conducted a systematic mapping study
using a hybrid search strategy that combines database searches with iterative
backward and forward snowballing. [Results] In total, we found 75 papers that
met our inclusion criteria. The analysis highlights the main types of
requirements addressed, the most frequently considered quality attributes, and
recurring challenges in the formalization, acquisition, and long-term
maintenance of domain knowledge. The results provide support for researchers
and practitioners in identifying established approaches and unresolved issues.
The study also outlines promising directions for future research, emphasizing
the development of scalable, automated, and sustainable solutions to integrate
domain knowledge into RE processes. [Conclusion] The study contributes by
providing a comprehensive overview that helps to build a conceptual and
methodological foundation for knowledge-driven requirements engineering.

</details>


### [25] [Agile Management for Machine Learning: A Systematic Mapping Study](https://arxiv.org/abs/2506.20759)
*Lucas Romao,Hugo Villamizar,Romeu Oliveira,Silvio Alonso,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文通过系统映射研究总结了机器学习（ML）支持系统中敏捷管理的现状，识别了8个关键主题和主要挑战。


<details>
  <summary>Details</summary>
Motivation: ML的动态特性对传统项目管理提出挑战，敏捷方法因其灵活性可能适用，但缺乏具体应用指导。

Method: 采用混合搜索策略（数据库搜索与雪球迭代）进行系统映射研究，分析了2008-2024年的27篇论文。

Result: 识别了8个框架和8个关键主题（如迭代灵活性、ML特定工件的创新），主要挑战是ML任务的工作量估算。

Conclusion: 研究填补了领域空白，但需更多实证评估以验证现有成果。

Abstract: [Context] Machine learning (ML)-enabled systems are present in our society,
driving significant digital transformations. The dynamic nature of ML
development, characterized by experimental cycles and rapid changes in data,
poses challenges to traditional project management. Agile methods, with their
flexibility and incremental delivery, seem well-suited to address this
dynamism. However, it is unclear how to effectively apply these methods in the
context of ML-enabled systems, where challenges require tailored approaches.
[Goal] Our goal is to outline the state of the art in agile management for
ML-enabled systems. [Method] We conducted a systematic mapping study using a
hybrid search strategy that combines database searches with backward and
forward snowballing iterations. [Results] Our study identified 27 papers
published between 2008 and 2024. From these, we identified eight frameworks and
categorized recommendations and practices into eight key themes, such as
Iteration Flexibility, Innovative ML-specific Artifacts, and the Minimal Viable
Model. The main challenge identified across studies was accurate effort
estimation for ML-related tasks. [Conclusion] This study contributes by mapping
the state of the art and identifying open gaps in the field. While relevant
work exists, more robust empirical evaluation is still needed to validate these
contributions.

</details>


### [26] [Engineering RAG Systems for Real-World Applications: Design, Development, and Evaluation](https://arxiv.org/abs/2506.20869)
*Md Toufique Hasan,Muhammad Waseem,Kai-Kristian Kemell,Ayman Asad Khan,Mika Saari,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 本文介绍了五个基于检索增强生成（RAG）的实际应用案例，并总结了开发过程中的12个关键经验教训。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在事实准确性和上下文相关性方面的局限性，缺乏基于实际用例的RAG系统实证研究。

Method: 开发了五个领域特定的RAG应用，结合多语言OCR、语义检索和领域适应的LLMs，并通过100名参与者的网络评估。

Result: 评估了六个维度（如易用性、准确性等），总结了12个技术、操作和伦理方面的经验教训。

Conclusion: RAG系统在实际应用中面临可靠性、可用性等多方面挑战，需进一步优化。

Abstract: Retrieval-Augmented Generation (RAG) systems are emerging as a key approach
for grounding Large Language Models (LLMs) in external knowledge, addressing
limitations in factual accuracy and contextual relevance. However, there is a
lack of empirical studies that report on the development of RAG-based
implementations grounded in real-world use cases, evaluated through general
user involvement, and accompanied by systematic documentation of lessons
learned. This paper presents five domain-specific RAG applications developed
for real-world scenarios across governance, cybersecurity, agriculture,
industrial research, and medical diagnostics. Each system incorporates
multilingual OCR, semantic retrieval via vector embeddings, and domain-adapted
LLMs, deployed through local servers or cloud APIs to meet distinct user needs.
A web-based evaluation involving a total of 100 participants assessed the
systems across six dimensions: (i) Ease of Use, (ii) Relevance, (iii)
Transparency, (iv) Responsiveness, (v) Accuracy, and (vi) Likelihood of
Recommendation. Based on user feedback and our development experience, we
documented twelve key lessons learned, highlighting technical, operational, and
ethical challenges affecting the reliability and usability of RAG systems in
practice.

</details>


### [27] [Complex Model Transformations by Reinforcement Learning with Uncertain Human Guidance](https://arxiv.org/abs/2506.20883)
*Kyanna Dagenais,Istvan David*

Main category: cs.SE

TL;DR: 论文提出了一种结合强化学习（RL）和人类指导的方法，用于开发复杂的模型转换（MT）序列，以提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 手动开发复杂的模型转换序列容易出错且不可行，强化学习虽能缓解问题，但在复杂场景中性能不足，因此引入人类指导以提升效果。

Method: 通过将用户定义的模型转换映射到强化学习原语，并作为强化学习程序执行，以寻找最优的模型转换序列，同时结合人类建议（即使不确定）。

Result: 评估表明，人类指导显著提高了强化学习的性能，并更高效地开发了复杂的模型转换序列。

Conclusion: 该方法在人类建议的确定性和及时性之间取得平衡，为强化学习驱动的人机协同工程方法迈出了一步。

Abstract: Model-driven engineering problems often require complex model transformations
(MTs), i.e., MTs that are chained in extensive sequences. Pertinent examples of
such problems include model synchronization, automated model repair, and design
space exploration. Manually developing complex MTs is an error-prone and often
infeasible process. Reinforcement learning (RL) is an apt way to alleviate
these issues. In RL, an autonomous agent explores the state space through trial
and error to identify beneficial sequences of actions, such as MTs. However, RL
methods exhibit performance issues in complex problems. In these situations,
human guidance can be of high utility. In this paper, we present an approach
and technical framework for developing complex MT sequences through RL, guided
by potentially uncertain human advice. Our framework allows user-defined MTs to
be mapped onto RL primitives, and executes them as RL programs to find optimal
MT sequences. Our evaluation shows that human guidance, even if uncertain,
substantially improves RL performance, and results in more efficient
development of complex MTs. Through a trade-off between the certainty and
timeliness of human advice, our method takes a step towards RL-driven
human-in-the-loop engineering methods.

</details>


### [28] [Boosting Vulnerability Detection with Inter-function Multilateral Association Insights](https://arxiv.org/abs/2506.21014)
*Shaojian Qiu,Mengyang Huang,Jiahao Cheng*

Main category: cs.SE

TL;DR: IFMA-VD是一个基于超图卷积的漏洞检测框架，通过分析函数间多边关联提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法忽略函数间复杂关联，导致漏洞检测不足。

Method: 构建代码行为超图，利用超边卷积提取多边关联特征。

Result: 在三个数据集上F-measure和Recall优于基线方法。

Conclusion: 多边关联特征增强代码表示，IFMA-VD在实际数据中有效。

Abstract: Vulnerability detection is a crucial yet challenging technique for ensuring
the security of software systems. Currently, most deep learning-based
vulnerability detection methods focus on stand-alone functions, neglecting the
complex inter-function interrelations, particularly the multilateral
associations. This oversight can fail to detect vulnerabilities in these
interrelations. To address this gap, we present an Inter-Function Multilateral
Association analysis framework for Vulnerability Detection (IFMA-VD). The
cornerstone of the IFMA-VD lies in constructing a code behavior hypergraph and
utilizing hyperedge convolution to extract multilateral association features.
Specifically, we first parse functions into a code property graph to generate
intra-function features. Following this, we construct a code behavior
hypergraph by segmenting the program dependency graph to isolate and encode
behavioral features into hyperedges. Finally, we utilize a hypergraph network
to capture the multilateral association knowledge for augmenting vulnerability
detection. We evaluate IFMA-VD on three widely used vulnerability datasets and
demonstrate improvements in F-measure and Recall compared to baseline methods.
Additionally, we illustrate that multilateral association features can boost
code feature representation and validate the effectiveness of IFMA-VD on
real-world datasets.

</details>


### [29] [How Good Are Synthetic Requirements ? Evaluating LLM-Generated Datasets for AI4RE](https://arxiv.org/abs/2506.21138)
*Abdelkarim El-Hajjami,Camille Salinesi*

Main category: cs.SE

TL;DR: 论文提出Synthline v1，通过优化生成策略和后期处理技术，提升合成需求数据的质量，并在多个分类任务中验证其效果。


<details>
  <summary>Details</summary>
Motivation: 公开标记的需求数据集稀缺是AI4RE发展的主要障碍，需要系统化方法优化合成数据的质量。

Method: 采用增强的Product Line方法（Synthline v1），结合多样本提示、自动提示优化（PACE）和后期处理技术，生成并优化合成需求数据。

Result: 多样本提示显著提升数据效用和多样性；PACE对功能分类效果显著（+32.5分）；合成数据在某些任务中表现优于人工数据（如安全分类+7.8分）。

Conclusion: 合成需求数据可有效缓解数据集稀缺问题，并在特定任务中超越人工数据，为AI4RE提供实用路径。

Abstract: The shortage of publicly available, labeled requirements datasets remains a
major barrier to advancing Artificial Intelligence for Requirements Engineering
(AI4RE). While Large Language Models offer promising capabilities for synthetic
data generation, systematic approaches to control and optimize the quality of
generated requirements remain underexplored. This paper presents Synthline v1,
an enhanced Product Line approach for generating synthetic requirements data
that extends our earlier v0 version with advanced generation strategies and
curation techniques. We investigate four research questions assessing how
prompting strategies, automated prompt optimization, and post-generation
curation affect data quality across four classification tasks: defect
detection, functional vs. non-functional, quality vs. non-quality, and security
vs. non-security. Our evaluation shows that multi-sample prompting
significantly boosts both utility and diversity over single-sample generation,
with F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic
Editing) for automated prompt optimization yields task-dependent results,
greatly improving functional classification (+32.5 points) but reducing
performance on others. Interestingly, similarity-based curation improves
diversity but often harms classification performance, indicating that some
redundancy may help ML models. Most importantly, our results show that
synthetic requirements can match or outperform human-authored ones for specific
tasks, with synthetic data surpassing human data for security (+7.8 points) and
defect classification (+15.4 points). These findings offer practical insights
for AI4RE and chart a viable path to mitigating dataset scarcity through
systematic synthetic generation.

</details>


### [30] [$T^3$: Multi-level Tree-based Automatic Program Repair with Large Language Models](https://arxiv.org/abs/2506.21211)
*Quanming Liu,Xupeng Bu,Zhichao Yan,Ru Li*

Main category: cs.SE

TL;DR: 本文提出了一种名为$T^3$的创新框架，将大型语言模型（LLMs）的强大推理能力与树搜索相结合，显著提高了自动程序修复（APR）任务中生成候选修复方案的精度。


<details>
  <summary>Details</summary>
Motivation: 由于自动程序修复（APR）任务需要复杂的逻辑和多步推理能力，而现有的链式思维（CoT）技术在该领域的应用尚不充分，因此本研究旨在填补这一空白。

Method: 研究系统地评估了几种常见的CoT技术在APR任务中的表现，并提出$T^3$框架，结合LLMs的推理能力和树搜索技术。

Result: $T^3$框架显著提高了生成候选修复方案的精度，并为APR任务中的样本选择和修复策略优化提供了有价值的指导。

Conclusion: $T^3$框架为高效自动化调试建立了坚实的基础，展示了结合LLMs和树搜索在APR领域的潜力。

Abstract: Automatic Program Repair (APR) is a core technology in software development
and maintenance, with aims to enable automated defect repair with minimal human
intervention. In recent years, the substantial advancements in Large Language
Models (LLMs) and the Chain-of-Thought (CoT) techniques have significantly
enhanced the reasoning capabilities of these models. However, due to the
complex logic and multi-step reasoning ability needed, the application of CoT
techniques in the APR domain remains insufficient. This study systematically
evaluates the performance of several common CoT techniques in APR tasks and
proposes an innovative framework $T^3$, which integrates the powerful reasoning
capabilities of LLMs with tree search, effectively improving the precision of
generating candidate repair solutions. Furthermore, $T^3$ provides valuable
guidance for optimizing sample selection and repair strategies in APR tasks,
establishing a robust framework for achieving efficient automated debugging.

</details>


### [31] [KOALA: a Configurable Tool for Collecting IDE Data When Solving Programming Tasks](https://arxiv.org/abs/2506.21266)
*Daniil Karol,Elizaveta Artser,Ilya Vlasov,Yaroslav Golubev,Hieke Keuning,Anastasiia Birillo*

Main category: cs.SE

TL;DR: KOALA是一个用于收集学生在JetBrains IDE中编程任务数据的工具，解决了现有工具在数据粒度控制和配置灵活性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有数据收集工具在代码粒度控制、编程环境事件收集和配置灵活性方面存在局限，影响了数据质量和研究效果。

Method: 开发了KOALA插件，可安装在JetBrains IDE中，支持任务配置、IDE功能控制（如代码补全）、运行调查，并收集代码快照、IDE操作（如调试）及其他未收集的数据（如快捷键使用）。

Result: 工具成功收集了28名学生在两门课程中的编程任务数据，并展示了数据的潜在价值。

Conclusion: KOALA是一个灵活且功能强大的工具，能够为编程教育研究提供高质量的数据支持。

Abstract: Collecting data of students solving programming tasks is incredibly valuable
for researchers and educators. It allows verifying that the students correctly
apply the features and concepts they are taught, or finding students'
misconceptions. However, existing data collection tools have limitations, e.g.,
no control over the granularity of the collected code, not collecting the
specific events of the programming environment used, and overall being hard to
configure.
  To overcome these limitations, we propose KOALA, a convenient and highly
configurable tool for collecting code snapshots and feature usage from students
solving programming tasks in JetBrains IDEs. The plugin can be installed in
IDEs and configured to provide the students with the necessary tasks, enable or
disable certain IDE features like code completion, and run surveys. During
problem solving, the plugin collects code snapshots at the configured
granularity, all IDE actions like running and debugging, as well as some data
not collected in prior works, like employed hotkeys and switching focus between
files. The collected data is sent to the server that comes with the tool, where
it is stored and can be converted to the standardized ProgSnap2 format. To
showcase the tool, we collected data from 28 students solving tasks in two
courses within the IDE, highlighting some insights from this data.

</details>


### [32] [Exploring Micro Frontends: A Case Study Application in E-Commerce](https://arxiv.org/abs/2506.21297)
*Ricardo Hideki Hangai Kojo,Luiz Fernando Corte Real,Renato Cordeiro Ferreira,Thatiane de Oliveira Rosa,Alfredo Goldman*

Main category: cs.SE

TL;DR: 论文探讨了微前端架构在工业场景中的适用性，通过案例研究发现其虽能提升团队独立性，但并非总是必要。


<details>
  <summary>Details</summary>
Motivation: 研究微前端架构的适用性，特别是在工业环境中，以解决紧耦合、技术过时和开发体验差等问题。

Method: 结合学术与灰色文献调研，并在一个手工艺品市场平台实施微前端架构，通过开发者问卷调查评估效果。

Result: 微前端架构成功实施，但研究发现其他替代方案（如单体前端）也能达到类似效果。微前端的优势在于与微服务架构的协同。

Conclusion: 微前端架构在特定情境下（如已有微服务架构）更具优势，但需权衡复杂性和实际需求。

Abstract: In the micro frontends architectural style, the frontend is divided into
smaller components, which can range from a simple button to an entire page. The
goal is to improve scalability, resilience, and team independence, albeit at
the cost of increased complexity and infrastructure demands. This paper seeks
to understand when it is worth adopting micro frontends, particularly in the
context of industry. To achieve this, we conducted an investigation into the
state of the art of micro frontends, based on both academic and gray
literature. We then implemented this architectural style in a marketplace for
handcrafted products, which already used microservices. Finally, we evaluated
the implementation through a semi-open questionnaire with the developers. At
the studied marketplace company, the need for architectural change arose due to
the tight coupling between their main system (a Java monolith) and a dedicated
frontend system. Additionally, there were deprecated technologies and poor
developer experience. To address these issues, the micro frontends architecture
was adopted, along with the API Gateway and Backend for Frontend patterns, and
technologies such as Svelte and Fastify. Although the adoption of Micro
Frontends was successful, it was not strictly necessary to meet the company's
needs. According to the analysis of the mixed questionnaire responses, other
alternatives, such as a monolithic frontend, could have achieved comparable
results. What made adopting micro frontends the most convenient choice in the
company's context was the monolith strangulation and microservices adoption,
which facilitated implementation through infrastructure reuse and knowledge
sharing between teams.

</details>


### [33] [An object-centric core metamodel for IoT-enhanced event logs](https://arxiv.org/abs/2506.21300)
*Yannis Bertrand,Christian Imenkamp,Lukas Malburg,Matthias Ehrendorfer,Marco Franceschetti,Joscha Grüger,Francesco Leotta,Jürgen Mangler,Ronny Seiger,Agnes Koschmider,Stefanie Rinderle-Ma,Barbara Weber,Estefania Serral*

Main category: cs.SE

TL;DR: 提出了一种核心数据模型，整合了现有IoT与流程数据模型的特性，以促进流程挖掘领域的数据共享与合作。


<details>
  <summary>Details</summary>
Motivation: IoT设备与业务流程的集成产生了大量数据，但现有数据模型碎片化，阻碍了数据交换与合作。

Method: 提出一个基于共同需求的核心模型，并通过Python原型实现验证其满足这些需求。

Result: 核心模型成功整合了现有模型的特性，并通过用例验证其有效性。

Conclusion: 核心模型为流程挖掘领域的数据共享与合作提供了统一解决方案。

Abstract: Advances in Internet-of-Things (IoT) technologies have prompted the
integration of IoT devices with business processes (BPs) in many organizations
across various sectors, such as manufacturing, healthcare and smart spaces. The
proliferation of IoT devices leads to the generation of large amounts of IoT
data providing a window on the physical context of BPs, which facilitates the
discovery of new insights about BPs using process mining (PM) techniques.
However, to achieve these benefits, IoT data need to be combined with
traditional process (event) data, which is challenging due to the very
different characteristics of IoT and process data, for instance in terms of
granularity levels. Recently, several data models were proposed to integrate
IoT data with process data, each focusing on different aspects of data
integration based on different assumptions and requirements. This fragmentation
hampers data exchange and collaboration in the field of PM, e.g., making it
tedious for researchers to share data. In this paper, we present a core model
synthesizing the most important features of existing data models. As the core
model is based on common requirements, it greatly facilitates data sharing and
collaboration in the field. A prototypical Python implementation is used to
evaluate the model against various use cases and demonstrate that it satisfies
these common requirements.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [34] [Drift-Adaptive Slicing-Based Resource Management for Cooperative ISAC Networks](https://arxiv.org/abs/2506.20762)
*Shisheng Hu,Jie Gao,Xue Qin,Conghao Zhou,Xinyu Huang,Mushu Li,Mingcheng He,Xuemin Shen*

Main category: cs.NI

TL;DR: 提出了一种基于切片的资源管理方案，用于集成感知与通信网络，通过数字孪生和漂移自适应模型优化资源分配。


<details>
  <summary>Details</summary>
Motivation: 解决移动设备和感知目标空间分布非平稳性导致的资源规划失效问题。

Method: 建立两个网络切片分别提供感知和通信服务，利用数字孪生和漂移自适应统计模型优化资源分配。

Result: 服务满意度提升18%，资源消耗减少13.1%。

Conclusion: 方案有效提升了资源管理效率和网络性能。

Abstract: In this paper, we propose a novel drift-adaptive slicing-based resource
management scheme for cooperative integrated sensing and communication (ISAC)
networks. Particularly, we establish two network slices to provide sensing and
communication services, respectively. In the large-timescale planning for the
slices, we partition the sensing region of interest (RoI) of each mobile device
and reserve network resources accordingly, facilitating low-complexity
distance-based sensing target assignment in small timescales. To cope with the
non-stationary spatial distributions of mobile devices and sensing targets,
which can result in the drift in modeling the distributions and ineffective
planning decisions, we construct digital twins (DTs) of the slices. In each DT,
a drift-adaptive statistical model and an emulation function are developed for
the spatial distributions in the corresponding slice, which facilitates
closed-form decision-making and efficient validation of a planning decision,
respectively. Numerical results show that the proposed drift-adaptive
slicing-based resource management scheme can increase the service satisfaction
ratio by up to 18% and reduce resource consumption by up to 13.1% when compared
with benchmark schemes.

</details>


### [35] [Flowcut Switching: High-Performance Adaptive Routing with In-Order Delivery Guarantees](https://arxiv.org/abs/2506.21406)
*Tommaso Bonato,Daniele De Sensi,Salvatore Di Girolamo,Abdulla Bataineh,David Hewson,Duncan Roweth,Torsten Hoefler*

Main category: cs.NI

TL;DR: 提出了一种名为flowcut switching的新型自适应路由算法，旨在保证高性能的有序数据包传输，适用于各种网络条件。


<details>
  <summary>Details</summary>
Motivation: 网络延迟严重影响超级计算机上应用程序的性能，而现有自适应路由算法可能导致数据包乱序，影响传输协议性能。

Method: 提出flowcut switching算法，不同于现有基于突发流量的解决方案（如flowlet switching），该算法保证任何网络条件下的有序传输。

Result: flowcut switching能够有效减少数据包乱序，提升网络性能，尤其适用于非突发流量（如RDMA）。

Conclusion: flowcut switching是一种高效的自适应路由算法，适用于多种网络条件，解决了现有算法在有序传输上的不足。

Abstract: Network latency severely impacts the performance of applications running on
supercomputers. Adaptive routing algorithms route packets over different
available paths to reduce latency and improve network utilization. However, if
a switch routes packets belonging to the same network flow on different paths,
they might arrive at the destination out-of-order due to differences in the
latency of these paths. For some transport protocols like TCP, QUIC, and RoCE,
out-of-order (OOO) packets might cause large performance drops or significantly
increase CPU utilization. In this work, we propose flowcut switching, a new
adaptive routing algorithm that provides high-performance in-order packet
delivery. Differently from existing solutions like flowlet switching, which are
based on the assumption of bursty traffic and that might still reorder packets,
flowcut switching guarantees in-order delivery under any network conditions,
and is effective also for non-bursty traffic, as it is often the case for RDMA.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [36] [Progressive Size-Adaptive Federated Learning: A Comprehensive Framework for Heterogeneous Multi-Modal Data Systems](https://arxiv.org/abs/2506.20685)
*Sajid Hussain,Muhammad Sohail,Nauman Ali Khan,Naima Iltaf,Ihtesham ul Islam*

Main category: cs.LG

TL;DR: SAFL是一种基于数据集大小特性的自适应联邦学习框架，揭示了数据集大小对联邦学习效果的关键影响，并在多模态数据上验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法主要关注模型异构性和聚合技术，忽视了数据集大小特性对训练动态的根本影响。

Method: 提出SAFL框架，基于数据集大小特性组织联邦学习，并在13个多模态数据集上进行实验评估。

Result: 发现1000-1500样本为最优数据集大小范围，结构化数据表现优于非结构化数据，SAFL平均准确率达87.68%，通信效率高。

Conclusion: SAFL填补了数据特性驱动联邦学习策略的空白，为实际部署提供了理论和实践指导。

Abstract: Federated Learning (FL) has emerged as a transformative paradigm for
distributed machine learning while preserving data privacy. However, existing
approaches predominantly focus on model heterogeneity and aggregation
techniques, largely overlooking the fundamental impact of dataset size
characteristics on federated training dynamics. This paper introduces
Size-Based Adaptive Federated Learning (SAFL), a novel progressive training
framework that systematically organizes federated learning based on dataset
size characteristics across heterogeneous multi-modal data. Our comprehensive
experimental evaluation across 13 diverse datasets spanning 7 modalities
(vision, text, time series, audio, sensor, medical vision, and multimodal)
reveals critical insights: 1) an optimal dataset size range of 1000-1500
samples for federated learning effectiveness; 2) a clear modality performance
hierarchy with structured data (time series, sensor) significantly
outperforming unstructured data (text, multimodal); and 3) systematic
performance degradation for large datasets exceeding 2000 samples. SAFL
achieves an average accuracy of 87.68% across all datasets, with structured
data modalities reaching 99%+ accuracy. The framework demonstrates superior
communication efficiency, reducing total data transfer to 7.38 GB across 558
communications while maintaining high performance. Our real-time monitoring
framework provides unprecedented insights into system resource utilization,
network efficiency, and training dynamics. This work fills critical gaps in
understanding how data characteristics should drive federated learning
strategies, providing both theoretical insights and practical guidance for
real-world FL deployments in neural network and learning systems.

</details>


### [37] [E-ABIN: an Explainable module for Anomaly detection in BIological Networks](https://arxiv.org/abs/2506.20693)
*Ugo Lomoio,Tommaso Mazza,Pierangelo Veltri,Pietro Hiram Guzzi*

Main category: cs.LG

TL;DR: E-ABIN是一个可解释的、通用的生物网络异常检测框架，结合了经典机器学习和图深度学习技术，用于识别基因表达或甲基化网络中的异常。


<details>
  <summary>Details</summary>
Motivation: 当前基因异常检测方法通常局限于单一数据集且缺乏用户友好的图形界面，E-ABIN旨在解决这些问题。

Method: E-ABIN整合了支持向量机、随机森林、图自编码器和图对抗属性网络等算法，提供高预测准确性和可解释性。

Result: 在膀胱癌和乳糜泻的案例研究中，E-ABIN成功识别了生物学相关的异常，并揭示了疾病机制。

Conclusion: E-ABIN是一个高效且用户友好的工具，适用于复杂生物数据的异常检测和解释。

Abstract: The increasing availability of large-scale omics data calls for robust
analytical frameworks capable of handling complex gene expression datasets
while offering interpretable results. Recent advances in artificial
intelligence have enabled the identification of aberrant molecular patterns
distinguishing disease states from healthy controls. Coupled with improvements
in model interpretability, these tools now support the identification of genes
potentially driving disease phenotypes. However, current approaches to gene
anomaly detection often remain limited to single datasets and lack accessible
graphical interfaces. Here, we introduce E-ABIN, a general-purpose, explainable
framework for Anomaly detection in Biological Networks. E-ABIN combines
classical machine learning and graph-based deep learning techniques within a
unified, user-friendly platform, enabling the detection and interpretation of
anomalies from gene expression or methylation-derived networks. By integrating
algorithms such as Support Vector Machines, Random Forests, Graph Autoencoders
(GAEs), and Graph Adversarial Attributed Networks (GAANs), E-ABIN ensures a
high predictive accuracy while maintaining interpretability. We demonstrate the
utility of E-ABIN through case studies of bladder cancer and coeliac disease,
where it effectively uncovers biologically relevant anomalies and offers
insights into disease mechanisms.

</details>


### [38] [On Context-Content Uncertainty Principle](https://arxiv.org/abs/2506.20699)
*Xin Li*

Main category: cs.LG

TL;DR: 论文提出了一种基于上下文-内容不确定性原理（CCUP）的计算框架，通过熵不对称性指导推理，并分层阐述了四个操作原则。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索大脑和机器如何通过结构-特异性对齐最小化不确定性，揭示推理的熵梯度解析机制。

Method: 方法包括建立分层计算框架（L1-L4），涵盖核心推理约束、资源分配、时间引导学习和空间层次组合，并通过形式化定理和模拟验证。

Result: 结果显示CCUP对齐的推理效率显著提升，证明了结构-特异性对齐的递归有效性。

Conclusion: 结论是CCUP为理解大脑和机器的推理提供了统一理论框架，强调内容引导的循环一致性熵梯度解析。

Abstract: The Context-Content Uncertainty Principle (CCUP) proposes that inference
under uncertainty is governed by an entropy asymmetry between context and
content: high-entropy contexts must be interpreted through alignment with
low-entropy, structured content. In this paper, we develop a layered
computational framework that derives operational principles from this
foundational asymmetry. At the base level, CCUP formalizes inference as
directional entropy minimization, establishing a variational gradient that
favors content-first structuring. Building upon this, we identify four
hierarchical layers of operational principles: (\textbf{L1}) \emph{Core
Inference Constraints}, including structure-before-specificity, asymmetric
inference flow, cycle-consistent bootstrapping, and conditional compression,
all shown to be mutually reducible; (\textbf{L2}) \emph{Resource Allocation
Principles}, such as precision-weighted attention, asymmetric learning rates,
and attractor-based memory encoding; (\textbf{L3}) \emph{Temporal Bootstrapping
Dynamics}, which organize learning over time via structure-guided curricula;
and (\textbf{L4}) \emph{Spatial Hierarchical Composition}, which integrates
these mechanisms into self-organizing cycles of memory, inference, and
planning. We present formal equivalence theorems, a dependency lattice among
principles, and computational simulations demonstrating the efficiency gains of
CCUP-aligned inference. This work provides a unified theoretical foundation for
understanding how brains and machines minimize uncertainty through recursive
structure-specificity alignment. The brain is not just an inference machine. It
is a cycle-consistent entropy gradient resolver, aligning structure and
specificity via path-dependent, content-seeded simulation.

</details>


### [39] [Diffusion Tree Sampling: Scalable inference-time alignment of diffusion models](https://arxiv.org/abs/2506.20701)
*Vineet Jain,Kusha Sareen,Mohammad Pedramfar,Siamak Ravanbakhsh*

Main category: cs.LG

TL;DR: 提出了一种基于树的方法（DTS和DTS*），通过重用过去计算来改进扩散模型的推理对齐，显著减少计算量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在高噪声水平下估计不准确且计算效率低，需要改进。

Method: 将推理对齐视为搜索问题，通过树结构传播终端奖励并迭代优化值估计。

Result: 在MNIST和CIFAR-10上，DTS以10倍更少计算达到最佳基线FID；在文本生成任务中，DTS*以5倍更少计算匹配最佳样本。

Conclusion: DTS和DTS*通过重用信息，提供了一种可扩展的推理对齐方法，将额外计算转化为更好的样本。

Abstract: Adapting a pretrained diffusion model to new objectives at inference time
remains an open problem in generative modeling. Existing steering methods
suffer from inaccurate value estimation, especially at high noise levels, which
biases guidance. Moreover, information from past runs is not reused to improve
sample quality, resulting in inefficient use of compute. Inspired by the
success of Monte Carlo Tree Search, we address these limitations by casting
inference-time alignment as a search problem that reuses past computations. We
introduce a tree-based approach that samples from the reward-aligned target
density by propagating terminal rewards back through the diffusion chain and
iteratively refining value estimates with each additional generation. Our
proposed method, Diffusion Tree Sampling (DTS), produces asymptotically exact
samples from the target distribution in the limit of infinite rollouts, and its
greedy variant, Diffusion Tree Search (DTS$^\star$), performs a global search
for high reward samples. On MNIST and CIFAR-10 class-conditional generation,
DTS matches the FID of the best-performing baseline with up to $10\times$ less
compute. In text-to-image generation and language completion tasks, DTS$^\star$
effectively searches for high reward samples that match best-of-N with up to
$5\times$ less compute. By reusing information from previous generations, we
get an anytime algorithm that turns additional compute into steadily better
samples, providing a scalable approach for inference-time alignment of
diffusion models.

</details>


### [40] [On Convolutions, Intrinsic Dimension, and Diffusion Models](https://arxiv.org/abs/2506.20705)
*Kin Kwan Leung,Rasa Hosseinzadeh,Gabriel Loaiza-Ganem*

Main category: cs.LG

TL;DR: 论文证明了FLIPD方法在现实假设下的正确性，并探讨了高斯卷积替换为均匀卷积的类似结果。


<details>
  <summary>Details</summary>
Motivation: 扩散模型（DMs）在高维数据中表现出色，但FLIPD方法的理论基础仅在不现实的仿射子流形假设下成立，需要填补这一理论空白。

Method: 通过形式化证明，验证FLIPD在现实假设下的正确性，并研究高斯卷积替换为均匀卷积时的类似结果。

Result: FLIPD在现实假设下被证明是正确的，且均匀卷积也能得到类似结论。

Conclusion: 研究填补了FLIPD的理论空白，并扩展了其适用范围，为LID估计提供了更坚实的理论基础。

Abstract: The manifold hypothesis asserts that data of interest in high-dimensional
ambient spaces, such as image data, lies on unknown low-dimensional
submanifolds. Diffusion models (DMs) -- which operate by convolving data with
progressively larger amounts of Gaussian noise and then learning to revert this
process -- have risen to prominence as the most performant generative models,
and are known to be able to learn distributions with low-dimensional support.
For a given datum in one of these submanifolds, we should thus intuitively
expect DMs to have implicitly learned its corresponding local intrinsic
dimension (LID), i.e. the dimension of the submanifold it belongs to. Kamkari
et al. (2024b) recently showed that this is indeed the case by linking this LID
to the rate of change of the log marginal densities of the DM with respect to
the amount of added noise, resulting in an LID estimator known as FLIPD. LID
estimators such as FLIPD have a plethora of uses, among others they quantify
the complexity of a given datum, and can be used to detect outliers,
adversarial examples and AI-generated text. FLIPD achieves state-of-the-art
performance at LID estimation, yet its theoretical underpinnings are incomplete
since Kamkari et al. (2024b) only proved its correctness under the highly
unrealistic assumption of affine submanifolds. In this work we bridge this gap
by formally proving the correctness of FLIPD under realistic assumptions.
Additionally, we show that an analogous result holds when Gaussian convolutions
are replaced with uniform ones, and discuss the relevance of this result.

</details>


### [41] [Test-time Scaling Techniques in Theoretical Physics -- A Comparison of Methods on the TPBench Dataset](https://arxiv.org/abs/2506.20729)
*Zhiqi Gao,Tianyi Li,Yurii Kvasiuk,Sai Chaitanya Tadepalli,Maja Rudolph,Daniel J. H. Chung,Frederic Sala,Moritz Münchmeyer*

Main category: cs.LG

TL;DR: 论文研究了测试时扩展技术在高级理论物理领域的泛化能力，提出了一种符号弱验证框架，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 验证从数学推理基准（如AIME）中学到的测试时扩展技术是否适用于高级理论物理领域。

Method: 开发了一种符号弱验证框架，利用物理问题的结构提升并行扩展效果。

Result: 新方法在TPBench物理数据集上显著优于现有测试时扩展技术，并在AIME上也表现优异。

Conclusion: 逐步符号验证是解决复杂科学问题的有效方法。

Abstract: Large language models (LLMs) have shown strong capabilities in complex
reasoning, and test-time scaling techniques can enhance their performance with
comparably low cost. Many of these methods have been developed and evaluated on
mathematical reasoning benchmarks such as AIME. This paper investigates whether
the lessons learned from these benchmarks generalize to the domain of advanced
theoretical physics. We evaluate a range of common test-time scaling methods on
the TPBench physics dataset and compare their effectiveness with results on
AIME. To better leverage the structure of physics problems, we develop a novel,
symbolic weak-verifier framework to improve parallel scaling results. Our
empirical results demonstrate that this method significantly outperforms
existing test-time scaling approaches on TPBench. We also evaluate our method
on AIME, confirming its effectiveness in solving advanced mathematical
problems. Our findings highlight the power of step-wise symbolic verification
for tackling complex scientific problems.

</details>


### [42] [A Survey of AI for Materials Science: Foundation Models, LLM Agents, Datasets, and Tools](https://arxiv.org/abs/2506.20743)
*Minh-Hao Van,Prateek Verma,Chen Zhao,Xintao Wu*

Main category: cs.LG

TL;DR: 基础模型（FMs）正在推动材料科学的变革，通过多模态AI系统实现跨领域通用性和新兴能力。本文综述了FMs的应用、工具及挑战，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型在材料科学中受限于任务特定性，而FMs提供了跨领域通用性和多模态能力，适合解决材料科学的多样化挑战。

Method: 本文通过任务驱动的分类法，总结了FMs在六个应用领域的使用，并讨论了数据集、工具及实验平台。

Result: FMs在材料科学中展现出潜力，但仍面临通用性、可解释性、数据不平衡等挑战。

Conclusion: 未来研究应关注可扩展预训练、持续学习、数据治理和可信性，以推动FMs在材料科学中的进一步发展。

Abstract: Foundation models (FMs) are catalyzing a transformative shift in materials
science (MatSci) by enabling scalable, general-purpose, and multimodal AI
systems for scientific discovery. Unlike traditional machine learning models,
which are typically narrow in scope and require task-specific engineering, FMs
offer cross-domain generalization and exhibit emergent capabilities. Their
versatility is especially well-suited to materials science, where research
challenges span diverse data types and scales. This survey provides a
comprehensive overview of foundation models, agentic systems, datasets, and
computational tools supporting this growing field. We introduce a task-driven
taxonomy encompassing six broad application areas: data extraction,
interpretation and Q\&A; atomistic simulation; property prediction; materials
structure, design and discovery; process planning, discovery, and optimization;
and multiscale modeling. We discuss recent advances in both unimodal and
multimodal FMs, as well as emerging large language model (LLM) agents.
Furthermore, we review standardized datasets, open-source tools, and autonomous
experimental platforms that collectively fuel the development and integration
of FMs into research workflows. We assess the early successes of foundation
models and identify persistent limitations, including challenges in
generalizability, interpretability, data imbalance, safety concerns, and
limited multimodal fusion. Finally, we articulate future research directions
centered on scalable pretraining, continual learning, data governance, and
trustworthiness.

</details>


### [43] [Multiple Streams of Relation Extraction: Enriching and Recalling in Transformers](https://arxiv.org/abs/2506.20746)
*Todd Nief,David Reber,Sean Richardson,Ari Holtzman*

Main category: cs.LG

TL;DR: 论文探讨了LLM在微调过程中学习到的关系信息存储和提取方式，提出了动态权重嫁接方法，揭示了信息提取和回忆的两种路径及其作用。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在微调过程中学习的关系信息如何存储和提取，填补现有方法（如激活修补）的不足。

Method: 采用动态权重嫁接技术，分析微调模型在实体处理和预测生成时的信息提取与回忆路径。

Result: 发现微调模型通过两种路径（提取和回忆）处理信息，某些情况下需要两者结合，某些情况下单一路径足够。

Conclusion: 揭示了关系信息提取和回忆的具体机制，包括任务特定注意力机制和关系提取步骤的作用。

Abstract: When an LLM learns a relation during finetuning (e.g., new movie releases,
corporate mergers, etc.), where does this information go? Is it extracted when
the model processes an entity, recalled just-in-time before a prediction, or
are there multiple separate heuristics? Existing localization approaches (e.g.
activation patching) are ill-suited for this analysis because they tend to
replace parts of the residual stream, potentially deleting information. To fill
this gap, we propose dynamic weight-grafting between fine-tuned and pre-trained
language models to show that fine-tuned language models both (1) extract
relation information learned during finetuning while processing entities and
(2) ``recall" this information in later layers while generating predictions. In
some cases, models need both of these pathways to correctly generate finetuned
information while, in other cases, a single ``enrichment" or ``recall" pathway
alone is sufficient. We examine the necessity and sufficiency of these
information pathways, examining what layers they occur at, how much redundancy
they exhibit, and which model components are involved -- finding that the
``recall" pathway occurs via both task-specific attention mechanisms and a
relation extraction step in the output of the attention and the feedforward
networks at the final layers before next token prediction.

</details>


### [44] [Characterization and Mitigation of Training Instabilities in Microscaling Formats](https://arxiv.org/abs/2506.20752)
*Huangyuan Su,Mujin Kwun,Stephanie Gil,Sham Kakade,Nikhil Anand*

Main category: cs.LG

TL;DR: 论文研究了在训练大型语言模型时使用块缩放精度格式（如MX格式）的挑战和可行性，发现训练过程中会出现不稳定性，通过实验提出了缓解策略。


<details>
  <summary>Details</summary>
Motivation: 降低训练大型语言模型的成本，利用下一代硬件加速器支持的低精度算术格式提高效率。

Method: 通过训练近千个语言模型，研究MX格式在不同计算规模和精度组合下的表现，并通过代理模型实验分析不稳定性原因。

Result: 发现MX格式训练会导致损失的不稳定性，尤其是大规模计算时，但通过调整精度方案可以缓解问题。

Conclusion: 某些混合精度配置可以恢复与全精度训练相当的性能，为高效训练提供了新思路。

Abstract: Training large language models is an expensive, compute-bound process that
must be repeated as models scale, algorithms improve, and new data is
collected. To address this, next-generation hardware accelerators increasingly
support lower-precision arithmetic formats, such as the Microscaling (MX)
formats introduced in NVIDIA's Blackwell architecture. These formats use a
shared scale within blocks of parameters to extend representable range and
perform forward/backward GEMM operations in reduced precision for efficiency
gains. In this work, we investigate the challenges and viability of
block-scaled precision formats during model training. Across nearly one
thousand language models trained from scratch -- spanning compute budgets from
$2 \times 10^{17}$ to $4.8 \times 10^{19}$ FLOPs and sweeping over a broad
range of weight-activation precision combinations -- we consistently observe
that training in MX formats exhibits sharp, stochastic instabilities in the
loss, particularly at larger compute scales. To explain this phenomenon, we
conduct controlled experiments and ablations on a smaller proxy model that
exhibits similar behavior as the language model, sweeping across architectural
settings, hyperparameters, and precision formats. These experiments motivate a
simple model in which multiplicative gradient bias introduced by the
quantization of layer-norm affine parameters and a small fraction of
activations can trigger runaway divergence. Through \emph{in situ} intervention
experiments on our proxy model, we demonstrate that instabilities can be
averted or delayed by modifying precision schemes mid-training. Guided by these
findings, we evaluate stabilization strategies in the LLM setting and show that
certain hybrid configurations recover performance competitive with
full-precision training. We release our code at
https://github.com/Hither1/systems-scaling.

</details>


### [45] [Stochastic and Non-local Closure Modeling for Nonlinear Dynamical Systems via Latent Score-based Generative Models](https://arxiv.org/abs/2506.20771)
*Xinghao Dong,Huchen Yang,Jin-Long Wu*

Main category: cs.LG

TL;DR: 提出了一种基于潜在分数的生成AI框架，用于学习计算力学中非线性动力系统的随机非局部闭合模型和本构关系。通过联合训练卷积自编码器和条件扩散模型，显著降低了采样过程的维度，同时保持了物理特性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂多尺度动力系统建模的挑战，特别是在缺乏明确尺度分离的情况下，传统方法的确定性和局部假设过于受限。

Method: 联合训练卷积自编码器与条件扩散模型，在潜在空间中降低采样维度。

Result: 数值结果表明，该方法在潜在空间中不仅保证了小的重构误差，还确保了扩散模型的良好性能。

Conclusion: 提出的随机建模框架在数值模拟中实现了显著的计算加速，同时保持了与物理空间中标准扩散模型相当的预测精度。

Abstract: We propose a latent score-based generative AI framework for learning
stochastic, non-local closure models and constitutive laws in nonlinear
dynamical systems of computational mechanics. This work addresses a key
challenge of modeling complex multiscale dynamical systems without a clear
scale separation, for which numerically resolving all scales is prohibitively
expensive, e.g., for engineering turbulent flows. While classical closure
modeling methods leverage domain knowledge to approximate subgrid-scale
phenomena, their deterministic and local assumptions can be too restrictive in
regimes lacking a clear scale separation. Recent developments of
diffusion-based stochastic models have shown promise in the context of closure
modeling, but their prohibitive computational inference cost limits practical
applications for many real-world applications. This work addresses this
limitation by jointly training convolutional autoencoders with conditional
diffusion models in the latent spaces, significantly reducing the
dimensionality of the sampling process while preserving essential physical
characteristics. Numerical results demonstrate that the joint training approach
helps discover a proper latent space that not only guarantees small
reconstruction errors but also ensures good performance of the diffusion model
in the latent space. When integrated into numerical simulations, the proposed
stochastic modeling framework via latent conditional diffusion models achieves
significant computational acceleration while maintaining comparable predictive
accuracy to standard diffusion models in physical spaces.

</details>


### [46] [Stochastic Parameter Decomposition](https://arxiv.org/abs/2506.20790)
*Lucius Bushnaq,Dan Braun,Lee Sharkey*

Main category: cs.LG

TL;DR: 论文提出了一种名为随机参数分解（SPD）的新方法，用于解决现有线性参数分解框架中计算成本高和超参数敏感的问题。


<details>
  <summary>Details</summary>
Motivation: 当前主流的基于归因的参数分解（APD）方法存在计算成本高和超参数敏感的问题，限制了其在大规模模型中的应用。

Method: 提出了一种更可扩展且对超参数更鲁棒的随机参数分解（SPD）方法，并通过实验验证了其有效性。

Result: SPD能够分解比APD更大、更复杂的模型，避免了参数收缩问题，并在玩具模型中更好地识别了真实机制。

Conclusion: SPD通过结合因果中介分析和网络分解方法，为更大模型的线性参数分解提供了可能性，推动了机制可解释性研究的发展。

Abstract: A key step in reverse engineering neural networks is to decompose them into
simpler parts that can be studied in relative isolation. Linear parameter
decomposition -- a framework that has been proposed to resolve several issues
with current decomposition methods -- decomposes neural network parameters into
a sum of sparsely used vectors in parameter space. However, the current main
method in this framework, Attribution-based Parameter Decomposition (APD), is
impractical on account of its computational cost and sensitivity to
hyperparameters. In this work, we introduce \textit{Stochastic Parameter
Decomposition} (SPD), a method that is more scalable and robust to
hyperparameters than APD, which we demonstrate by decomposing models that are
slightly larger and more complex than was possible to decompose with APD. We
also show that SPD avoids other issues, such as shrinkage of the learned
parameters, and better identifies ground truth mechanisms in toy models. By
bridging causal mediation analysis and network decomposition methods, this
demonstration opens up new research possibilities in mechanistic
interpretability by removing barriers to scaling linear parameter decomposition
methods to larger models. We release a library for running SPD and reproducing
our experiments at https://github.com/goodfire-ai/spd.

</details>


### [47] [GPU Kernel Scientist: An LLM-Driven Framework for Iterative Kernel Optimization](https://arxiv.org/abs/2506.20807)
*Martin Andrews,Sam Witteveen*

Main category: cs.LG

TL;DR: 论文提出了一种基于LLM的自动化方法“GPU Kernel Scientist”，用于迭代优化GPU内核，尤其适用于资源受限或硬件快速变化的环境。


<details>
  <summary>Details</summary>
Motivation: 优化GPU内核通常需要深厚的架构知识和大量实验，而在新架构或文档不足的硬件上更为困难。本文旨在通过LLM驱动的自动化方法解决这一问题。

Method: 采用多阶段进化过程：(a)选择有潜力的代码版本作为迭代基础；(b)基于现有代码和GPU文献生成优化假设；(c)通过代码修改和外部评估系统自主实现实验。

Result: 由于性能竞赛的定量结果暂未公开，论文重点介绍了架构设计、工作流程和定性见解。

Conclusion: LLM驱动的代理工具有潜力在资源受限或快速变化的硬件环境中加速GPU内核优化，降低对领域专家的依赖。

Abstract: Optimizing GPU kernels for high performance is a complex task, often
demanding deep architectural knowledge, extensive profiling, and iterative
experimentation. This challenge is amplified when targeting newer or
less-documented GPU architectures where traditional development aids are
scarce. This paper introduces an LLM-powered "GPU Kernel Scientist," an
automated methodology for iteratively refining accelerator kernels.
  Our methodology employs LLMs in a multi-stage, evolutionary process: (a)
strategically selecting promising prior code versions as a basis for new
iterations; (b) generating hypotheses for optimization experiments, based on
existing code and assimilated knowledge from general GPU literature; and (c)
autonomously implementing these experiments through code modification and
subsequent submission to an external evaluation system, using only observed
timing data as performance feedback. We detail how this approach navigates the
challenges of the AMD MI300 target architecture and leverages LLMs to
compensate for limited domain-specific human expertise.
  Since quantitative results from an ongoing performance competition were
embargoed on paper submission date, we present the architectural design,
operational workflow, and qualitative insights, highlighting the potential of
LLM-driven agents to democratise and accelerate GPU kernel optimization,
especially in resource-constrained or rapidly evolving hardware environments.

</details>


### [48] [FINN-GL: Generalized Mixed-Precision Extensions for FPGA-Accelerated LSTMs](https://arxiv.org/abs/2506.20810)
*Shashwat Khandelwal,Jakoba Petri-Koenig,Thomas B. Preußer,Michaela Blott,Shreejith Shanker*

Main category: cs.LG

TL;DR: 本文提出了一种基于FINN框架的通用LSTM部署方法，解决了FPGA上LSTM加速的定制化问题，实现了性能与资源消耗的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决RNN（尤其是LSTM）在资源受限环境中实时部署的挑战，现有工具主要针对前馈网络，LSTM加速需要定制实现。

Method: 利用FINN框架和ONNX的Scan算子建模LSTM计算，支持混合量化和功能验证；通过FINN编译器将量化ONNX计算图映射到硬件块。

Result: 生成的量化ConvLSTM加速器在性能和资源消耗间取得平衡，推理精度与现有模型相当或更好。

Conclusion: 提出的通用流程为FPGA上资源高效的RNN加速器设计铺平了道路。

Abstract: Recurrent neural networks (RNNs), particularly LSTMs, are effective for
time-series tasks like sentiment analysis and short-term stock prediction.
However, their computational complexity poses challenges for real-time
deployment in resource constrained environments. While FPGAs offer a promising
platform for energy-efficient AI acceleration, existing tools mainly target
feed-forward networks, and LSTM acceleration typically requires full custom
implementation. In this paper, we address this gap by leveraging the
open-source and extensible FINN framework to enable the generalized deployment
of LSTMs on FPGAs. Specifically, we leverage the Scan operator from the Open
Neural Network Exchange (ONNX) specification to model the recurrent nature of
LSTM computations, enabling support for mixed quantisation within them and
functional verification of LSTM-based models. Furthermore, we introduce custom
transformations within the FINN compiler to map the quantised ONNX computation
graph to hardware blocks from the HLS kernel library of the FINN compiler and
Vitis HLS. We validate the proposed tool-flow by training a quantised ConvLSTM
model for a mid-price stock prediction task using the widely used dataset and
generating a corresponding hardware IP of the model using our flow, targeting
the XCZU7EV device. We show that the generated quantised ConvLSTM accelerator
through our flow achieves a balance between performance (latency) and resource
consumption, while matching (or bettering) inference accuracy of
state-of-the-art models with reduced precision. We believe that the
generalisable nature of the proposed flow will pave the way for
resource-efficient RNN accelerator designs on FPGAs.

</details>


### [49] [An Information-Theoretic Analysis for Federated Learning under Concept Drift](https://arxiv.org/abs/2506.21036)
*Fu Peng,Meng Zhang,Ming Tang*

Main category: cs.LG

TL;DR: 该论文分析了联邦学习（FL）在概念漂移下的性能，提出了一种基于信息理论的算法来缓解性能下降。通过建模概念漂移为马尔可夫链，并引入“稳态泛化误差”评估模型对未来数据的适应性。实验验证了算法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据通常是动态的，而现有联邦学习研究多基于静态数据集，导致性能下降。本文旨在解决概念漂移对FL性能的影响。

Method: 提出了一种算法，通过KL散度和互信息对经验风险最小化进行正则化，并研究了三种漂移模式（周期性、渐变性、随机性）的影响。

Result: 实验结果表明，漂移模式显著影响性能，所提算法在三种模式中均优于现有方法。

Conclusion: 该算法有效适应概念漂移，提升了FL在动态数据环境中的长期性能。

Abstract: Recent studies in federated learning (FL) commonly train models on static
datasets. However, real-world data often arrives as streams with shifting
distributions, causing performance degradation known as concept drift. This
paper analyzes FL performance under concept drift using information theory and
proposes an algorithm to mitigate the performance degradation. We model concept
drift as a Markov chain and introduce the \emph{Stationary Generalization
Error} to assess a model's capability to capture characteristics of future
unseen data. Its upper bound is derived using KL divergence and mutual
information. We study three drift patterns (periodic, gradual, and random) and
their impact on FL performance. Inspired by this, we propose an algorithm that
regularizes the empirical risk minimization approach with KL divergence and
mutual information, thereby enhancing long-term performance. We also explore
the performance-cost tradeoff by identifying a Pareto front. To validate our
approach, we build an FL testbed using Raspberry Pi4 devices. Experimental
results corroborate with theoretical findings, confirming that drift patterns
significantly affect performance. Our method consistently outperforms existing
approaches for these three patterns, demonstrating its effectiveness in
adapting concept drift in FL.

</details>


### [50] [Divide, Specialize, and Route: A New Approach to Efficient Ensemble Learning](https://arxiv.org/abs/2506.20814)
*Jakub Piwko,Jędrzej Ruciński,Dawid Płudowski,Antoni Zajko,Patryzja Żak,Mateusz Zacharecki,Anna Kozak,Katarzyna Woźnica*

Main category: cs.LG

TL;DR: Hellsemble是一种新颖的集成框架，通过动态分配实例到不同难度的子集，提升分类性能并保持高效和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统集成方法（如bagging、boosting和DES）计算成本高且对异构数据适应性差，Hellsemble旨在解决这些问题。

Method: Hellsemble通过逐步将误分类实例传递给后续模型，形成专门的基础学习器委员会，并使用路由模型分配新实例。

Result: 在OpenML-CC18和Tabzilla基准测试中，Hellsemble表现优于传统集成方法。

Conclusion: 基于实例难度的动态分配是构建高效、鲁棒集成系统的有效方向。

Abstract: Ensemble learning has proven effective in boosting predictive performance,
but traditional methods such as bagging, boosting, and dynamic ensemble
selection (DES) suffer from high computational cost and limited adaptability to
heterogeneous data distributions. To address these limitations, we propose
Hellsemble, a novel and interpretable ensemble framework for binary
classification that leverages dataset complexity during both training and
inference. Hellsemble incrementally partitions the dataset into circles of
difficulty by iteratively passing misclassified instances from simpler models
to subsequent ones, forming a committee of specialised base learners. Each
model is trained on increasingly challenging subsets, while a separate router
model learns to assign new instances to the most suitable base model based on
inferred difficulty. Hellsemble achieves strong classification accuracy while
maintaining computational efficiency and interpretability. Experimental results
on OpenML-CC18 and Tabzilla benchmarks demonstrate that Hellsemble often
outperforms classical ensemble methods. Our findings suggest that embracing
instance-level difficulty offers a promising direction for constructing
efficient and robust ensemble systems.

</details>


### [51] [Universal and Efficient Detection of Adversarial Data through Nonuniform Impact on Network Layers](https://arxiv.org/abs/2506.20816)
*Furkan Mumcu,Yasin Yilmaz*

Main category: cs.LG

TL;DR: 论文提出了一种轻量级回归模型，通过分析攻击对不同DNN层的影响来检测对抗样本，具有高效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗样本检测方法要么效果不佳，要么计算效率低，无法实时处理。

Method: 训练一个轻量级回归模型，预测深层特征并利用预测误差检测对抗样本。

Result: 该方法高效、实时兼容，适用于多种DNN架构和领域。

Conclusion: 提出的检测方法在效果和效率上均优于现有方法，具有广泛适用性。

Abstract: Deep Neural Networks (DNNs) are notoriously vulnerable to adversarial input
designs with limited noise budgets. While numerous successful attacks with
subtle modifications to original input have been proposed, defense techniques
against these attacks are relatively understudied. Existing defense approaches
either focus on improving DNN robustness by negating the effects of
perturbations or use a secondary model to detect adversarial data. Although
equally important, the attack detection approach, which is studied in this
work, provides a more practical defense compared to the robustness approach. We
show that the existing detection methods are either ineffective against the
state-of-the-art attack techniques or computationally inefficient for real-time
processing. We propose a novel universal and efficient method to detect
adversarial examples by analyzing the varying degrees of impact of attacks on
different DNN layers. {Our method trains a lightweight regression model that
predicts deeper-layer features from early-layer features, and uses the
prediction error to detect adversarial samples.} Through theoretical arguments
and extensive experiments, we demonstrate that our detection method is highly
effective, computationally efficient for real-time processing, compatible with
any DNN architecture, and applicable across different domains, such as image,
video, and audio.

</details>


### [52] [Demystifying Distributed Training of Graph Neural Networks for Link Prediction](https://arxiv.org/abs/2506.20818)
*Xin Huang,Chul-Ho Lee*

Main category: cs.LG

TL;DR: 论文探讨了分布式图神经网络（GNN）在链接预测中的性能下降问题，提出SpLPG方法通过图稀疏化减少通信成本并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有分布式GNN框架多针对节点分类优化，链接预测性能未充分研究，尤其在子图分区训练时性能下降显著。

Method: 提出SpLPG方法，利用图稀疏化减少信息损失和负采样问题，降低通信成本。

Result: 实验显示SpLPG在多个数据集上减少约80%通信开销，同时基本保持链接预测准确性。

Conclusion: SpLPG有效解决了分布式GNN链接预测的性能问题，为实际应用提供了高效解决方案。

Abstract: Graph neural networks (GNNs) are powerful tools for solving graph-related
problems. Distributed GNN frameworks and systems enhance the scalability of
GNNs and accelerate model training, yet most are optimized for node
classification. Their performance on link prediction remains underexplored.
This paper demystifies distributed training of GNNs for link prediction by
investigating the issue of performance degradation when each worker trains a
GNN on its assigned partitioned subgraph without having access to the entire
graph. We discover that the main sources of the issue come from not only the
information loss caused by graph partitioning but also the ways of drawing
negative samples during model training. While sharing the complete graph
information with each worker resolves the issue and preserves link prediction
accuracy, it incurs a high communication cost. We propose SpLPG, which
effectively leverages graph sparsification to mitigate the issue of performance
degradation at a reduced communication cost. Experiment results on several
public real-world datasets demonstrate the effectiveness of SpLPG, which
reduces the communication overhead by up to about 80% while mostly preserving
link prediction accuracy.

</details>


### [53] [Learning-Based Resource Management in Integrated Sensing and Communication Systems](https://arxiv.org/abs/2506.20849)
*Ziyang Lu,M. Cenk Gursoy,Chilukuri K. Mohan,Pramod K. Varshney*

Main category: cs.LG

TL;DR: 提出了一种基于约束深度强化学习（CDRL）的方法，用于优化雷达-通信系统中的时间资源分配，以提升目标通信质量。


<details>
  <summary>Details</summary>
Motivation: 解决雷达-通信系统中多目标跟踪和数据传输的时间分配问题。

Method: 采用约束深度强化学习（CDRL）框架，优化跟踪和通信的时间分配。

Result: 数值结果表明，CDRL框架能在动态环境中最大化通信质量，同时满足时间约束。

Conclusion: CDRL方法有效提升了雷达-通信系统的资源分配效率和通信质量。

Abstract: In this paper, we tackle the task of adaptive time allocation in integrated
sensing and communication systems equipped with radar and communication units.
The dual-functional radar-communication system's task involves allocating dwell
times for tracking multiple targets and utilizing the remaining time for data
transmission towards estimated target locations. We introduce a novel
constrained deep reinforcement learning (CDRL) approach, designed to optimize
resource allocation between tracking and communication under time budget
constraints, thereby enhancing target communication quality. Our numerical
results demonstrate the efficiency of our proposed CDRL framework, confirming
its ability to maximize communication quality in highly dynamic environments
while adhering to time constraints.

</details>


### [54] [Multi-Objective Reinforcement Learning for Cognitive Radar Resource Management](https://arxiv.org/abs/2506.20853)
*Ziyang Lu,Subodh Kalia,M. Cenk Gursoy,Chilukuri K. Mohan,Pramod K. Varshney*

Main category: cs.LG

TL;DR: 论文研究了多功能认知雷达系统中的时间分配问题，通过深度强化学习比较了DDPG和SAC算法，并利用NSGA-II估计Pareto前沿。


<details>
  <summary>Details</summary>
Motivation: 解决认知雷达系统中新目标扫描与已检测目标跟踪之间的权衡问题。

Method: 采用多目标优化框架，使用DDPG和SAC算法寻找Pareto最优解，并用NSGA-II估计Pareto前沿。

Result: SAC算法在稳定性和样本效率上优于DDPG，两种算法均能适应不同场景。

Conclusion: 该研究为开发更高效、自适应的认知雷达系统提供了方法，能够平衡动态环境中的多目标竞争。

Abstract: The time allocation problem in multi-function cognitive radar systems focuses
on the trade-off between scanning for newly emerging targets and tracking the
previously detected targets. We formulate this as a multi-objective
optimization problem and employ deep reinforcement learning to find
Pareto-optimal solutions and compare deep deterministic policy gradient (DDPG)
and soft actor-critic (SAC) algorithms. Our results demonstrate the
effectiveness of both algorithms in adapting to various scenarios, with SAC
showing improved stability and sample efficiency compared to DDPG. We further
employ the NSGA-II algorithm to estimate an upper bound on the Pareto front of
the considered problem. This work contributes to the development of more
efficient and adaptive cognitive radar systems capable of balancing multiple
competing objectives in dynamic environments.

</details>


### [55] [Leaner Training, Lower Leakage: Revisiting Memorization in LLM Fine-Tuning with LoRA](https://arxiv.org/abs/2506.20856)
*Fei Wang,Baochun Li*

Main category: cs.LG

TL;DR: 研究发现，LoRA微调显著降低了记忆风险，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLMs）在微调中的记忆问题，特别是LoRA微调的影响。

Method: 使用基于相似性的记忆度量，比较不同微调策略（如LoRA和全微调）的记忆效果。

Result: LoRA微调在减少记忆风险方面优于全微调，且任务性能不受影响。

Conclusion: LoRA微调是一种高效且低风险的参数优化方法。

Abstract: Memorization in large language models (LLMs) makes them vulnerable to data
extraction attacks. While pre-training memorization has been extensively
studied, fewer works have explored its impact in fine-tuning, particularly for
LoRA fine-tuning, a widely adopted parameter-efficient method.
  In this work, we re-examine memorization in fine-tuning and uncover a
surprising divergence from prior findings across different fine-tuning
strategies. Factors such as model scale and data duplication, which strongly
influence memorization in pre-training and full fine-tuning, do not follow the
same trend in LoRA fine-tuning. Using a more relaxed similarity-based
memorization metric, we demonstrate that LoRA significantly reduces
memorization risks compared to full fine-tuning, while still maintaining strong
task performance.

</details>


### [56] [Omniwise: Predicting GPU Kernels Performance with LLMs](https://arxiv.org/abs/2506.20886)
*Zixian Wang,Cole Ramos,Muhammad A. Awad,Keith Lowery*

Main category: cs.LG

TL;DR: Omniwise是一个端到端的自监督微调管道，利用大型语言模型（LLMs）预测GPU内核性能，无需代码执行或分析工具即可预测关键性能指标。


<details>
  <summary>Details</summary>
Motivation: 深度学习网络的快速发展推动了人工智能的进步，但在性能分析领域，尤其是GPU内核性能预测方面，仍存在挑战。

Method: Omniwise采用模型无关的轻量级方法，通过自监督微调LLMs，直接从内核代码预测性能指标。

Result: 在AMD MI250和MI300X架构上，Omniwise的预测误差在10%以内的准确率超过90%。

Conclusion: Omniwise为开发者提供了一个高效的工具，通过在线推理服务器和Visual Studio Code插件，将性能预测集成到工作流程中。

Abstract: In recent years, the rapid advancement of deep neural networks (DNNs) has
revolutionized artificial intelligence, enabling models with unprecedented
capabilities in understanding, generating, and processing complex data. These
powerful architectures have transformed a wide range of downstream
applications, tackling tasks beyond human reach. In this paper, we introduce
Omniwise, the first end-to-end, self-supervised fine-tuning pipeline that
applies large language models (LLMs) to GPU kernel performance prediction--a
novel use case in performance profiling. Omniwise is model-agnostic and
lightweight, achieving strong results even with a small 3B-parameter model. It
can predict key performance metrics, including memory bandwidth, cache hit
rates, GFLOPs, and arithmetic intensity, directly from kernel code without the
need for code execution or profiling tools. Our approach achieves over 90% of
predictions within 10% relative error on GPU kernels executed on AMD MI250 and
MI300X architectures. In addition to the pipeline, we develop an online
inference server and a Visual Studio Code plugin that seamlessly integrate
LLM-based performance prediction into developers' workflows.

</details>


### [57] [On the Necessity of Output Distribution Reweighting for Effective Class Unlearning](https://arxiv.org/abs/2506.20893)
*Yian Wang,Ali Ebrahimpour-Boroojeny,Hari Sundaram*

Main category: cs.LG

TL;DR: RWFT是一种轻量级的输出重加权遗忘方法，无需完全重新训练即可从分类器中删除整个类别，解决了现有遗忘方法在预测遗忘类别样本时的不足。


<details>
  <summary>Details</summary>
Motivation: 强制执行用户删除权利和减少有害或有偏见的预测需要从训练模型中遗忘特定类别，但完全重新训练成本高，现有方法无法复制重新训练模型的行为。

Method: 通过重新分配遗忘类别样本的预测概率质量，提出了一种对MIA-NN攻击鲁棒的方法，并引入基于总变差距离的新度量来量化残留泄漏。

Result: 实验表明，RWFT在现有评估指标和新提出的TV距离度量上均与完全重新训练结果匹配，性能优于现有最佳方法。

Conclusion: RWFT是一种高效且鲁棒的遗忘方法，显著提升了性能并解决了现有方法的局限性。

Abstract: In this work, we introduce an output-reweighting unlearning method, RWFT, a
lightweight technique that erases an entire class from a trained classifier
without full retraining. Forgetting specific classes from trained models is
essential for enforcing user deletion rights and mitigating harmful or biased
predictions. The full retraining is costly and existing unlearning methods fail
to replicate the behavior of the retrained models when predicting samples from
the unlearned class. We prove this failure by designing a variant of membership
inference attacks, MIA-NN that successfully reveals the unlearned class for any
of these methods. We propose a simple redistribution of the probability mass
for the prediction on the samples in the forgotten class which is robust to
MIA-NN. We also introduce a new metric based on the total variation (TV)
distance of the prediction probabilities to quantify residual leakage to
prevent future methods from susceptibility to the new attack. Through extensive
experiments with state of the art baselines in machine unlearning, we show that
our approach matches the results of full retraining in both metrics used for
evaluation by prior work and the new metric we propose in this work. Compare to
state-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%
in our new TV-based metric over the best existing method.

</details>


### [58] [Graph-Structured Feedback Multimodel Ensemble Online Conformal Prediction](https://arxiv.org/abs/2506.20898)
*Erfan Hajihashemi,Yanning Shen*

Main category: cs.LG

TL;DR: 提出了一种新的多模型在线共形预测算法，通过二分图反馈动态选择有效模型子集，降低计算复杂度并缩小预测集大小。


<details>
  <summary>Details</summary>
Motivation: 解决多模型在线共形预测中候选模型集过大或包含低效模型导致的计算复杂度和预测集性能下降问题。

Method: 利用二分图反馈动态识别有效模型子集，并结合预测集大小和模型损失优化模型选择。

Result: 算法确保了有效覆盖并实现次线性遗憾，实验显示其能生成更小的预测集且优于现有方法。

Conclusion: 所提方法在多模型在线共形预测中显著提升了效率和性能。

Abstract: Online conformal prediction has demonstrated its capability to construct a
prediction set for each incoming data point that covers the true label with a
predetermined probability. To cope with potential distribution shift,
multi-model online conformal prediction has been introduced to select and
leverage different models from a preselected candidate set. Along with the
improved flexibility, the choice of the preselected set also brings challenges.
A candidate set that includes a large number of models may increase the
computational complexity. In addition, the inclusion of irrelevant models with
poor performance may negatively impact the performance and lead to
unnecessarily large prediction sets. To address these challenges, we propose a
novel multi-model online conformal prediction algorithm that identifies a
subset of effective models at each time step by collecting feedback from a
bipartite graph, which is refined upon receiving new data. A model is then
selected from this subset to construct the prediction set, resulting in reduced
computational complexity and smaller prediction sets. Additionally, we
demonstrate that using prediction set size as feedback, alongside model loss,
can significantly improve efficiency by constructing smaller prediction sets
while still satisfying the required coverage guarantee. The proposed algorithms
are proven to ensure valid coverage and achieve sublinear regret. Experiments
on real and synthetic datasets validate that the proposed methods construct
smaller prediction sets and outperform existing multi-model online conformal
prediction approaches.

</details>


### [59] [Optimal Single-Policy Sample Complexity and Transient Coverage for Average-Reward Offline RL](https://arxiv.org/abs/2506.20904)
*Matthew Zurek,Guy Zamir,Yudong Chen*

Main category: cs.LG

TL;DR: 论文研究了平均奖励MDP中的离线强化学习，提出了仅依赖目标策略的复杂度度量（如偏差跨度和策略命中半径），并首次实现了完全单策略样本复杂度边界。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在平均奖励MDP中面临分布偏移和非均匀覆盖的挑战，且理论视角研究较少。现有工作依赖单策略数据覆盖假设，但需额外复杂度度量。

Method: 提出基于悲观折扣值迭代的算法，结合新颖的分位数裁剪技术，使用更尖锐的基于经验跨度的惩罚函数。

Result: 首次实现了完全单策略样本复杂度边界，并处理了一般弱通信MDP，无需先验参数知识。

Conclusion: 研究表明，学习需要超出目标策略稳态分布的覆盖假设，区分了单策略复杂度度量与先前研究。

Abstract: We study offline reinforcement learning in average-reward MDPs, which
presents increased challenges from the perspectives of distribution shift and
non-uniform coverage, and has been relatively underexamined from a theoretical
perspective. While previous work obtains performance guarantees under
single-policy data coverage assumptions, such guarantees utilize additional
complexity measures which are uniform over all policies, such as the uniform
mixing time. We develop sharp guarantees depending only on the target policy,
specifically the bias span and a novel policy hitting radius, yielding the
first fully single-policy sample complexity bound for average-reward offline
RL. We are also the first to handle general weakly communicating MDPs,
contrasting restrictive structural assumptions made in prior work. To achieve
this, we introduce an algorithm based on pessimistic discounted value iteration
enhanced by a novel quantile clipping technique, which enables the use of a
sharper empirical-span-based penalty function. Our algorithm also does not
require any prior parameter knowledge for its implementation. Remarkably, we
show via hard examples that learning under our conditions requires coverage
assumptions beyond the stationary distribution of the target policy,
distinguishing single-policy complexity measures from previously examined
cases. We also develop lower bounds nearly matching our main result.

</details>


### [60] [Explainable AI for Radar Resource Management: Modified LIME in Deep Reinforcement Learning](https://arxiv.org/abs/2506.20916)
*Ziyang Lu,M. Cenk Gursoy,Chilukuri K. Mohan,Pramod K. Varshney*

Main category: cs.LG

TL;DR: 论文提出了一种改进的LIME方法（DL-LIME），通过将深度学习融入采样过程，解决了传统LIME忽略特征相关性的问题，并在雷达资源管理中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在决策过程中表现出色，但其“黑盒”特性限制了可解释性。现有XAI方法如LIME因忽略特征相关性而存在不足。

Method: 提出DL-LIME，将深度学习融入LIME的采样过程，并在雷达资源管理的深度强化学习中应用。

Result: DL-LIME在保真度和任务性能上均优于传统LIME，并揭示了雷达资源管理决策中的关键因素。

Conclusion: DL-LIME是一种有效的XAI方法，显著提升了可解释性和决策性能。

Abstract: Deep reinforcement learning has been extensively studied in decision-making
processes and has demonstrated superior performance over conventional
approaches in various fields, including radar resource management (RRM).
However, a notable limitation of neural networks is their ``black box" nature
and recent research work has increasingly focused on explainable AI (XAI)
techniques to describe the rationale behind neural network decisions. One
promising XAI method is local interpretable model-agnostic explanations (LIME).
However, the sampling process in LIME ignores the correlations between
features. In this paper, we propose a modified LIME approach that integrates
deep learning (DL) into the sampling process, which we refer to as DL-LIME. We
employ DL-LIME within deep reinforcement learning for radar resource
management. Numerical results show that DL-LIME outperforms conventional LIME
in terms of both fidelity and task performance, demonstrating superior
performance with both metrics. DL-LIME also provides insights on which factors
are more important in decision making for radar resource management.

</details>


### [61] [LLM-guided Chemical Process Optimization with a Multi-Agent Approach](https://arxiv.org/abs/2506.20921)
*Tong Zeng,Srivathsan Badrinarayanan,Janghoon Ock,Cheng-Kai Lai,Amir Barati Farimani*

Main category: cs.LG

TL;DR: 论文提出了一种基于多智能体框架的化学过程优化方法，利用大型语言模型（LLM）智能体自主推断操作约束，并通过协作优化消除对预定义约束的需求。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法在操作约束不明确或不可用时效率低下，依赖主观启发式方法。本文旨在解决这一瓶颈。

Method: 采用AutoGen框架，结合OpenAI的o3模型，设计多个智能体分别负责约束生成、参数验证、模拟执行和优化指导。

Result: 在氢化脱烷基过程中验证，框架在计算效率上优于传统方法（20分钟内收敛，速度提升31倍），且性能相当。

Conclusion: 该方法在操作约束不明确或缺失的场景中具有显著潜力，尤其适用于新兴工艺和改造应用。

Abstract: Chemical process optimization is crucial to maximize production efficiency
and economic performance. Traditional methods, including gradient-based
solvers, evolutionary algorithms, and parameter grid searches, become
impractical when operating constraints are ill-defined or unavailable,
requiring engineers to rely on subjective heuristics to estimate feasible
parameter ranges. To address this constraint definition bottleneck, we present
a multi-agent framework of large language model (LLM) agents that autonomously
infer operating constraints from minimal process descriptions, then
collaboratively guide optimization using the inferred constraints. Our
AutoGen-based agentic framework employs OpenAI's o3 model, with specialized
agents for constraint generation, parameter validation, simulation execution,
and optimization guidance. Through two phases - autonomous constraint
generation using embedded domain knowledge, followed by iterative multi-agent
optimization - the framework eliminates the need for predefined operational
bounds. Validated on the hydrodealkylation process across cost, yield, and
yield-to-cost ratio metrics, the framework demonstrated competitive performance
with conventional optimization methods while achieving better computational
efficiency, requiring fewer iterations to converge. Our approach converged in
under 20 minutes, achieving a 31-fold speedup over grid search. Beyond
computational efficiency, the framework's reasoning-guided search demonstrates
sophisticated process understanding, correctly identifying utility trade-offs,
and applying domain-informed heuristics. This approach shows significant
potential for optimization scenarios where operational constraints are poorly
characterized or unavailable, particularly for emerging processes and retrofit
applications.

</details>


### [62] [Interpretable Representation Learning for Additive Rule Ensembles](https://arxiv.org/abs/2506.20927)
*Shahrzad Behzadimanesh,Pierre Le Bodic,Geoffrey I. Webb,Mario Boley*

Main category: cs.LG

TL;DR: 论文提出了一种改进的符号规则集成方法，通过引入可学习的稀疏线性变换，提升模型表达能力，同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统符号规则集成依赖轴平行决策区域，需要精心设计的特征才能达到高精度，否则需增加规则复杂度，牺牲可解释性。

Method: 引入可学习的稀疏线性变换（如$x^T w \geq t$），通过序列贪婪优化和迭代加权逻辑回归学习规则。

Result: 实验表明，该方法在保持测试风险与现有方法相当的同时，显著降低了模型复杂度。

Conclusion: 该方法在提升模型表达能力的同时，保持了可解释性，适用于多种基准数据集。

Abstract: Small additive ensembles of symbolic rules offer interpretable prediction
models. Traditionally, these ensembles use rule conditions based on
conjunctions of simple threshold propositions $x \geq t$ on a single input
variable $x$ and threshold $t$, resulting geometrically in axis-parallel
polytopes as decision regions. While this form ensures a high degree of
interpretability for individual rules and can be learned efficiently using the
gradient boosting approach, it relies on having access to a curated set of
expressive and ideally independent input features so that a small ensemble of
axis-parallel regions can describe the target variable well. Absent such
features, reaching sufficient accuracy requires increasing the number and
complexity of individual rules, which diminishes the interpretability of the
model. Here, we extend classical rule ensembles by introducing logical
propositions with learnable sparse linear transformations of input variables,
i.e., propositions of the form $\mathbf{x}^\mathrm{T}\mathbf{w} \geq t$, where
$\mathbf{w}$ is a learnable sparse weight vector, enabling decision regions as
general polytopes with oblique faces. We propose a learning method using
sequential greedy optimization based on an iteratively reweighted formulation
of logistic regression. Experimental results demonstrate that the proposed
method efficiently constructs rule ensembles with the same test risk as
state-of-the-art methods while significantly reducing model complexity across
ten benchmark datasets.

</details>


### [63] [Model State Arithmetic for Machine Unlearning](https://arxiv.org/abs/2506.20941)
*Keivan Rezaei,Mehrdad Saberi,Abhilasha Ravichander,Soheil Feizi*

Main category: cs.LG

TL;DR: 论文提出了一种名为MSA的新算法，通过利用模型检查点来估计和消除数据点的影响，旨在低成本地实现大语言模型的数据遗忘。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练数据可能包含隐私、版权或低质量内容，完全重新训练成本过高，因此需要高效的数据遗忘算法。

Method: 利用模型检查点（训练过程中的模型状态快照）来估计和消除特定数据点的影响。

Result: MSA在多个基准测试、模型和评估指标上优于现有算法，表明其有效性。

Conclusion: MSA为实现灵活的大语言模型数据遗忘提供了一种有效方法。

Abstract: Large language models are trained on massive corpora of web data, which may
include private data, copyrighted material, factually inaccurate data, or data
that degrades model performance. Eliminating the influence of such problematic
datapoints through complete retraining -- by repeatedly pretraining the model
on datasets that exclude these specific instances -- is computationally
prohibitive. For this reason, unlearning algorithms have emerged that aim to
eliminate the influence of particular datapoints, while otherwise preserving
the model -- at a low computational cost. However, precisely estimating and
undoing the influence of individual datapoints has proved to be challenging. In
this work, we propose a new algorithm, MSA, for estimating and undoing the
influence of datapoints -- by leveraging model checkpoints i.e. artifacts
capturing model states at different stages of pretraining. Our experimental
results demonstrate that MSA consistently outperforms existing machine
unlearning algorithms across multiple benchmarks, models, and evaluation
metrics, suggesting that MSA could be an effective approach towards more
flexible large language models that are capable of data erasure.

</details>


### [64] [Antibody Design and Optimization with Multi-scale Equivariant Graph Diffusion Models for Accurate Complex Antigen Binding](https://arxiv.org/abs/2506.20957)
*Jiameng Chen,Xiantao Cai,Jia Wu,Wenbin Hu*

Main category: cs.LG

TL;DR: AbMEGD是一种端到端框架，结合多尺度等变图扩散技术，用于抗体序列和结构的协同设计，显著提升了氨基酸恢复率和结构精度。


<details>
  <summary>Details</summary>
Motivation: 当前抗体设计方法在捕捉几何特征和泛化新抗原界面方面存在不足，AbMEGD旨在解决这些问题。

Method: AbMEGD整合了多尺度等变图扩散技术，结合原子级几何特征和残基级嵌入，确保几何精度和计算效率。

Result: 实验显示，AbMEGD在氨基酸恢复率、改进百分比和CDR-H3区域的均方根偏差上均优于DiffAb模型。

Conclusion: AbMEGD在保持结构完整性的同时提升了功能，为抗体设计设立了新标准。

Abstract: Antibody design remains a critical challenge in therapeutic and diagnostic
development, particularly for complex antigens with diverse binding interfaces.
Current computational methods face two main limitations: (1) capturing
geometric features while preserving symmetries, and (2) generalizing novel
antigen interfaces. Despite recent advancements, these methods often fail to
accurately capture molecular interactions and maintain structural integrity. To
address these challenges, we propose \textbf{AbMEGD}, an end-to-end framework
integrating \textbf{M}ulti-scale \textbf{E}quivariant \textbf{G}raph
\textbf{D}iffusion for antibody sequence and structure co-design. Leveraging
advanced geometric deep learning, AbMEGD combines atomic-level geometric
features with residue-level embeddings, capturing local atomic details and
global sequence-structure interactions. Its E(3)-equivariant diffusion method
ensures geometric precision, computational efficiency, and robust
generalizability for complex antigens. Furthermore, experiments using the
SAbDab database demonstrate a 10.13\% increase in amino acid recovery, 3.32\%
rise in improvement percentage, and a 0.062~\AA\ reduction in root mean square
deviation within the critical CDR-H3 region compared to DiffAb, a leading
antibody design model. These results highlight AbMEGD's ability to balance
structural integrity with improved functionality, establishing a new benchmark
for sequence-structure co-design and affinity optimization. The code is
available at: https://github.com/Patrick221215/AbMEGD.

</details>


### [65] [SharpZO: Hybrid Sharpness-Aware Vision Language Model Prompt Tuning via Forward-Only Passes](https://arxiv.org/abs/2506.20990)
*Yifan Yang,Zhen Zhang,Rupak Vignesh Swaminathan,Jing Liu,Nathan Susanj,Zheng Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为SharpZO的混合优化方法，用于在无需反向传播的情况下微调视觉语言模型，显著提升了性能和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有BP-free微调方法依赖高方差进化策略或零阶优化，性能不佳，无法满足边缘设备的需求。

Method: SharpZO采用两阶段优化：先通过锐度感知进化策略全局探索和平滑损失函数，再通过稀疏零阶优化进行局部搜索。

Result: 在CLIP模型上的实验显示，SharpZO平均性能提升7%，收敛速度更快。

Conclusion: SharpZO为边缘设备上的VLM微调提供了一种高效且性能优越的解决方案。

Abstract: Fine-tuning vision language models (VLMs) has achieved remarkable performance
across various downstream tasks; yet, it requires access to model gradients
through backpropagation (BP), making them unsuitable for memory-constrained,
inference-only edge devices. To address this limitation, previous work has
explored various BP-free fine-tuning methods. However, these approaches often
rely on high-variance evolutionary strategies (ES) or zeroth-order (ZO)
optimization, and often fail to achieve satisfactory performance. In this
paper, we propose a hybrid Sharpness-aware Zeroth-order optimization (SharpZO)
approach, specifically designed to enhance the performance of ZO VLM
fine-tuning via a sharpness-aware warm-up training. SharpZO features a
two-stage optimization process: a sharpness-aware ES stage that globally
explores and smooths the loss landscape to construct a strong initialization,
followed by a fine-grained local search via sparse ZO optimization. The entire
optimization relies solely on forward passes. Detailed theoretical analysis and
extensive experiments on CLIP models demonstrate that SharpZO significantly
improves accuracy and convergence speed, achieving up to 7% average gain over
state-of-the-art forward-only methods.

</details>


### [66] [Distilling Normalizing Flows](https://arxiv.org/abs/2506.21003)
*Steven Walton,Valeriy Klyukin,Maksim Artemev,Denis Derkach,Nikita Orlov,Humphrey Shi*

Main category: cs.LG

TL;DR: 论文研究了通过知识蒸馏提升小规模归一化流模型的采样质量和密度估计能力，发现该方法能显著提高性能并减少模型规模。


<details>
  <summary>Details</summary>
Motivation: 显式密度模型（如归一化流）在生成模型中具有优势，但训练困难且采样质量较低。本文旨在探索知识蒸馏在归一化流中的应用，以提升小模型的性能。

Method: 提出新颖的知识蒸馏技术，利用归一化流的中间层特性进行非传统的知识传递，优化学生模型的性能。

Result: 实验表明，通过蒸馏，学生模型在显著减小规模的同时，性能大幅提升，且吞吐量因参数减少而增加。

Conclusion: 知识蒸馏在归一化流中具有潜力，能有效平衡模型规模与性能，为实际应用提供更高效的解决方案。

Abstract: Explicit density learners are becoming an increasingly popular technique for
generative models because of their ability to better model probability
distributions. They have advantages over Generative Adversarial Networks due to
their ability to perform density estimation and having exact latent-variable
inference. This has many advantages, including: being able to simply
interpolate, calculate sample likelihood, and analyze the probability
distribution. The downside of these models is that they are often more
difficult to train and have lower sampling quality.
  Normalizing flows are explicit density models, that use composable bijective
functions to turn an intractable probability function into a tractable one. In
this work, we present novel knowledge distillation techniques to increase
sampling quality and density estimation of smaller student normalizing flows.
We seek to study the capacity of knowledge distillation in Compositional
Normalizing Flows to understand the benefits and weaknesses provided by these
architectures. Normalizing flows have unique properties that allow for a
non-traditional forms of knowledge transfer, where we can transfer that
knowledge within intermediate layers. We find that through this distillation,
we can make students significantly smaller while making substantial performance
gains over a non-distilled student. With smaller models there is a
proportionally increased throughput as this is dependent upon the number of
bijectors, and thus parameters, in the network.

</details>


### [67] [TRIDENT: Tri-Modal Molecular Representation Learning with Taxonomic Annotations and Local Correspondence](https://arxiv.org/abs/2506.21028)
*Feng Jiang,Mangal Prakash,Hehuan Ma,Jianyuan Deng,Yuzhi Guo,Amina Mollaysa,Tommaso Mansi,Rui Liao,Junzhou Huang*

Main category: cs.LG

TL;DR: TRIDENT是一个多模态分子表示学习框架，整合SMILES、文本描述和分类功能注释，通过全局和局部对齐目标提升分子属性预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有分子表示学习方法忽视了文本和分类信息，TRIDENT旨在填补这一空白，利用多模态数据提升表示学习效果。

Method: TRIDENT采用基于体积的全局对齐目标和局部对齐目标，结合动量机制动态平衡全局与局部对齐。

Result: 在11个下游任务中达到最优性能，验证了多模态数据的价值。

Conclusion: TRIDENT通过整合多模态信息，显著提升了分子属性预测的准确性。

Abstract: Molecular property prediction aims to learn representations that map chemical
structures to functional properties. While multimodal learning has emerged as a
powerful paradigm to learn molecular representations, prior works have largely
overlooked textual and taxonomic information of molecules for representation
learning. We introduce TRIDENT, a novel framework that integrates molecular
SMILES, textual descriptions, and taxonomic functional annotations to learn
rich molecular representations. To achieve this, we curate a comprehensive
dataset of molecule-text pairs with structured, multi-level functional
annotations. Instead of relying on conventional contrastive loss, TRIDENT
employs a volume-based alignment objective to jointly align tri-modal features
at the global level, enabling soft, geometry-aware alignment across modalities.
Additionally, TRIDENT introduces a novel local alignment objective that
captures detailed relationships between molecular substructures and their
corresponding sub-textual descriptions. A momentum-based mechanism dynamically
balances global and local alignment, enabling the model to learn both broad
functional semantics and fine-grained structure-function mappings. TRIDENT
achieves state-of-the-art performance on 11 downstream tasks, demonstrating the
value of combining SMILES, textual, and taxonomic functional annotations for
molecular property prediction.

</details>


### [68] [Little By Little: Continual Learning via Self-Activated Sparse Mixture-of-Rank Adaptive Learning](https://arxiv.org/abs/2506.21035)
*Haodong Lu,Chongyang Zhao,Jason Xue,Lina Yao,Kristen Moore,Dong Gong*

Main category: cs.LG

TL;DR: MoRA提出了一种基于稀疏秩激活的自适应学习方法，用于解决持续学习中的干扰、冗余和路由模糊问题。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中预训练模型面临的灾难性遗忘和任务干扰问题。

Method: MoRA将每个秩-r更新分解为r个秩-1组件，作为独立专家，并通过稀疏激活和秩剪枝自适应选择。

Result: 在CLIP和LLMs的持续学习任务中，MoRA显著提升了性能，减少了遗忘并改善了泛化能力。

Conclusion: MoRA通过细粒度的秩-1专家混合和自适应路由，有效解决了持续学习中的关键挑战。

Abstract: Continual learning (CL) with large pre-trained models is challenged by
catastrophic forgetting and task interference. Existing LoRA-based
Mixture-of-Experts (MoE) approaches mitigate forgetting by assigning and
freezing task-specific adapters, but suffer from interference, redundancy, and
ambiguous routing due to coarse adapter-level selection. However, this design
introduces three key challenges: 1) Interference: Activating full LoRA experts
per input leads to subspace interference and prevents selective reuse of useful
components across tasks. 2) Redundancy: Newly added experts often duplicate or
contradict existing knowledge due to unnecessary activation of unrelated ranks
and insufficient reuse of relevant ones. 3) Ambiguity: Overlapping features
across tasks confuse the router, resulting in unstable expert assignments. As
more experts accumulate, earlier task routing degrades, accelerating
forgetting. We propose MoRA, a Mixture-of-Rank Adaptive learning approach with
self-activated and sparse rank activation for CL. Unlike mixing multiple
low-rank matrices, MoRA decomposes each rank-r update into r rank-1 components,
each treated as an independent expert, enabling fine-grained mixture of rank-1
expert utilization while mitigating interference and redundancy. To avoid
ambiguous routing, we propose that each rank-1 expert can infer its own
relevance via intermediate activations. Coupled with our proposed rank pruning
and activation budgets, MoRA adaptively selects a sparse mixture of ranks per
input. We validate MoRA on continual learning tasks with CLIP and large
language models (LLMs), analyzing both in-domain learning and out-of-domain
forgetting/generalization during fine-tuning. MoRA shows significant
effectiveness on enhancing CL with PTMs, and improving generalization while
mitigating forgetting.

</details>


### [69] [RL-Selector: Reinforcement Learning-Guided Data Selection via Redundancy Assessment](https://arxiv.org/abs/2506.21037)
*Suorong Yang,Peijia Li,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: 论文提出RL-Selector，通过强化学习优化数据选择策略，减少冗余样本，提升训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习依赖大规模数据集，但训练成本高且存在冗余，需要更高效的数据选择方法。

Method: 引入epsilon-sample cover量化样本冗余，将数据选择建模为强化学习问题，设计RL-Selector优化选择策略。

Result: 实验表明RL-Selector优于现有方法，所选数据集训练的模型泛化性能更好且效率更高。

Conclusion: RL-Selector通过动态优化数据选择，显著提升训练效率和模型性能。

Abstract: Modern deep architectures often rely on large-scale datasets, but training on
these datasets incurs high computational and storage overhead. Real-world
datasets often contain substantial redundancies, prompting the need for more
data-efficient training paradigms. Data selection has shown promise to mitigate
redundancy by identifying the most representative samples, thereby reducing
training costs without compromising performance. Existing methods typically
rely on static scoring metrics or pretrained models, overlooking the combined
effect of selected samples and their evolving dynamics during training. We
introduce the concept of epsilon-sample cover, which quantifies sample
redundancy based on inter-sample relationships, capturing the intrinsic
structure of the dataset. Based on this, we reformulate data selection as a
reinforcement learning (RL) process and propose RL-Selector, where a
lightweight RL agent optimizes the selection policy by leveraging
epsilon-sample cover derived from evolving dataset distribution as a reward
signal. Extensive experiments across benchmark datasets and diverse
architectures demonstrate that our method consistently outperforms existing
state-of-the-art baselines. Models trained with our selected datasets show
enhanced generalization performance with improved training efficiency.

</details>


### [70] [Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement Learning](https://arxiv.org/abs/2506.21039)
*Jaebak Hwang,Sanghyeon Lee,Jeongmo Kim,Seungyul Han*

Main category: cs.LG

TL;DR: SSE是一种基于图的分层强化学习框架，通过强制单步子目标可达性和动态路径优化，显著提升了长时程目标任务的效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 长时程目标任务的奖励稀疏和子目标不可行性是传统强化学习方法的主要挑战。

Method: SSE通过结构约束高层决策、解耦探索策略和动态路径优化来解决这些问题。

Result: 实验表明，SSE在效率和成功率上均优于现有方法。

Conclusion: SSE为长时程目标任务提供了一种高效可靠的解决方案。

Abstract: Long-horizon goal-conditioned tasks pose fundamental challenges for
reinforcement learning (RL), particularly when goals are distant and rewards
are sparse. While hierarchical and graph-based methods offer partial solutions,
they often suffer from subgoal infeasibility and inefficient planning. We
introduce Strict Subgoal Execution (SSE), a graph-based hierarchical RL
framework that enforces single-step subgoal reachability by structurally
constraining high-level decision-making. To enhance exploration, SSE employs a
decoupled exploration policy that systematically traverses underexplored
regions of the goal space. Furthermore, a failure-aware path refinement, which
refines graph-based planning by dynamically adjusting edge costs according to
observed low-level success rates, thereby improving subgoal reliability.
Experimental results across diverse long-horizon benchmarks demonstrate that
SSE consistently outperforms existing goal-conditioned RL and hierarchical RL
approaches in both efficiency and success rate.

</details>


### [71] [Efficient Skill Discovery via Regret-Aware Optimization](https://arxiv.org/abs/2506.21044)
*He Zhang,Ming Zhou,Shaopeng Zhai,Ying Sun,Hui Xiong*

Main category: cs.LG

TL;DR: 提出了一种基于遗憾感知的无监督技能发现方法，通过对抗性框架提升技能多样性和效率，在高维环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有无监督技能发现方法在探索多样性方面表现良好，但在高维环境中效率不足。

Method: 将技能发现建模为技能生成与策略学习的极小极大博弈，利用遗憾评分指导技能探索，并通过可升级的技能生成器避免退化。

Result: 实验表明，该方法在效率和多样性上优于基线，高维环境中零样本性能提升15%。

Conclusion: 通过对抗性框架和遗憾感知机制，显著提升了技能发现的效率和多样性。

Abstract: Unsupervised skill discovery aims to learn diverse and distinguishable
behaviors in open-ended reinforcement learning. For existing methods, they
focus on improving diversity through pure exploration, mutual information
optimization, and learning temporal representation. Despite that they perform
well on exploration, they remain limited in terms of efficiency, especially for
the high-dimensional situations. In this work, we frame skill discovery as a
min-max game of skill generation and policy learning, proposing a regret-aware
method on top of temporal representation learning that expands the discovered
skill space along the direction of upgradable policy strength. The key insight
behind the proposed method is that the skill discovery is adversarial to the
policy learning, i.e., skills with weak strength should be further explored
while less exploration for the skills with converged strength. As an
implementation, we score the degree of strength convergence with regret, and
guide the skill discovery with a learnable skill generator. To avoid
degeneration, skill generation comes from an up-gradable population of skill
generators. We conduct experiments on environments with varying complexities
and dimension sizes. Empirical results show that our method outperforms
baselines in both efficiency and diversity. Moreover, our method achieves a 15%
zero shot improvement in high-dimensional environments, compared to existing
methods.

</details>


### [72] [FedDAA: Dynamic Client Clustering for Concept Drift Adaptation in Federated Learning](https://arxiv.org/abs/2506.21054)
*Fu Peng,Ming Tang*

Main category: cs.LG

TL;DR: FedDAA是一个动态聚类联邦学习框架，用于适应多源概念漂移并保留历史知识。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法主要关注真实漂移，而忽略了虚拟或标签漂移，导致灾难性遗忘。

Method: FedDAA包含三个模块：聚类数量确定、真实漂移检测和概念漂移适应。

Result: 在Fashion-MNIST、CIFAR-10和CIFAR-100上，FedDAA比现有方法准确率提高了7.84%至8.52%。

Conclusion: FedDAA能有效区分和适应多源概念漂移，提升模型性能。

Abstract: In federated learning (FL), the data distribution of each client may change
over time, introducing both temporal and spatial data heterogeneity, known as
concept drift. Data heterogeneity arises from three drift sources: real drift
(a shift in the conditional distribution P(y|x)), virtual drift (a shift in the
input distribution P(x)), and label drift (a shift in the label distribution
P(y)). However, most existing FL methods addressing concept drift primarily
focus on real drift. When clients experience virtual or label drift, these
methods often fail to selectively retain useful historical knowledge, leading
to catastrophic forgetting. A key challenge lies in distinguishing different
sources of drift, as they require distinct adaptation strategies: real drift
calls for discarding outdated data, while virtual or label drift benefits from
retaining historical data. Without explicitly identifying the drift sources, a
general adaptation strategy is suboptimal and may harm generalization. To
address this challenge, we propose FedDAA, a dynamic clustered FL framework
designed to adapt to multi-source concept drift while preserving valuable
historical knowledge. Specifically, FedDAA integrates three modules: a cluster
number determination module to find the optimal number of clusters; a real
drift detection module to distinguish real drift from virtual/label drift; and
a concept drift adaptation module to adapt to new data while retaining useful
historical information. We provide theoretical convergence guarantees, and
experiments show that FedDAA achieves 7.84% to 8.52% accuracy improvements over
state-of-the-art methods on Fashion-MNIST, CIFAR-10, and CIFAR-100.

</details>


### [73] [Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge Graph](https://arxiv.org/abs/2506.21071)
*Jingwei Wang,Zai Zhang,Hao Qian,Chunjing Gan,Binbin Hu,Ziqi Liu,Zhiqiang Zhang,Jun Zhou,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: 提出了一种利用知识图谱生成高质量指令数据的新方法，显著提升了大型语言模型的工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 提升大型语言模型（LLMs）的工具使用能力，解决现有方法生成的指令数据质量不足的问题。

Method: 从知识图谱中提取查询路径，转化为用户查询，并将实体关系转化为可操作工具，生成高质量指令数据。

Result: 实验表明，仅需少量合成数据微调即可显著提升LLMs的工具使用能力和整体性能。

Conclusion: 知识图谱生成的指令数据能有效提升LLMs的工具使用能力，为相关应用提供了新思路。

Abstract: Teaching large language models (LLMs) to use tools is crucial for improving
their problem-solving abilities and expanding their applications. However,
effectively using tools is challenging because it requires a deep understanding
of tool functionalities and user intentions. Previous methods relied mainly on
LLMs to generate instruction data, but the quality of these data was often
insufficient. In this paper, we propose a new method that uses knowledge graphs
to generate high-quality instruction data for LLMs. Knowledge graphs are
manually curated datasets rich in semantic information. We begin by extracting
various query pathways from a given knowledge graph, which are transformed into
a broad spectrum of user queries. We then translate the relationships between
entities into actionable tools and parse the pathways of each query into
detailed solution steps, thereby creating high-quality instruction data. Our
experiments show that fine-tuning on just a small sample of this synthetic data
can significantly improve the tool utilization and overall capabilities of
LLMs.

</details>


### [74] [Chain-of-Thought Enhanced Shallow Transformers for Wireless Symbol Detection](https://arxiv.org/abs/2506.21093)
*Li Fan,Peng Wang,Jing Yang,Cong Shen*

Main category: cs.LG

TL;DR: CHOOSE是一种基于CoT增强的浅层Transformer框架，用于无线符号检测，通过引入自回归潜在推理步骤提升浅层模型的推理能力，无需增加模型深度。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于ICL的Transformer模型因深度架构导致的高存储和计算成本问题。

Method: 提出CHOOSE框架，在隐藏空间中引入自回归潜在推理步骤，增强浅层Transformer的推理能力。

Result: 实验表明CHOOSE在性能上优于传统浅层Transformer，接近深度Transformer，同时保持高效存储和计算。

Conclusion: CHOOSE为资源受限的移动设备部署Transformer算法提供了有前景的方向。

Abstract: Transformers have shown potential in solving wireless communication problems,
particularly via in-context learning (ICL), where models adapt to new tasks
through prompts without requiring model updates. However, prior ICL-based
Transformer models rely on deep architectures with many layers to achieve
satisfactory performance, resulting in substantial storage and computational
costs. In this work, we propose CHain Of thOught Symbol dEtection (CHOOSE), a
CoT-enhanced shallow Transformer framework for wireless symbol detection. By
introducing autoregressive latent reasoning steps within the hidden space,
CHOOSE significantly improves the reasoning capacity of shallow models (1-2
layers) without increasing model depth. This design enables lightweight
Transformers to achieve detection performance comparable to much deeper models,
making them well-suited for deployment on resource-constrained mobile devices.
Experimental results demonstrate that our approach outperforms conventional
shallow Transformers and achieves performance comparable to that of deep
Transformers, while maintaining storage and computational efficiency. This
represents a promising direction for implementing Transformer-based algorithms
in wireless receivers with limited computational resources.

</details>


### [75] [FeDa4Fair: Client-Level Federated Datasets for Fairness Evaluation](https://arxiv.org/abs/2506.21095)
*Xenia Heilmann,Luca Corbucci,Mattia Cerrato,Anna Monreale*

Main category: cs.LG

TL;DR: 论文提出FeDa4Fair库，用于生成评估联邦学习中公平性方法的异构数据集，并发布四个数据集和基准测试工具。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的公平性问题因客户端数据分布不均而复杂化，现有方法多关注单一敏感属性，难以满足多样化需求。

Method: 开发FeDa4Fair库生成异构数据集，发布四个数据集和基准测试工具，提供公平性评估函数。

Result: 提供了一套工具和数据集，支持公平性研究的可重复性和一致性评估。

Conclusion: FeDa4Fair为联邦学习公平性研究提供了标准化工具和数据集，填补了现有方法的不足。

Abstract: Federated Learning (FL) enables collaborative model training across multiple
clients without sharing clients' private data. However, fairness remains a key
concern, as biases in local clients' datasets can impact the entire federated
system. Heterogeneous data distributions across clients may lead to models that
are fairer for some clients than others. Although several fairness-enhancing
solutions are present in the literature, most focus on mitigating bias for a
single sensitive attribute, typically binary, overlooking the diverse and
sometimes conflicting fairness needs of different clients. This limited
perspective can limit the effectiveness of fairness interventions for the
different clients. To support more robust and reproducible fairness research in
FL, we aim to enable a consistent benchmarking of fairness-aware FL methods at
both the global and client levels. In this paper, we contribute in three ways:
(1) We introduce FeDa4Fair, a library to generate tabular datasets tailored to
evaluating fair FL methods under heterogeneous client bias; (2) we release four
bias-heterogeneous datasets and corresponding benchmarks to compare fairness
mitigation methods in a controlled environment; (3) we provide ready-to-use
functions for evaluating fairness outcomes for these datasets.

</details>


### [76] [Interpretable Hierarchical Concept Reasoning through Attention-Guided Graph Learning](https://arxiv.org/abs/2506.21102)
*David Debot,Pietro Barbiero,Gabriele Dominici,Giuseppe Marra*

Main category: cs.LG

TL;DR: H-CMR是一种新的概念基础模型，通过分层概念记忆推理器提供概念和任务预测的可解释性，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 当前概念基础模型仅对最终任务预测提供可解释性，而概念预测本身仍为黑盒，H-CMR旨在解决这一局限性。

Method: H-CMR使用有向无环图建模概念间关系，通过神经注意力机制选择逻辑规则，分层预测概念和任务。

Result: H-CMR在性能上达到最先进水平，同时支持概念和模型干预，显著提升推理准确性和训练数据效率。

Conclusion: H-CMR为概念和任务预测提供全面可解释性，同时保持高性能，适合需要人机交互的场景。

Abstract: Concept-Based Models (CBMs) are a class of deep learning models that provide
interpretability by explaining predictions through high-level concepts. These
models first predict concepts and then use them to perform a downstream task.
However, current CBMs offer interpretability only for the final task
prediction, while the concept predictions themselves are typically made via
black-box neural networks. To address this limitation, we propose Hierarchical
Concept Memory Reasoner (H-CMR), a new CBM that provides interpretability for
both concept and task predictions. H-CMR models relationships between concepts
using a learned directed acyclic graph, where edges represent logic rules that
define concepts in terms of other concepts. During inference, H-CMR employs a
neural attention mechanism to select a subset of these rules, which are then
applied hierarchically to predict all concepts and the final task. Experimental
results demonstrate that H-CMR matches state-of-the-art performance while
enabling strong human interaction through concept and model interventions. The
former can significantly improve accuracy at inference time, while the latter
can enhance data efficiency during training when background knowledge is
available.

</details>


### [77] [Learning to Skip the Middle Layers of Transformers](https://arxiv.org/abs/2506.21103)
*Tim Lawson,Laurence Aitchison*

Main category: cs.LG

TL;DR: 论文提出了一种动态跳过Transformer中间层的新架构，但未能在计算效率与性能之间取得优于基线模型的平衡。


<details>
  <summary>Details</summary>
Motivation: 基于中间层冗余性和信息聚合的观察，希望通过动态跳过中间层来提升Transformer的效率。

Method: 使用学习门控机制动态跳过对称的中间层，并采用门控注意力机制防止跳过位置被后续token关注，同时通过正则化损失控制稀疏性。

Result: 在研究的规模下，该方法未能显著改善验证交叉熵与FLOPs之间的权衡。

Conclusion: 尽管方法未能达到预期效果，但代码已开源供进一步研究。

Abstract: Conditional computation is a popular strategy to make Transformers more
efficient. Existing methods often target individual modules (e.g.,
mixture-of-experts layers) or skip layers independently of one another.
However, interpretability research has demonstrated that the middle layers of
Transformers exhibit greater redundancy, and that early layers aggregate
information into token positions. Guided by these insights, we propose a novel
architecture that dynamically skips a variable number of layers from the middle
outward. In particular, a learned gating mechanism determines whether to bypass
a symmetric span of central blocks based on the input, and a gated attention
mechanism prevents subsequent tokens from attending to skipped token positions.
Residual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and
gate sparsity with an adaptive regularization loss. We had aimed to reduce
compute requirements for 'simpler' tokens and potentially foster an emergent
multi-level representational hierarchy but, at the scales investigated, our
approach does not achieve improvements in the trade-off between validation
cross-entropy and estimated FLOPs compared to dense baselines with fewer
layers. We release our code at https://github.com/tim-lawson/skip-middle.

</details>


### [78] [Unlasting: Unpaired Single-Cell Multi-Perturbation Estimation by Dual Conditional Diffusion Implicit Bridges](https://arxiv.org/abs/2506.21107)
*Changxi Chi,Jun Xia,Yufei Huang,Jingbo Zhou,Siyuan Li,Yunfan Liu,Chang Yu,Stan Z. Li*

Main category: cs.LG

TL;DR: 提出了一种基于双扩散隐式桥（DDIB）的框架，解决单细胞扰动数据未配对问题，整合基因调控网络（GRN）信息，并引入掩码机制预测沉默基因。


<details>
  <summary>Details</summary>
Motivation: 单细胞测序是破坏性过程，无法捕捉同一细胞在扰动前后的表型，导致数据未配对。现有方法或强制配对，或忽略未扰动与扰动细胞间关系。

Method: 基于DDIB框架学习数据分布映射，整合GRN信息传播扰动信号，引入掩码机制预测沉默基因，并提出双条件扩散模型Unlasting。

Result: 模型有效解决了未配对数据问题，提高了生成质量，并引入新的评估指标更好地反映单细胞响应的异质性。

Conclusion: Unlasting框架通过GRN指导和掩码机制，显著提升单细胞扰动数据分析的准确性和生成质量。

Abstract: Estimating single-cell responses across various perturbations facilitates the
identification of key genes and enhances drug screening, significantly boosting
experimental efficiency. However, single-cell sequencing is a destructive
process, making it impossible to capture the same cell's phenotype before and
after perturbation. Consequently, data collected under perturbed and
unperturbed conditions are inherently unpaired. Existing methods either attempt
to forcibly pair unpaired data using random sampling, or neglect the inherent
relationship between unperturbed and perturbed cells during the modeling. In
this work, we propose a framework based on Dual Diffusion Implicit Bridges
(DDIB) to learn the mapping between different data distributions, effectively
addressing the challenge of unpaired data. We further interpret this framework
as a form of data augmentation. We integrate gene regulatory network (GRN)
information to propagate perturbation signals in a biologically meaningful way,
and further incorporate a masking mechanism to predict silent genes, improving
the quality of generated profiles. Moreover, gene expression under the same
perturbation often varies significantly across cells, frequently exhibiting a
bimodal distribution that reflects intrinsic heterogeneity. To capture this, we
introduce a more suitable evaluation metric. We propose Unlasting, dual
conditional diffusion models that overcome the problem of unpaired single-cell
perturbation data and strengthen the model's insight into perturbations under
the guidance of the GRN, with a dedicated mask model designed to improve
generation quality by predicting silent genes. In addition, we introduce a
biologically grounded evaluation metric that better reflects the inherent
heterogeneity in single-cell responses.

</details>


### [79] [Robust Policy Switching for Antifragile Reinforcement Learning for UAV Deconfliction in Adversarial Environments](https://arxiv.org/abs/2506.21127)
*Deepak Kumar Panda,Weisi Guo*

Main category: cs.LG

TL;DR: 该论文提出了一种抗脆弱强化学习框架，通过动态选择策略来应对无人机导航中的对抗攻击，优于传统鲁棒方法。


<details>
  <summary>Details</summary>
Motivation: 无人机导航自动化增加使其易受对抗攻击，现有鲁棒强化学习方法对分布外偏移泛化能力有限。

Method: 引入基于折扣汤普森采样的切换机制，动态选择多鲁棒策略，最小化对抗性分布偏移。

Result: 在复杂导航环境中验证有效性，对抗攻击下表现优于传统方法，路径更短且冲突更少。

Conclusion: 抗脆弱框架显著提升无人机导航的适应性和鲁棒性，适用于动态对抗环境。

Abstract: The increasing automation of navigation for unmanned aerial vehicles (UAVs)
has exposed them to adversarial attacks that exploit vulnerabilities in
reinforcement learning (RL) through sensor manipulation. Although existing
robust RL methods aim to mitigate such threats, their effectiveness has limited
generalization to out-of-distribution shifts from the optimal value
distribution, as they are primarily designed to handle fixed perturbation. To
address this limitation, this paper introduces an antifragile RL framework that
enhances adaptability to broader distributional shifts by incorporating a
switching mechanism based on discounted Thompson sampling (DTS). This mechanism
dynamically selects among multiple robust policies to minimize adversarially
induced state-action-value distribution shifts. The proposed approach first
derives a diverse ensemble of action robust policies by accounting for a range
of perturbations in the policy space. These policies are then modeled as a
multiarmed bandit (MAB) problem, where DTS optimally selects policies in
response to nonstationary Bernoulli rewards, effectively adapting to evolving
adversarial strategies. Theoretical framework has also been provided where by
optimizing the DTS to minimize the overall regrets due to distributional shift,
results in effective adaptation against unseen adversarial attacks thus
inducing antifragility. Extensive numerical simulations validate the
effectiveness of the proposed framework in complex navigation environments with
multiple dynamic three-dimensional obstacles and with stronger projected
gradient descent (PGD) and spoofing attacks. Compared to conventional robust,
non-adaptive RL methods, the antifragile approach achieves superior
performance, demonstrating shorter navigation path lengths and a higher rate of
conflict-free navigation trajectories compared to existing robust RL techniques

</details>


### [80] [Curriculum-Guided Antifragile Reinforcement Learning for Secure UAV Deconfliction under Observation-Space Attacks](https://arxiv.org/abs/2506.21129)
*Deepak Kumar Panda,Adolfo Perrusquia,Weisi Guo*

Main category: cs.LG

TL;DR: 提出了一种抗脆弱的强化学习框架，通过逐步增加的对抗扰动训练RL代理，使其适应和泛化到更广泛的OOD观测，并在无人机导航场景中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键系统中，RL策略易受观测空间中的OOD对抗攻击影响，导致决策不安全或次优。为解决这一问题，研究提出了抗脆弱框架。

Method: 通过模拟攻击者逐步增加观测空间扰动，结合理论分析，使用Wasserstein距离最小化进行专家引导的批评对齐，确保价值函数分布的有界性。

Result: 在无人机避障场景中，抗脆弱策略显著优于标准RL和鲁棒RL基线，累积奖励提高15%，冲突事件减少30%。

Conclusion: 抗脆弱强化学习在动态威胁环境中具有理论和实践可行性，能够提升决策的安全性和弹性。

Abstract: Reinforcement learning (RL) policies deployed in safety-critical systems,
such as unmanned aerial vehicle (UAV) navigation in dynamic airspace, are
vulnerable to out-ofdistribution (OOD) adversarial attacks in the observation
space. These attacks induce distributional shifts that significantly degrade
value estimation, leading to unsafe or suboptimal decision making rendering the
existing policy fragile. To address this vulnerability, we propose an
antifragile RL framework designed to adapt against curriculum of incremental
adversarial perturbations. The framework introduces a simulated attacker which
incrementally increases the strength of observation-space perturbations which
enables the RL agent to adapt and generalize across a wider range of OOD
observations and anticipate previously unseen attacks. We begin with a
theoretical characterization of fragility, formally defining catastrophic
forgetting as a monotonic divergence in value function distributions with
increasing perturbation strength. Building on this, we define antifragility as
the boundedness of such value shifts and derive adaptation conditions under
which forgetting is stabilized. Our method enforces these bounds through
iterative expert-guided critic alignment using Wasserstein distance
minimization across incrementally perturbed observations. We empirically
evaluate the approach in a UAV deconfliction scenario involving dynamic 3D
obstacles. Results show that the antifragile policy consistently outperforms
standard and robust RL baselines when subjected to both projected gradient
descent (PGD) and GPS spoofing attacks, achieving up to 15% higher cumulative
reward and over 30% fewer conflict events. These findings demonstrate the
practical and theoretical viability of antifragile reinforcement learning for
secure and resilient decision-making in environments with evolving threat
scenarios.

</details>


### [81] [NaLaFormer: Norm-Aware Linear Attention for Transformer Models](https://arxiv.org/abs/2506.21137)
*Weikang Meng,Yadan Luo,Liangyu Huo,Yaowei Wang,Xin Li,Zheng Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的Norm-Aware Linear Attention机制，通过解耦查询和键矩阵的范数与方向，恢复动态稀疏性和范数一致性，提升了线性注意力的性能。


<details>
  <summary>Details</summary>
Motivation: 现有线性注意力方法忽略了查询范数，导致熵差距，同时抑制负值限制了内积交互。

Method: 解耦查询和键矩阵为范数和方向两部分，设计查询范数感知的核函数，并使用范数保持映射确保非负性。

Result: NaLaFormer在视觉和语言任务中性能提升高达4.2%。

Conclusion: 提出的方法有效解决了线性注意力中的熵差距和范数一致性问题，提升了表达能力和效率。

Abstract: Linear attention has emerged as a viable alternative to softmax attention by
reducing complexity from quadratic to linear in sequence length. To preserve
two fundamental properties of softmax, non-negativity and entropy reduction,
current works employ various linearly separatable kernel functions with $L1$
normalization instead of softmax operator. However, query norms are neglected
by the normalization operation in linear attention, such degradation heavily
leads to an entropy gap. Meanwhile, existing works inhibit negative values of
query and key vectors resulting in a missing inner-product interactions after
being mapped. To address these dual challenges, we propose a novel Norm-Aware
Linear Attention mechanism serving to restore norm-guided dynamic spikiness and
recover kernel-perturbed norm distributions. Specifically, we first decouple
query and key matrices into two components: norm and direction, to achieve
norm-aware spikiness control and norm consistency, respectively. We
mathematically reveal that the extent of entropy reduction varies with the
query norm in softmax normalization, motivating a query-norm aware kernel
function for dynamic control over entropy reduction. Furthermore, to ensure
norm consistency and enforce non-negativity constraints, we employ a
norm-preserving mapping to project all elements of the angular matrix into
positive values, leveraging cosine similarity to inhibit dimensions with
opposite directions. We conduct extensive experiments demonstrating that the
NaLaFormer improves performance on vision and language tasks, enhancing both
expressiveness and efficiency by up to 4.2\%.

</details>


### [82] [DBConformer: Dual-Branch Convolutional Transformer for EEG Decoding](https://arxiv.org/abs/2506.21140)
*Ziwei Wang,Hongbin Wang,Tianwang Jia,Xingyi He,Siyang Li,Dongrui Wu*

Main category: cs.LG

TL;DR: DBConformer是一种双分支卷积Transformer网络，用于EEG解码，结合时间和空间特征，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有的CNN-Transformer混合模型在EEG解码中难以有效整合局部和全局特征，且缺乏显式的通道建模。

Method: 提出DBConformer，包含时间Conformer和空间Conformer分支，以及轻量级通道注意力模块。

Result: 在五个运动想象数据集和两个癫痫检测数据集上，DBConformer性能优于10个基线模型，参数更少。

Conclusion: DBConformer性能优越且特征可解释，适用于稳健和可解释的EEG解码。

Abstract: Electroencephalography (EEG)-based brain-computer interfaces (BCIs) transform
spontaneous/evoked neural activity into control commands for external
communication. While convolutional neural networks (CNNs) remain the mainstream
backbone for EEG decoding, their inherently short receptive field makes it
difficult to capture long-range temporal dependencies and global inter-channel
relationships. Recent CNN-Transformer (Conformers) hybrids partially address
this issue, but most adopt a serial design, resulting in suboptimal integration
of local and global features, and often overlook explicit channel-wise
modeling. To address these limitations, we propose DBConformer, a dual-branch
convolutional Transformer network tailored for EEG decoding. It integrates a
temporal Conformer to model long-range temporal dependencies and a spatial
Conformer to extract inter-channel interactions, capturing both temporal
dynamics and spatial patterns in EEG signals. A lightweight channel attention
module further refines spatial representations by assigning data-driven
importance to EEG channels. Extensive experiments on five motor imagery (MI)
datasets and two seizure detection datasets under three evaluation settings
demonstrate that DBConformer consistently outperforms 10 competitive baseline
models, with over eight times fewer parameters than the high-capacity EEG
Conformer baseline. Further, the visualization results confirm that the
features extracted by DBConformer are physiologically interpretable and aligned
with sensorimotor priors in MI. The superior performance and interpretability
of DBConformer make it reliable for robust and explainable EEG decoding. Code
is publicized at https://github.com/wzwvv/DBConformer.

</details>


### [83] [Generative Adversarial Evasion and Out-of-Distribution Detection for UAV Cyber-Attacks](https://arxiv.org/abs/2506.21142)
*Deepak Kumar Panda,Weisi Guo*

Main category: cs.LG

TL;DR: 本文提出了一种基于条件生成对抗网络（cGAN）的框架，用于生成能逃避入侵检测系统（IDS）的隐蔽对抗攻击，并通过条件变分自编码器（CVAE）检测这些攻击。


<details>
  <summary>Details</summary>
Motivation: 随着无人机在民用空域的广泛应用，传统异常检测方法难以识别新型威胁，且现有方法无法区分隐蔽对抗攻击与真实异常事件。

Method: 设计了一个基于cGAN的框架，生成隐蔽对抗攻击样本，并使用CVAE检测这些攻击。

Result: CVAE在检测隐蔽对抗攻击方面显著优于传统方法。

Conclusion: 强调了高级概率建模在增强IDS对抗生成模型攻击能力中的重要性。

Abstract: The growing integration of UAVs into civilian airspace underscores the need
for resilient and intelligent intrusion detection systems (IDS), as traditional
anomaly detection methods often fail to identify novel threats. A common
approach treats unfamiliar attacks as out-of-distribution (OOD) samples;
however, this leaves systems vulnerable when mitigation is inadequate.
Moreover, conventional OOD detectors struggle to distinguish stealthy
adversarial attacks from genuine OOD events. This paper introduces a
conditional generative adversarial network (cGAN)-based framework for crafting
stealthy adversarial attacks that evade IDS mechanisms. We first design a
robust multi-class IDS classifier trained on benign UAV telemetry and known
cyber-attacks, including Denial of Service (DoS), false data injection (FDI),
man-in-the-middle (MiTM), and replay attacks. Using this classifier, our cGAN
perturbs known attacks to generate adversarial samples that misclassify as
benign while retaining statistical resemblance to OOD distributions. These
adversarial samples are iteratively refined to achieve high stealth and success
rates. To detect such perturbations, we implement a conditional variational
autoencoder (CVAE), leveraging negative log-likelihood to separate adversarial
inputs from authentic OOD samples. Comparative evaluation shows that CVAE-based
regret scores significantly outperform traditional Mahalanobis distance-based
detectors in identifying stealthy adversarial threats. Our findings emphasize
the importance of advanced probabilistic modeling to strengthen IDS
capabilities against adaptive, generative-model-based cyber intrusions.

</details>


### [84] [Personalized Federated Learning via Dual-Prompt Optimization and Cross Fusion](https://arxiv.org/abs/2506.21144)
*Yuguang Zhang,Kuangpu Guo,Zhihe Lu,Yunbo Wang,Jian Liang*

Main category: cs.LG

TL;DR: 提出了一种基于双提示学习和交叉融合的个性化联邦学习框架pFedDC，通过全局和局部提示捕获共享知识和客户端特定语义，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在数据、计算和通信异质性上面临挑战，现有方法仅依赖文本提示且忽略联合标签-域分布偏移。

Method: 提出pFedDC框架，客户端维护全局和局部提示，并通过交叉融合模块自适应整合提示，生成个性化表示。

Result: 在九种异质性数据集上的实验表明，pFedDC优于现有方法。

Conclusion: pFedDC通过双提示学习和交叉融合有效解决了联邦学习中的异质性问题。

Abstract: Federated learning (FL) enables collaborative model training across
decentralized clients without sharing local data, but is challenged by
heterogeneity in data, computation, and communication. Pretrained
vision-language models (VLMs), with their strong generalization and lightweight
tuning via prompts, offer a promising solution. However, existing federated
prompt-learning methods rely only on text prompts and overlook joint
label-domain distribution shifts. In this paper, we propose a personalized FL
framework based on dual-prompt learning and cross fusion, termed pFedDC.
Specifically, each client maintains both global and local prompts across vision
and language modalities: global prompts capture common knowledge shared across
the federation, while local prompts encode client-specific semantics and domain
characteristics. Meanwhile, a cross-fusion module is designed to adaptively
integrate prompts from different levels, enabling the model to generate
personalized representations aligned with each client's unique data
distribution. Extensive experiments across nine datasets with various types of
heterogeneity show that pFedDC consistently outperforms state-of-the-art
methods.

</details>


### [85] [Linearity-based neural network compression](https://arxiv.org/abs/2506.21146)
*Silas Dobler,Florian Lemmerich*

Main category: cs.LG

TL;DR: 提出一种基于线性特性的神经网络压缩方法，通过合并线性行为神经元实现无损压缩，模型大小可减少至1/4。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络压缩方法主要通过测量重要性和冗余性减少参数，但已高度优化的解决方案仍有改进空间。

Method: 利用ReLU类激活函数中几乎总是激活的神经元表现线性的特性，提出合并后续层的线性压缩方法。

Result: 实验证明，该方法在多数测试模型中实现无损压缩至原始大小的1/4，且与基于重要性的剪枝方法兼容。

Conclusion: 该方法为神经网络压缩提供了新思路，可实现更小、更高效的模型。

Abstract: In neural network compression, most current methods reduce unnecessary
parameters by measuring importance and redundancy. To augment already highly
optimized existing solutions, we propose linearity-based compression as a novel
way to reduce weights in a neural network. It is based on the intuition that
with ReLU-like activation functions, neurons that are almost always activated
behave linearly, allowing for merging of subsequent layers. We introduce the
theory underlying this compression and evaluate our approach experimentally.
Our novel method achieves a lossless compression down to 1/4 of the original
model size in over the majority of tested models. Applying our method on
already importance-based pruned models shows very little interference between
different types of compression, demonstrating the option of successful
combination of techniques. Overall, our work lays the foundation for a new type
of compression method that enables smaller and ultimately more efficient neural
network models.

</details>


### [86] [Diverse Mini-Batch Selection in Reinforcement Learning for Efficient Chemical Exploration in de novo Drug Design](https://arxiv.org/abs/2506.21158)
*Hampus Gummesson Svensson,Ola Engkvist,Jon Paul Janet,Christian Tyrchan,Morteza Haghir Chehreghani*

Main category: cs.LG

TL;DR: 论文提出了一种用于强化学习的多样化小批量选择方法，使用行列式点过程来提高化学探索的多样性和质量。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，评估实例的质量通常成本高昂且耗时，尤其是在强化学习中，需要与环境交互以提供奖励信号。多样化探索对避免模式崩溃至关重要。

Method: 采用行列式点过程（DPP）进行多样化小批量选择，并在药物发现的实际问题中验证其有效性。

Result: 实验表明，该方法显著提高了解决方案的多样性，同时保持了高质量。

Conclusion: 多样化小批量选择框架在药物发现中具有潜力，可更快满足未满足的医疗需求。

Abstract: In many real-world applications, evaluating the goodness of instances is
often costly and time-consuming, e.g., human feedback and physics simulations,
in contrast to proposing new instances. In particular, this is even more
critical in reinforcement learning, as new interactions with the environment
(i.e., new instances) need to be evaluated to provide a reward signal to learn
from. As sufficient exploration is crucial, learning from a diverse mini-batch
can have a large impact and help mitigate mode collapse. In this paper, we
introduce diverse mini-batch selection for reinforcement learning and propose
to use determinantal point processes for this task. We study this framework in
the context of a real-world problem, namely drug discovery. We experimentally
study how our proposed framework can improve the effectiveness of chemical
exploration in de novo drug design, where finding diverse and high-quality
solutions is essential. We conduct a comprehensive evaluation with three
well-established molecular generation oracles over numerous generative steps.
Our experiments conclude that our diverse mini-batch selection framework can
substantially improve the diversity of the solutions, while still obtaining
solutions of high quality. In drug discovery, such outcome can potentially lead
to fulfilling unmet medication needs faster.

</details>


### [87] [Artificial Delegates Resolve Fairness Issues in Perpetual Voting with Partial Turnout](https://arxiv.org/abs/2506.21186)
*Apurva Shah,Axel Abels,Ann Nowé,Tom Lenaerts*

Main category: cs.LG

TL;DR: 研究探讨了在部分参与情况下，通过引入人工智能代表（Artificial Delegates）来提升持续性投票系统的公平性和代表性。


<details>
  <summary>Details</summary>
Motivation: 现有持续性投票规则假设完全参与和完整的偏好信息，但实践中部分参与是常态，导致公平性和代表性不足。

Method: 研究通过引入人工智能代表（偏好学习代理）来代表缺席选民，分析其对不同投票方法下公平性和代表性的影响。

Result: 研究发现缺席显著影响公平性，但人工智能代表能有效缓解这些影响，并在多种场景中提升系统的稳健性。

Conclusion: 人工智能代表是解决持续性投票系统中部分参与问题的有效方法，能显著提升公平性和代表性。

Abstract: Perpetual voting addresses fairness in sequential collective decision-making
by evaluating representational equity over time. However, existing perpetual
voting rules rely on full participation and complete approval information,
assumptions that rarely hold in practice, where partial turnout is the norm. In
this work, we study the integration of Artificial Delegates,
preference-learning agents trained to represent absent voters, into perpetual
voting systems. We examine how absenteeism affects fairness and
representativeness under various voting methods and evaluate the extent to
which Artificial Delegates can compensate for missing participation. Our
findings indicate that while absenteeism significantly affects fairness,
Artificial Delegates reliably mitigate these effects and enhance robustness
across diverse scenarios.

</details>


### [88] [Complexity-aware fine-tuning](https://arxiv.org/abs/2506.21220)
*Andrey Goncharov,Daniil Vyazhev,Petr Sychev,Edvard Khalafyan,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 提出了一种基于熵分类的高效微调方法，显著优于标准SFT，同时减少数据需求。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型在特定领域的性能，同时减少计算和数据的成本。

Method: 通过熵分类训练数据，结合SFT和蒸馏方法进行微调。

Result: 在减少62%数据的情况下，性能与蒸馏方法相当，优于标准SFT。

Conclusion: 该方法为高效微调提供了新思路，代码和数据已公开。

Abstract: General-purpose Large Language Models (LLMs) are frequently fine-tuned
through supervised fine-tuning (SFT) to enhance performance in specific
domains. Better results can be achieved by distilling the chain-of-thought of a
larger model at the cost of numerous expensive calls and a much greater amount
of data. We propose a novel blueprint for efficient fine-tuning that uses
reasoning only for complex data identified by entropy. Specifically, across two
small open models ($\approx 3B$) we split the training data into complexity
categories by a single token answer entropy (ROC AUC $0.73$), fine-tune large
language models (LLMs) via SFT and distillation, and show that our pipeline
significantly outperforms the standard SFT approach ($0.55$ vs $0.43$ average
accuracy) and provides comparable with distillation performance while using
$62\%$ less data ($0.55$ average accuracy for both). We publish our code and
data to facilitate further research in this direction.

</details>


### [89] [Zero-Shot Learning for Obsolescence Risk Forecasting](https://arxiv.org/abs/2506.21240)
*Elie Saad,Aya Mrabah,Mariem Besbes,Marc Zolghadri,Victor Czmil,Claude Baron,Vincent Bourgeois*

Main category: cs.LG

TL;DR: 提出了一种基于零样本学习和大语言模型的方法，用于预测电子元件过时风险，解决了数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 电子元件过时对依赖这些组件的行业造成高成本和系统可用性问题，但缺乏可靠数据阻碍了准确预测。

Method: 使用零样本学习（ZSL）结合大语言模型（LLMs），利用表格数据中的领域知识进行预测。

Result: 在两个真实数据集上验证了方法的有效性，并比较了四种LLMs的性能。

Conclusion: 选择合适的LLM对特定预测任务至关重要，该方法为解决数据不足问题提供了新思路。

Abstract: Component obsolescence poses significant challenges in industries reliant on
electronic components, causing increased costs and disruptions in the security
and availability of systems. Accurate obsolescence risk prediction is essential
but hindered by a lack of reliable data. This paper proposes a novel approach
to forecasting obsolescence risk using zero-shot learning (ZSL) with large
language models (LLMs) to address data limitations by leveraging
domain-specific knowledge from tabular datasets. Applied to two real-world
datasets, the method demonstrates effective risk prediction. A comparative
evaluation of four LLMs underscores the importance of selecting the right model
for specific forecasting tasks.

</details>


### [90] [DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster](https://arxiv.org/abs/2506.21263)
*Ji Qi,WenPeng Zhu,Li Li,Ming Wu,YingJun Wu,Wu He,Xun Gao,Jason Zeng,Michael Heinrich*

Main category: cs.LG

TL;DR: DiLoCoX是一个低通信的大规模去中心化集群训练框架，用于训练超过1000亿参数的基础模型，显著提升了训练速度和规模。


<details>
  <summary>Details</summary>
Motivation: 解决在慢速网络上训练大规模基础模型（如超过1000亿参数的LLM）的挑战，释放去中心化集群的潜力。

Method: 结合Pipeline Parallelism、Dual Optimizer Policy、One-Step-Delay Overlap of Communication and Local Training，以及Adaptive Gradient Compression Scheme。

Result: 在1Gbps网络上成功预训练107B基础模型，相比AllReduce实现357倍加速，模型收敛几乎无退化。

Conclusion: DiLoCoX是首个成功应用于超过1000亿参数模型的去中心化训练框架，具有显著的实际应用价值。

Abstract: The distributed training of foundation models, particularly large language
models (LLMs), demands a high level of communication. Consequently, it is
highly dependent on a centralized cluster with fast and reliable interconnects.
Can we conduct training on slow networks and thereby unleash the power of
decentralized clusters when dealing with models exceeding 100 billion
parameters? In this paper, we propose DiLoCoX, a low-communication large-scale
decentralized cluster training framework. It combines Pipeline Parallelism with
Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local
Training, and an Adaptive Gradient Compression Scheme. This combination
significantly improves the scale of parameters and the speed of model
pre-training. We justify the benefits of one-step-delay overlap of
communication and local training, as well as the adaptive gradient compression
scheme, through a theoretical analysis of convergence. Empirically, we
demonstrate that DiLoCoX is capable of pre-training a 107B foundation model
over a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x
speedup in distributed training while maintaining negligible degradation in
model convergence. To the best of our knowledge, this is the first
decentralized training framework successfully applied to models with over 100
billion parameters.

</details>


### [91] [Improved seeding strategies for k-means and k-GMM](https://arxiv.org/abs/2506.21291)
*Guillaume Carrière,Frédéric Cazals*

Main category: cs.LG

TL;DR: 论文重新审视了k-means聚类和k-GMM的随机种子技术，提出了基于前瞻原则和多轮策略的新初始化方法，实验显示性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 改进k-means和k-GMM的种子初始化方法，通过前瞻和多轮策略提升最终性能。

Method: 提出基于前瞻原则的种子选择和多轮策略，优化种子采样和选择。

Result: 实验表明新方法在SSE（k-means）和log-likelihood（k-GMM）上优于现有技术。

Conclusion: 新方法有望成为标准技术，并为理论分析开辟新方向。

Abstract: We revisit the randomized seeding techniques for k-means clustering and k-GMM
(Gaussian Mixture model fitting with Expectation-Maximization), formalizing
their three key ingredients: the metric used for seed sampling, the number of
candidate seeds, and the metric used for seed selection. This analysis yields
novel families of initialization methods exploiting a lookahead
principle--conditioning the seed selection to an enhanced coherence with the
final metric used to assess the algorithm, and a multipass strategy to tame
down the effect of randomization.
  Experiments show a consistent constant factor improvement over classical
contenders in terms of the final metric (SSE for k-means, log-likelihood for
k-GMM), at a modest overhead. In particular, for k-means, our methods improve
on the recently designed multi-swap strategy, which was the first one to
outperform the greedy k-means++ seeding.
  Our experimental analysis also shed light on subtle properties of k-means
often overlooked, including the (lack of) correlations between the SSE upon
seeding and the final SSE, the variance reduction phenomena observed in
iterative seeding methods, and the sensitivity of the final SSE to the pool
size for greedy methods.
  Practically, our most effective seeding methods are strong candidates to
become one of the--if not the--standard techniques. From a theoretical
perspective, our formalization of seeding opens the door to a new line of
analytical approaches.

</details>


### [92] [Latent Prototype Routing: Achieving Near-Perfect Load Balancing in Mixture-of-Experts](https://arxiv.org/abs/2506.21328)
*Jiajie Yang*

Main category: cs.LG

TL;DR: 提出了一种新的路由框架Latent Prototype Routing (LPR)，通过聚类视角解决MoE架构中的负载不平衡问题，显著提升专家利用率。


<details>
  <summary>Details</summary>
Motivation: 当前MoE系统中存在严重的负载不平衡问题，导致模型容量和计算资源利用不足。

Method: 采用聚类视角重新设计专家路由，提出LPR框架，在保持下游性能的同时促进负载均衡。

Result: 实验表明，LPR将专家负载的基尼系数从0.70降至0.035，最小-最大负载比从1e-6提升至0.70，实现近乎完美的负载均衡。

Conclusion: LPR是一种高效的路由框架，能显著改善MoE系统的负载均衡问题。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a key strategy for
scaling large language models (LLMs) efficiently. However, current MoE systems
suffer from severe load imbalance, where only a small subset of experts is
consistently activated during training and inference, leading to significant
underutilization of model capacity and computational resources. In this work,
we revisit expert routing through a clustering perspective and propose Latent
Prototype Routing (LPR), a novel routing framework that generalizes existing
approaches while promoting balanced expert utilization without compromising
downstream performance. Extensive experiments across multiple open-source MoE
models -- including DeepSeek-V3, Qwen3-MoE, and Mixtral -- demonstrate that LPR
reduces the Gini coefficient of expert load from 0.70 to 0.035 on average,
improves the min-max expert load ratio from 1e-6 to 0.70, achieving
near-perfect load balancing.

</details>


### [93] [AGTCNet: A Graph-Temporal Approach for Principled Motor Imagery EEG Classification](https://arxiv.org/abs/2506.21338)
*Galvin Brice S. Lim,Brian Godwin S. Lim,Argel A. Bandala,John Anthony C. Jose,Timothy Scott C. Chu,Edwin Sybingco*

Main category: cs.LG

TL;DR: 论文提出了一种新颖的图-时序卷积网络（AGTCNet），用于运动想象脑电图（MI-EEG）分类，显著提升了分类性能并减少了模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有脑机接口（BCI）系统难以捕捉多通道脑电图信号的复杂时空依赖性，且受限于个体间和会话间的变异性。

Method: AGTCNet利用脑电图电极的拓扑结构作为归纳偏置，结合图卷积注意力网络（GCAT）共同学习时空特征。

Result: AGTCNet在多个数据集上实现了最先进的性能，模型尺寸减少49.87%，推理时间缩短64.65%，分类准确率显著提升。

Conclusion: AGTCNet在运动想象脑电图分类中表现出高效性和实用性，为BCI系统的实际部署提供了有力支持。

Abstract: Brain-computer interface (BCI) technology utilizing electroencephalography
(EEG) marks a transformative innovation, empowering motor-impaired individuals
to engage with their environment on equal footing. Despite its promising
potential, developing subject-invariant and session-invariant BCI systems
remains a significant challenge due to the inherent complexity and variability
of neural activity across individuals and over time, compounded by EEG hardware
constraints. While prior studies have sought to develop robust BCI systems,
existing approaches remain ineffective in capturing the intricate
spatiotemporal dependencies within multichannel EEG signals. This study
addresses this gap by introducing the attentive graph-temporal convolutional
network (AGTCNet), a novel graph-temporal model for motor imagery EEG (MI-EEG)
classification. Specifically, AGTCNet leverages the topographic configuration
of EEG electrodes as an inductive bias and integrates graph convolutional
attention network (GCAT) to jointly learn expressive spatiotemporal EEG
representations. The proposed model significantly outperformed existing MI-EEG
classifiers, achieving state-of-the-art performance while utilizing a compact
architecture, underscoring its effectiveness and practicality for BCI
deployment. With a 49.87% reduction in model size, 64.65% faster inference
time, and shorter input EEG signal, AGTCNet achieved a moving average accuracy
of 66.82% for subject-independent classification on the BCI Competition IV
Dataset 2a, which further improved to 82.88% when fine-tuned for
subject-specific classification. On the EEG Motor Movement/Imagery Dataset,
AGTCNet achieved moving average accuracies of 64.14% and 85.22% for 4-class and
2-class subject-independent classifications, respectively, with further
improvements to 72.13% and 90.54% for subject-specific classifications.

</details>


### [94] [DynamicBench: Evaluating Real-Time Report Generation in Large Language Models](https://arxiv.org/abs/2506.21343)
*Jingyao Li,Hao Sun,Zile Qiao,Yong Jiang,Pengjun Xie,Fei Huang,Hong Xu,Jiaya Jia*

Main category: cs.LG

TL;DR: DynamicBench是一个新基准，用于评估大语言模型（LLMs）实时处理最新数据的能力，通过双路径检索管道和领域知识验证，在无文档和有文档场景下均优于GPT4o。


<details>
  <summary>Details</summary>
Motivation: 传统基准测试无法满足实时信息处理的需求，DynamicBench旨在填补这一空白。

Method: 采用双路径检索管道（结合网络搜索和本地报告数据库），并引入高级报告生成系统。

Result: 在无文档和有文档场景下，分别超越GPT4o 7.0%和5.8%。

Conclusion: DynamicBench有效评估LLMs的实时信息处理能力，代码和数据将公开。

Abstract: Traditional benchmarks for large language models (LLMs) typically rely on
static evaluations through storytelling or opinion expression, which fail to
capture the dynamic requirements of real-time information processing in
contemporary applications. To address this limitation, we present DynamicBench,
a benchmark designed to evaluate the proficiency of LLMs in storing and
processing up-to-the-minute data. DynamicBench utilizes a dual-path retrieval
pipeline, integrating web searches with local report databases. It necessitates
domain-specific knowledge, ensuring accurate responses report generation within
specialized fields. By evaluating models in scenarios that either provide or
withhold external documents, DynamicBench effectively measures their capability
to independently process recent information or leverage contextual
enhancements. Additionally, we introduce an advanced report generation system
adept at managing dynamic information synthesis. Our experimental results
confirm the efficacy of our approach, with our method achieving
state-of-the-art performance, surpassing GPT4o in document-free and
document-assisted scenarios by 7.0% and 5.8%, respectively. The code and data
will be made publicly available.

</details>


### [95] [Lipschitz Bounds for Persistent Laplacian Eigenvalues under One-Simplex Insertions](https://arxiv.org/abs/2506.21352)
*Le Vu Anh,Mehmet Dik,Nguyen Viet Anh*

Main category: cs.LG

TL;DR: 论文证明了持久拉普拉斯算子的特征值在添加一个单纯形时的变化上限，为谱拓扑数据分析提供了首个特征值级别的鲁棒性保证。


<details>
  <summary>Details</summary>
Motivation: 持久拉普拉斯算子的特征值是描述数据几何和拓扑特征的重要工具，但其在局部更新时的变化尚未明确，影响了下游应用的可靠性。

Method: 通过数学证明，提出了一个统一的Lipschitz界，表明添加一个单纯形后，每个上持久拉普拉斯特征值的变化不超过该单纯形边界欧几里得范数的两倍。

Result: 证明了特征值在局部更新时的稳定性，为动态数据环境中的误差控制提供了理论支持。

Conclusion: 该研究填补了持久拉普拉斯算子特征值稳定性理论的空白，为谱拓扑数据分析的可靠性提供了保障。

Abstract: Persistent Laplacians are matrix operators that track how the shape and
structure of data transform across scales and are popularly adopted in biology,
physics, and machine learning. Their eigenvalues are concise descriptors of
geometric and topological features in a filtration. Although earlier work
established global algebraic stability for these operators, the precise change
in a single eigenvalue when one simplex, such as a vertex, edge, or triangle,
is added has remained unknown. This is important because downstream tools,
including heat-kernel signatures and spectral neural networks, depend directly
on these eigenvalues. We close this gap by proving a uniform Lipschitz bound:
after inserting one simplex, every up-persistent Laplacian eigenvalue can vary
by at most twice the Euclidean norm of that simplex's boundary, independent of
filtration scale and complex size. This result delivers the first
eigenvalue-level robustness guarantee for spectral topological data analysis.
It guarantees that spectral features remain stable under local updates and
enables reliable error control in dynamic data settings.

</details>


### [96] [SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning](https://arxiv.org/abs/2506.21355)
*Melanie Rieff,Maya Varma,Ossian Rabow,Subathra Adithan,Julie Kim,Ken Chang,Hannah Lee,Nidhi Rohatgi,Christian Bluethgen,Mohamed S. Muneer,Jean-Benoit Delbrouck,Michael Moor*

Main category: cs.LG

TL;DR: 论文提出了SMMILE，首个专家驱动的多模态ICL基准测试，用于评估MLLMs在医学任务中的上下文学习能力，发现当前模型表现不佳且易受无关示例干扰。


<details>
  <summary>Details</summary>
Motivation: 探索多模态上下文学习（ICL）在医学领域的潜力，填补现有研究空白，解决临床任务中从有限示例中学习的需求。

Method: 引入SMMILE基准测试，包含111个问题（517个问答图像三元组）和增强版SMMILE++（1038个问题），评估15种MLLMs的表现。

Result: 大多数模型在医学任务中表现中等或较差，ICL仅带来8%-9.4%的性能提升，且易受无关示例和示例顺序影响。

Conclusion: 当前MLLMs在多模态医学任务中的上下文学习能力有限，需进一步优化以应对噪声和顺序偏差。

Abstract: Multimodal in-context learning (ICL) remains underexplored despite
significant potential for domains such as medicine. Clinicians routinely
encounter diverse, specialized tasks requiring adaptation from limited
examples, such as drawing insights from a few relevant prior cases or
considering a constrained set of differential diagnoses. While multimodal large
language models (MLLMs) have shown advances in medical visual question
answering (VQA), their ability to learn multimodal tasks from context is
largely unknown. We introduce SMMILE, the first expert-driven multimodal ICL
benchmark for medical tasks. Eleven medical experts curated problems, each
including a multimodal query and multimodal in-context examples as task
demonstrations. SMMILE encompasses 111 problems (517 question-image-answer
triplets) covering 6 medical specialties and 13 imaging modalities. We further
introduce SMMILE++, an augmented variant with 1038 permuted problems. A
comprehensive evaluation of 15 MLLMs demonstrates that most models exhibit
moderate to poor multimodal ICL ability in medical tasks. In open-ended
evaluations, ICL contributes only 8% average improvement over zero-shot on
SMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant
in-context examples: even a single noisy or irrelevant example can degrade
performance by up to 9.5%. Moreover, example ordering exhibits a recency bias,
i.e., placing the most relevant example last can lead to substantial
performance improvements by up to 71%. Our findings highlight critical
limitations and biases in current MLLMs when learning multimodal medical tasks
from context.

</details>


### [97] [rQdia: Regularizing Q-Value Distributions With Image Augmentation](https://arxiv.org/abs/2506.21367)
*Sam Lerman,Jing Bi*

Main category: cs.LG

TL;DR: rQdia通过增强图像正则化Q值分布，提升像素深度强化学习性能，显著提高了多个任务的表现。


<details>
  <summary>Details</summary>
Motivation: 解决像素深度强化学习中Q值分布的不稳定性问题，提升模型性能。

Method: 使用辅助损失函数（MSE）均衡Q值分布，结合增强图像进行正则化。

Result: 在MuJoCo和Atari任务中显著提升性能，样本效率和长期训练表现均有改善。

Conclusion: rQdia成功将无模型连续控制从像素提升至超越状态编码基线的水平。

Abstract: rQdia regularizes Q-value distributions with augmented images in pixel-based
deep reinforcement learning. With a simple auxiliary loss, that equalizes these
distributions via MSE, rQdia boosts DrQ and SAC on 9/12 and 10/12 tasks
respectively in the MuJoCo Continuous Control Suite from pixels, and
Data-Efficient Rainbow on 18/26 Atari Arcade environments. Gains are measured
in both sample efficiency and longer-term training. Moreover, the addition of
rQdia finally propels model-free continuous control from pixels over the state
encoding baseline.

</details>


### [98] [MAx-DNN: Multi-Level Arithmetic Approximation for Energy-Efficient DNN Hardware Accelerators](https://arxiv.org/abs/2506.21371)
*Vasileios Leon,Georgios Makris,Sotirios Xydis,Kiamal Pekmestzi,Dimitrios Soudris*

Main category: cs.LG

TL;DR: 论文探讨了在低功耗DNN计算中，通过硬件近似技术与DNN工作负载的细粒度容错性结合，以提高能效。使用ROUP近似乘法器，在ResNet-8模型上实现了54%的能效提升，仅损失4%的准确率。


<details>
  <summary>Details</summary>
Motivation: 针对低功耗DNN计算的需求，研究如何通过硬件近似技术与DNN的容错性结合，实现更高的能效。

Method: 采用ROUP近似乘法器，通过层、滤波器和内核级别的细粒度分布策略，评估其对准确性和能效的影响。

Result: 在CIFAR-10数据集上，与基线量化模型相比，能效提升54%，准确率损失仅4%；与现有DNN近似方法相比，能效提升2倍且准确性更高。

Conclusion: 通过硬件近似技术与DNN容错性的结合，可以有效提升能效，同时保持较高的准确性。

Abstract: Nowadays, the rapid growth of Deep Neural Network (DNN) architectures has
established them as the defacto approach for providing advanced Machine
Learning tasks with excellent accuracy. Targeting low-power DNN computing, this
paper examines the interplay of fine-grained error resilience of DNN workloads
in collaboration with hardware approximation techniques, to achieve higher
levels of energy efficiency. Utilizing the state-of-the-art ROUP approximate
multipliers, we systematically explore their fine-grained distribution across
the network according to our layer-, filter-, and kernel-level approaches, and
examine their impact on accuracy and energy. We use the ResNet-8 model on the
CIFAR-10 dataset to evaluate our approximations. The proposed solution delivers
up to 54% energy gains in exchange for up to 4% accuracy loss, compared to the
baseline quantized model, while it provides 2x energy gains with better
accuracy versus the state-of-the-art DNN approximations.

</details>


### [99] [Pay Attention to Small Weights](https://arxiv.org/abs/2506.21374)
*Chao Zhou,Tom Jacobs,Advait Gadhikar,Rebekka Burkholz*

Main category: cs.LG

TL;DR: 论文提出NANOADAM方法，通过动态更新小幅度权重来优化微调过程，减少资源消耗并提升性能。


<details>
  <summary>Details</summary>
Motivation: 微调大型预训练神经网络资源消耗大，通过分析梯度与权重关系，发现大梯度常与小幅度权重相关，提出针对性优化方法。

Method: 提出NANOADAM方法，动态更新小幅度权重，无需梯度计算，保留大幅度权重以减少灾难性遗忘，支持更大学习率。

Result: 在NLP和视觉任务中验证了方法的有效性，提升了泛化性能。

Conclusion: NANOADAM通过选择性更新权重，显著降低微调资源需求，同时保持或提升模型性能。

Abstract: Finetuning large pretrained neural networks is known to be
resource-intensive, both in terms of memory and computational cost. To mitigate
this, a common approach is to restrict training to a subset of the model
parameters. By analyzing the relationship between gradients and weights during
finetuning, we observe a notable pattern: large gradients are often associated
with small-magnitude weights. This correlation is more pronounced in finetuning
settings than in training from scratch. Motivated by this observation, we
propose NANOADAM, which dynamically updates only the small-magnitude weights
during finetuning and offers several practical advantages: first, this
criterion is gradient-free -- the parameter subset can be determined without
gradient computation; second, it preserves large-magnitude weights, which are
likely to encode critical features learned during pretraining, thereby reducing
the risk of catastrophic forgetting; thirdly, it permits the use of larger
learning rates and consistently leads to better generalization performance in
experiments. We demonstrate this for both NLP and vision tasks.

</details>


### [100] [Temporal-Aware Graph Attention Network for Cryptocurrency Transaction Fraud Detection](https://arxiv.org/abs/2506.21382)
*Zhi Zheng,Bochuan Zhou,Yuping Song*

Main category: cs.LG

TL;DR: 论文提出了一种增强型时序感知图注意力网络（ATGAT），用于解决加密货币交易欺诈检测中的复杂交易模式和类别不平衡问题，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 加密货币交易欺诈检测面临复杂交易模式和严重类别不平衡的双重挑战，传统方法难以捕捉交易网络的时序和结构依赖关系。

Method: ATGAT通过三个模块提升性能：(1) 多尺度时间差特征与周期性位置编码融合的时序嵌入模块；(2) 结构、时序和全局上下文注意力联合优化的时序感知三重注意力机制；(3) 加权BCE损失解决类别不平衡。

Result: 在Elliptic++数据集上，ATGAT的AUC达到0.9130，比XGBoost提升9.2%，比GCN提升12.0%，比标准GAT提升10.0%。

Conclusion: ATGAT验证了时序感知和多重注意力机制对图神经网络的增强效果，为金融机构提供了更可靠的欺诈检测工具，其设计原则可推广至其他时序图异常检测任务。

Abstract: Cryptocurrency transaction fraud detection faces the dual challenges of
increasingly complex transaction patterns and severe class imbalance.
Traditional methods rely on manual feature engineering and struggle to capture
temporal and structural dependencies in transaction networks. This paper
proposes an Augmented Temporal-aware Graph Attention Network (ATGAT) that
enhances detection performance through three modules: (1) designing an advanced
temporal embedding module that fuses multi-scale time difference features with
periodic position encoding; (2) constructing a temporal-aware triple attention
mechanism that jointly optimizes structural, temporal, and global context
attention; (3) employing weighted BCE loss to address class imbalance.
Experiments on the Elliptic++ cryptocurrency dataset demonstrate that ATGAT
achieves an AUC of 0.9130, representing a 9.2% improvement over the best
traditional method XGBoost, 12.0% over GCN, and 10.0% over standard GAT. This
method not only validates the enhancement effect of temporal awareness and
triple attention mechanisms on graph neural networks, but also provides
financial institutions with more reliable fraud detection tools, with its
design principles generalizable to other temporal graph anomaly detection
tasks.

</details>


### [101] [Early Stopping Tabular In-Context Learning](https://arxiv.org/abs/2506.21387)
*Jaris Küken,Lennart Purucker,Frank Hutter*

Main category: cs.LG

TL;DR: 论文提出了一种动态提前停止上下文学习的方法，以降低推理成本，同时保持预测性能。


<details>
  <summary>Details</summary>
Motivation: 表格基础模型在上下文学习中表现优异，但推理成本高，尤其是大数据集。

Method: 动态评估是否在每层Transformer编码器后停止上下文学习，并使用预训练的逐层解码器解码嵌入。

Result: 在34个小分类任务中，推理速度提升1.3倍，性能几乎无损；在5个大分类任务中，速度提升2.2倍。

Conclusion: 提前停止是一种有效且实用的策略，可提升表格上下文学习的效率。

Abstract: Tabular foundation models have shown strong performance across various
tabular learning tasks via in-context learning, offering robust generalization
without any downstream finetuning. However, their inference-time costs remain
high, particularly for larger datasets. To address this, we propose
early-stopping the in-context learning process. We achieve this by dynamically
evaluating whether to stop in-context learning after each Transformer encoder
layer. Once stopped, we decode the embedding using a pre-trained layer-wise
decoder. Experiments across 34 small classification tasks size show that early
stopping in-context learning accelerates inference by up to x1.3 with
negligible degradation in predictive performance. To assess scalability, we
further evaluate our method on five larger classification tasks, achieving
speedups of up to x2.2. Our results demonstrate the potential of early exiting
as an effective and practical strategy for improving the efficiency of tabular
in-context learning.

</details>


### [102] [Scalable Bayesian Low-Rank Adaptation of Large Language Models via Stochastic Variational Subspace Inference](https://arxiv.org/abs/2506.21408)
*Colin Samplawski,Adam D. Cobb,Manoj Acharya,Ramneet Kaur,Susmit Jha*

Main category: cs.LG

TL;DR: ScalaBL提出了一种可扩展的贝叶斯低秩适应方法，通过随机变分子空间推理，解决了大语言模型不确定性量化的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）存在幻觉和校准不足的问题，在高风险领域（如自主性和医疗）中，量化其不确定性至关重要。

Method: 通过LoRA参数的贝叶斯推理，在低维子空间中学习，并将其映射到LLM的全权重空间，使用随机变分推断优化所有参数。

Result: 尽管子空间维度低，ScalaBL性能与最先进方法相当，仅需约1000额外参数，并成功扩展到迄今最大的贝叶斯LLM。

Conclusion: ScalaBL提供了一种高效且可扩展的方法，用于大语言模型的不确定性量化。

Abstract: Despite their widespread use, large language models (LLMs) are known to
hallucinate incorrect information and be poorly calibrated. This makes the
uncertainty quantification of these models of critical importance, especially
in high-stakes domains, such as autonomy and healthcare. Prior work has made
Bayesian deep learning-based approaches to this problem more tractable by
performing inference over the low-rank adaptation (LoRA) parameters of a
fine-tuned model. While effective, these approaches struggle to scale to larger
LLMs due to requiring further additional parameters compared to LoRA. In this
work we present $\textbf{Scala}$ble $\textbf{B}$ayesian $\textbf{L}$ow-Rank
Adaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform
Bayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By
repurposing the LoRA parameters as projection matrices, we are able to map
samples from this subspace into the full weight space of the LLM. This allows
us to learn all the parameters of our approach using stochastic variational
inference. Despite the low dimensionality of our subspace, we are able to
achieve competitive performance with state-of-the-art approaches while only
requiring ${\sim}1000$ additional parameters. Furthermore, it allows us to
scale up to the largest Bayesian LLM to date, with four times as a many base
parameters as prior work.

</details>


### [103] [Distributed Cross-Channel Hierarchical Aggregation for Foundation Models](https://arxiv.org/abs/2506.21411)
*Aristeidis Tsaris,Isaac Lyngaas,John Lagregren,Mohamed Wahib,Larry York,Prasanna Balaprakash,Dan Lu,Feiyi Wang,Xiao Wang*

Main category: cs.LG

TL;DR: 提出了一种分布式跨通道分层聚合（D-CHAG）方法，用于处理多通道图像数据，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型在科学发现中潜力巨大，但图像标记和聚合计算密集，现有分布式方法未能完全解决。

Method: D-CHAG方法适用于任何模型并行策略和视觉Transformer架构，结合张量并行和模型分片。

Result: 在超光谱成像和天气预报任务中，内存使用减少75%，吞吐量提高一倍以上。

Conclusion: D-CHAG显著提升了多通道图像数据的处理效率，适用于大规模计算。

Abstract: Vision-based scientific foundation models hold significant promise for
advancing scientific discovery and innovation. This potential stems from their
ability to aggregate images from diverse sources such as varying physical
groundings or data acquisition systems and to learn spatio-temporal
correlations using transformer architectures. However, tokenizing and
aggregating images can be compute-intensive, a challenge not fully addressed by
current distributed methods. In this work, we introduce the Distributed
Cross-Channel Hierarchical Aggregation (D-CHAG) approach designed for datasets
with a large number of channels across image modalities. Our method is
compatible with any model-parallel strategy and any type of vision transformer
architecture, significantly improving computational efficiency. We evaluated
D-CHAG on hyperspectral imaging and weather forecasting tasks. When integrated
with tensor parallelism and model sharding, our approach achieved up to a 75%
reduction in memory usage and more than doubled sustained throughput on up to
1,024 AMD GPUs on the Frontier Supercomputer.

</details>


### [104] [Flow-Based Single-Step Completion for Efficient and Expressive Policy Learning](https://arxiv.org/abs/2506.21427)
*Prajwal Koirala,Cody Fleming*

Main category: cs.LG

TL;DR: 提出了一种名为SSCP的单步完成策略，通过增强的流匹配目标训练生成策略，实现高效的单步动作生成，解决了扩散和流匹配模型在离线强化学习中的高推理成本和训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 扩散和流匹配模型在离线强化学习中能够捕捉丰富的多模态动作分布，但其迭代采样导致高推理成本和训练不稳定。

Method: 提出SSCP，通过增强的流匹配目标训练生成策略，直接从中间流样本预测完成向量，实现单步动作生成。结合离线策略的actor-critic框架，兼顾生成模型的表达能力和单模态策略的效率。

Result: SSCP在离线、离线到在线和在线强化学习场景中均表现出色，速度和适应性优于基于扩散的基线方法，并在目标条件强化学习中扩展应用。

Conclusion: SSCP是一种高效、表达力强且通用的深度强化学习框架，适用于序列决策任务。

Abstract: Generative models such as diffusion and flow-matching offer expressive
policies for offline reinforcement learning (RL) by capturing rich, multimodal
action distributions, but their iterative sampling introduces high inference
costs and training instability due to gradient propagation across sampling
steps. We propose the \textit{Single-Step Completion Policy} (SSCP), a
generative policy trained with an augmented flow-matching objective to predict
direct completion vectors from intermediate flow samples, enabling accurate,
one-shot action generation. In an off-policy actor-critic framework, SSCP
combines the expressiveness of generative models with the training and
inference efficiency of unimodal policies, without requiring long
backpropagation chains. Our method scales effectively to offline,
offline-to-online, and online RL settings, offering substantial gains in speed
and adaptability over diffusion-based baselines. We further extend SSCP to
goal-conditioned RL, enabling flat policies to exploit subgoal structures
without explicit hierarchical inference. SSCP achieves strong results across
standard offline RL and behavior cloning benchmarks, positioning it as a
versatile, expressive, and efficient framework for deep RL and sequential
decision-making.

</details>


### [105] [Deception Detection in Dyadic Exchanges Using Multimodal Machine Learning: A Study on a Swedish Cohort](https://arxiv.org/abs/2506.21429)
*Franco Rugolon,Thomas Jack Samuels,Stephan Hau,Lennart Högman*

Main category: cs.LG

TL;DR: 多模态机器学习技术用于检测双向互动中的欺骗行为，结合语音和面部信息效果最佳，准确率达71%。


<details>
  <summary>Details</summary>
Motivation: 研究多模态数据（语音和面部信息）在欺骗检测中的效果，探索双向互动中欺骗行为的特征。

Method: 采用早期和晚期融合策略，结合音频和视频数据（动作单元和视线信息），分析不同模态和参与者的组合。

Result: 多模态数据结合双向参与者信息显著提升检测准确率，晚期融合策略表现最佳（71%）。

Conclusion: 研究支持心理学理论，为未来双向互动（如心理治疗）的欺骗检测奠定基础。

Abstract: This study investigates the efficacy of using multimodal machine learning
techniques to detect deception in dyadic interactions, focusing on the
integration of data from both the deceiver and the deceived. We compare early
and late fusion approaches, utilizing audio and video data - specifically,
Action Units and gaze information - across all possible combinations of
modalities and participants. Our dataset, newly collected from Swedish native
speakers engaged in truth or lie scenarios on emotionally relevant topics,
serves as the basis for our analysis. The results demonstrate that
incorporating both speech and facial information yields superior performance
compared to single-modality approaches. Moreover, including data from both
participants significantly enhances deception detection accuracy, with the best
performance (71%) achieved using a late fusion strategy applied to both
modalities and participants. These findings align with psychological theories
suggesting differential control of facial and vocal expressions during initial
interactions. As the first study of its kind on a Scandinavian cohort, this
research lays the groundwork for future investigations into dyadic
interactions, particularly within psychotherapy settings.

</details>


### [106] [Towards an Optimal Control Perspective of ResNet Training](https://arxiv.org/abs/2506.21453)
*Jens Püttschneider,Simon Heilig,Asja Fischer,Timm Faulwasser*

Main category: cs.LG

TL;DR: 提出了一种基于最优控制问题的ResNet训练方法，通过惩罚隐藏状态的中间输出来优化网络权重，并展示了该方法可以自动剪枝不必要的深层残差层。


<details>
  <summary>Details</summary>
Motivation: 将最优控制理论与ResNet训练结合，为网络优化和剪枝提供理论支持。

Method: 通过惩罚隐藏状态的中间输出（类似最优控制中的阶段成本），利用跳跃连接和输出层传播状态，优化网络权重。

Result: 训练动态使得不必要的深层残差层权重趋于零，表明该方法可用于理论驱动的层剪枝。

Conclusion: 该方法为ResNet训练和剪枝提供了新的理论框架，具有潜在的应用价值。

Abstract: We propose a training formulation for ResNets reflecting an optimal control
problem that is applicable for standard architectures and general loss
functions. We suggest bridging both worlds via penalizing intermediate outputs
of hidden states corresponding to stage cost terms in optimal control. For
standard ResNets, we obtain intermediate outputs by propagating the state
through the subsequent skip connections and the output layer. We demonstrate
that our training dynamic biases the weights of the unnecessary deeper residual
layers to vanish. This indicates the potential for a theory-grounded layer
pruning strategy.

</details>


### [107] [A Keyword-Based Technique to Evaluate Broad Question Answer Script](https://arxiv.org/abs/2506.21461)
*Tamim Al Mahmud,Md Gulzar Hussain,Sumaiya Kabir,Hasnain Ahmad,Mahmudus Sobhan*

Main category: cs.LG

TL;DR: 论文提出了一种电子评估主观答题脚本的高效方法，通过关键词匹配和语法检查实现自动化评分。


<details>
  <summary>Details</summary>
Motivation: 传统主观答题评估效率低且易受主观影响，需一种自动化解决方案。

Method: 集成系统通过提取答题脚本关键词并与开放/封闭领域关键词对比，同时检查语法和拼写错误。

Result: 系统测试100份答题脚本，精确度达0.91。

Conclusion: 该方法高效且准确，适用于主观答题的自动化评估。

Abstract: Evaluation is the method of assessing and determining the educational system
through various techniques such as verbal or viva-voice test, subjective or
objective written test. This paper presents an efficient solution to evaluate
the subjective answer script electronically. In this paper, we proposed and
implemented an integrated system that examines and evaluates the written answer
script. This article focuses on finding the keywords from the answer script and
then compares them with the keywords that have been parsed from both open and
closed domain. The system also checks the grammatical and spelling errors in
the answer script. Our proposed system tested with answer scripts of 100
students and gives precision score 0.91.

</details>


### [108] [Optimising 4th-Order Runge-Kutta Methods: A Dynamic Heuristic Approach for Efficiency and Low Storage](https://arxiv.org/abs/2506.21465)
*Gavin Lee Goodship,Luis Miralles-Pechuan,Stephen O'Sullivan*

Main category: cs.LG

TL;DR: 本文提出了一种结合遗传算法（GA）和强化学习（RL）的混合方法，用于优化低存储扩展稳定性Runge-Kutta（ESRK）方法，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决大规模科学和工程计算问题中高精度、稳定性和计算效率的平衡问题，特别是针对低存储高阶ESRK方法。

Method: 采用GA驱动的突变进行搜索空间探索，并结合RL启发的状态转移机制动态优化启发式选择。

Result: 在基准测试中，最佳启发式方法实现了25%的IPOPT运行时减少，同时保持数值稳定性和精度。

Conclusion: 该研究为数值方法的启发式优化提供了新范式，拓宽了低存储Runge-Kutta方法在实际应用中的潜力。

Abstract: Extended Stability Runge-Kutta (ESRK) methods are crucial for solving
large-scale computational problems in science and engineering, including
weather forecasting, aerodynamic analysis, and complex biological modelling.
However, balancing accuracy, stability, and computational efficiency remains
challenging, particularly for high-order, low-storage schemes. This study
introduces a hybrid Genetic Algorithm (GA) and Reinforcement Learning (RL)
approach for automated heuristic discovery, optimising low-storage ESRK
methods. Unlike traditional approaches that rely on manually designed
heuristics or exhaustive numerical searches, our method leverages GA-driven
mutations for search-space exploration and an RL-inspired state transition
mechanism to refine heuristic selection dynamically. This enables systematic
parameter reduction, preserving fourth-order accuracy while significantly
improving computational efficiency.The proposed GA-RL heuristic optimisation
framework is validated through rigorous testing on benchmark problems,
including the 1D and 2D Brusselator systems and the steady-state Navier-Stokes
equations. The best-performing heuristic achieves a 25\% reduction in IPOPT
runtime compared to traditional ESRK optimisation processes while maintaining
numerical stability and accuracy. These findings demonstrate the potential of
adaptive heuristic discovery to improve resource efficiency in high-fidelity
simulations and broaden the applicability of low-storage Runge-Kutta methods in
real-world computational fluid dynamics, physics simulations, and other
demanding fields. This work establishes a new paradigm in heuristic
optimisation for numerical methods, opening pathways for further exploration
using Deep RL and AutoML-based heuristic search

</details>


### [109] [Devising a solution to the problems of Cancer awareness in Telangana](https://arxiv.org/abs/2506.21500)
*Priyanka Avhad,Vedanti Kshirsagar,Urvi Ranjan,Mahek Nakhua*

Main category: cs.LG

TL;DR: 论文提出了一种基于机器学习的分类模型，用于预测乳腺癌和宫颈癌的易感性，并结合地理位置提供医疗服务建议，旨在提高癌症筛查意识和降低死亡率。


<details>
  <summary>Details</summary>
Motivation: 由于Telangana地区女性对乳腺癌和宫颈癌的筛查率极低（2020年分别为0.3%和3.3%），且公众对癌症的认知不足，作者希望通过技术手段提高癌症意识和筛查率。

Method: 使用决策树分类和支持向量机分类算法分别预测宫颈癌和乳腺癌的易感性，并开发系统提供就近医院建议和健康记录管理。

Result: 通过模型和系统设计，为癌症筛查和医疗服务提供了技术支持，有望提高癌症意识和筛查率。

Conclusion: 该解决方案有助于提升Telangana地区的癌症认知水平，降低死亡率，并推动癌症筛查的普及。

Abstract: According to the data, the percent of women who underwent screening for
cervical cancer, breast and oral cancer in Telangana in the year 2020 was 3.3
percent, 0.3 percent and 2.3 percent respectively. Although early detection is
the only way to reduce morbidity and mortality, people have very low awareness
about cervical and breast cancer signs and symptoms and screening practices. We
developed an ML classification model to predict if a person is susceptible to
breast or cervical cancer based on demographic factors. We devised a system to
provide suggestions for the nearest hospital or Cancer treatment centres based
on the users location or address. In addition to this, we can integrate the
health card to maintain medical records of all individuals and conduct
awareness drives and campaigns. For ML classification models, we used decision
tree classification and support vector classification algorithms for cervical
cancer susceptibility and breast cancer susceptibility respectively. Thus, by
devising this solution we come one step closer to our goal which is spreading
cancer awareness, thereby, decreasing the cancer mortality and increasing
cancer literacy among the people of Telangana.

</details>


### [110] [Process mining-driven modeling and simulation to enhance fault diagnosis in cyber-physical systems](https://arxiv.org/abs/2506.21502)
*Francesco Vitale,Nicola Dall'Ora,Sebastiano Gaiardelli,Enrico Fraccaroli,Nicola Mazzocca,Franco Fummi*

Main category: cs.LG

TL;DR: 提出了一种结合多变量时间序列分析、过程挖掘和随机模拟的无监督故障诊断方法，用于CPS中的异常检测和根因分析。


<details>
  <summary>Details</summary>
Motivation: 手动建模故障行为需要大量领域知识且模型复杂、易错、难以解释，因此需要一种更高效的方法。

Method: 通过多变量时间序列分析检测异常，将其转化为结构化事件日志，利用过程挖掘提取可解释的模型，并通过随机模拟增强根因分析。

Result: 在Robotic Arm Dataset上验证了方法的有效性，能够建模、模拟和分类CPS中的故障行为。

Conclusion: 该方法支持创建全面的故障字典，有助于预测性维护和工业环境中的数字孪生开发。

Abstract: Fault diagnosis in Cyber-Physical Systems (CPSs) is essential for ensuring
system dependability and operational efficiency by accurately detecting
anomalies and identifying their root causes. However, the manual modeling of
faulty behaviors often demands extensive domain expertise and produces models
that are complex, error-prone, and difficult to interpret. To address this
challenge, we present a novel unsupervised fault diagnosis methodology that
integrates collective anomaly detection in multivariate time series, process
mining, and stochastic simulation. Initially, collective anomalies are detected
from low-level sensor data using multivariate time-series analysis. These
anomalies are then transformed into structured event logs, enabling the
discovery of interpretable process models through process mining. By
incorporating timing distributions into the extracted Petri nets, the approach
supports stochastic simulation of faulty behaviors, thereby enhancing root
cause analysis and behavioral understanding. The methodology is validated using
the Robotic Arm Dataset (RoAD), a widely recognized benchmark in smart
manufacturing. Experimental results demonstrate its effectiveness in modeling,
simulating, and classifying faulty behaviors in CPSs. This enables the creation
of comprehensive fault dictionaries that support predictive maintenance and the
development of digital twins for industrial environments.

</details>


### [111] [mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and Model Selection at Scale](https://arxiv.org/abs/2506.21550)
*Xiaona Zhou,Constantin Brif,Ismini Lourentzou*

Main category: cs.LG

TL;DR: mTSBench是迄今为止最大的多变量时间序列异常检测（MTS-AD）和无监督模型选择基准，涵盖19个数据集中的344个标记时间序列，评估了24种异常检测方法，并揭示了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列异常检测在多个领域至关重要，但由于复杂的变量间依赖关系、时间动态性和稀疏的异常标签，仍具有挑战性。

Method: 引入mTSBench基准，评估24种异常检测方法（包括基于大语言模型的方法），并系统性地测试无监督模型选择技术。

Result: 结果显示没有单一检测器在所有数据集上表现优异，且现有模型选择方法远未达到最优。

Conclusion: mTSBench提供了一个统一的评估套件，以促进未来自适应异常检测和稳健模型选择的研究。

Abstract: Multivariate time series anomaly detection (MTS-AD) is critical in domains
like healthcare, cybersecurity, and industrial monitoring, yet remains
challenging due to complex inter-variable dependencies, temporal dynamics, and
sparse anomaly labels. We introduce mTSBench, the largest benchmark to date for
MTS-AD and unsupervised model selection, spanning 344 labeled time series
across 19 datasets and 12 diverse application domains. mTSBench evaluates 24
anomaly detection methods, including large language model (LLM)-based detectors
for multivariate time series, and systematically benchmarks unsupervised model
selection techniques under standardized conditions. Consistent with prior
findings, our results confirm that no single detector excels across datasets,
underscoring the importance of model selection. However, even state-of-the-art
selection methods remain far from optimal, revealing critical gaps. mTSBench
provides a unified evaluation suite to enable rigorous, reproducible
comparisons and catalyze future advances in adaptive anomaly detection and
robust model selection.

</details>


### [112] [Where to find Grokking in LLM Pretraining? Monitor Memorization-to-Generalization without Test](https://arxiv.org/abs/2506.21551)
*Ziyue Li,Chenrui Fan,Tianyi Zhou*

Main category: cs.LG

TL;DR: 研究验证了在大规模预训练模型中仍存在“grokking”现象，即测试性能在训练损失收敛后持续提升，并通过分析模型内部动态揭示了从记忆到泛化的转换机制。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络训练中“grokking”现象的机制及其在大型语言模型预训练中的表现，以理解泛化能力的延迟出现。

Method: 在7B参数的OLMoE模型预训练过程中，计算训练损失并评估其在数学推理、代码生成等任务上的泛化能力，同时开发新指标量化样本路径的距离和复杂度。

Result: 研究发现“grokking”现象在大型模型预训练中依然存在，样本路径从随机演变为结构化，复杂度降低，泛化能力提升。新指标能有效预测下游任务表现。

Conclusion: 研究揭示了“grokking”的机制，提供了从记忆到泛化的解释，并提出实用指标用于监控预训练中的泛化性能。

Abstract: Grokking, i.e., test performance keeps improving long after training loss
converged, has been recently witnessed in neural network training, making the
mechanism of generalization and other emerging capabilities such as reasoning
mysterious. While prior studies usually train small models on a few toy or
highly-specific tasks for thousands of epochs, we conduct the first study of
grokking on checkpoints during one-pass pretraining of a 7B large language
model (LLM), i.e., OLMoE. We compute the training loss and evaluate
generalization on diverse benchmark tasks, including math reasoning, code
generation, and commonsense/domain-specific knowledge retrieval tasks.
  Our study, for the first time, verifies that grokking still happens in the
pretraining of large-scale foundation models, though different data may enter
grokking stages asynchronously. We further demystify grokking's "emergence of
generalization" by investigating LLM internal dynamics. Specifically, we find
that training samples' pathways (i.e., expert choices across layers) evolve
from random, instance-specific to more structured and shareable between samples
during grokking. Also, the complexity of a sample's pathway reduces despite the
converged loss. These indicate a memorization-to-generalization conversion,
providing a mechanistic explanation of delayed generalization. In the study, we
develop two novel metrics to quantify pathway distance and the complexity of a
single pathway. We show their ability to predict the generalization improvement
on diverse downstream tasks. They are efficient, simple to compute and solely
dependent on training data. Hence, they have practical value for pretraining,
enabling us to monitor the generalization performance without finetuning and
test. Theoretically, we show that more structured pathways reduce model
complexity and improve the generalization bound.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [113] [Guarding Offices with Maximum Dispersion](https://arxiv.org/abs/2506.21307)
*Sándor P. Fekete,Kai Kobbe,Dominik Krupke,Joseph S. B. Mitchell,Christian Rieck,Christian Scheffer*

Main category: cs.CG

TL;DR: 研究了在正交多边形中分散顶点守卫的Art Gallery问题，证明了某些条件下的NP完全性，并提出了多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 探索实际建筑平面图中分散守卫的优化问题，解决现有理论中的开放性问题。

Method: 结合NP完全性证明和多项式时间算法设计，包括动态规划和多种求解器比较。

Result: 证明了分散距离为4的问题的NP完全性，并提供了分散距离为3和2的多项式时间算法。

Conclusion: 该研究为实际应用中的分散守卫问题提供了理论支持和实用算法。

Abstract: We investigate the Dispersive Art Gallery Problem with vertex guards and
rectangular visibility ($r$-visibility) for a class of orthogonal polygons that
reflect the properties of real-world floor plans: these office-like polygons
consist of rectangular rooms and corridors. In the dispersive variant of the
Art Gallery Problem, the objective is not to minimize the number of guards but
to maximize the minimum geodesic $L_1$-distance between any two guards, called
the dispersion distance.
  Our main contributions are as follows. We prove that determining whether a
vertex guard set can achieve a dispersion distance of $4$ in office-like
polygons is NP-complete, where vertices of the polygon are restricted to
integer coordinates. Additionally, we present a simple worst-case optimal
algorithm that guarantees a dispersion distance of $3$ in polynomial time. Our
complexity result extends to polyominoes, resolving an open question posed by
Rieck and Scheffer (CGTA 2024). When vertex coordinates are allowed to be
rational, we establish analogous results, proving that achieving a dispersion
distance of $2+\varepsilon$ is NP-hard for any $\varepsilon > 0$, while the
classic Art Gallery Problem remains solvable in polynomial time for this class
of polygons. Furthermore, we give a straightforward polynomial-time algorithm
that computes worst-case optimal solutions with a dispersion distance of $2$.
  On the other hand, for the more restricted class of hole-free independent
office-like polygons, we propose a dynamic programming approach that computes
optimal solutions. Moreover, we demonstrate that the problem is practically
tractable for arbitrary orthogonal polygons. To this end, we compare solvers
based on SAT, CP, and MIP formulations. Notably, SAT solvers efficiently
compute optimal solutions for randomly generated instances with up to $1600$
vertices in under $15$s.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [114] [Inside Job: Defending Kubernetes Clusters Against Network Misconfigurations](https://arxiv.org/abs/2506.21134)
*Jacopo Bufalino,Jose Luis Martin-Navarro,Mario Di Francesco,Tuomas Aura*

Main category: cs.CR

TL;DR: 论文分析了Kubernetes集群中网络配置对安全性的影响，特别是横向移动问题，发现634个配置错误，远超现有解决方案的能力，并成功帮助修复了30多个应用的问题。


<details>
  <summary>Details</summary>
Motivation: Kubernetes作为容器编排的事实标准，其安全性研究较少关注网络配置的影响，尤其是横向移动问题。

Method: 对6个不同组织的287个开源应用进行了全面分析，评估网络配置错误。

Result: 发现634个配置错误，远超现有解决方案，并成功帮助修复30多个应用的问题。

Conclusion: 网络配置对Kubernetes安全性至关重要，现有解决方案需改进以应对此类问题。

Abstract: Kubernetes has emerged as the de facto standard for container orchestration.
Unfortunately, its increasing popularity has also made it an attractive target
for malicious actors. Despite extensive research on securing Kubernetes, little
attention has been paid to the impact of network configuration on the security
of application deployments. This paper addresses this gap by conducting a
comprehensive analysis of network misconfigurations in a Kubernetes cluster
with specific reference to lateral movement. Accordingly, we carried out an
extensive evaluation of 287 open-source applications belonging to six different
organizations, ranging from IT companies and public entities to non-profits. As
a result, we identified 634 misconfigurations, well beyond what could be found
by solutions in the state of the art. We responsibly disclosed our findings to
the concerned organizations and engaged in a discussion to assess their
severity. As of now, misconfigurations affecting more than thirty applications
have been fixed with the mitigations we proposed.

</details>


### [115] [Empowering Digital Agriculture: A Privacy-Preserving Framework for Data Sharing and Collaborative Research](https://arxiv.org/abs/2506.20872)
*Osama Zafar,Rosemarie Santa González,Mina Namazi,Alfonso Morales,Erman Ayday*

Main category: cs.CR

TL;DR: 提出了一种隐私保护框架，结合降维技术和差分隐私，支持农业数据的安全共享与合作，同时保护农民隐私。


<details>
  <summary>Details</summary>
Motivation: 解决农民因隐私问题不愿共享数据的问题，以促进数据驱动农业的发展。

Method: 结合主成分分析（PCA）和拉普拉斯噪声的差分隐私技术，支持联邦学习和隐私保护数据聚合。

Result: 在真实数据集上验证了框架的隐私保护效果和实用性，性能接近集中式系统。

Conclusion: 该框架有助于农民和研究者的合作，推动农业数据的安全利用和创新。

Abstract: Data-driven agriculture, which integrates technology and data into
agricultural practices, has the potential to improve crop yield, disease
resilience, and long-term soil health. However, privacy concerns, such as
adverse pricing, discrimination, and resource manipulation, deter farmers from
sharing data, as it can be used against them. To address this barrier, we
propose a privacy-preserving framework that enables secure data sharing and
collaboration for research and development while mitigating privacy risks. The
framework combines dimensionality reduction techniques (like Principal
Component Analysis (PCA)) and differential privacy by introducing Laplacian
noise to protect sensitive information. The proposed framework allows
researchers to identify potential collaborators for a target farmer and train
personalized machine learning models either on the data of identified
collaborators via federated learning or directly on the aggregated
privacy-protected data. It also allows farmers to identify potential
collaborators based on similarities. We have validated this on real-life
datasets, demonstrating robust privacy protection against adversarial attacks
and utility performance comparable to a centralized system. We demonstrate how
this framework can facilitate collaboration among farmers and help researchers
pursue broader research objectives. The adoption of the framework can empower
researchers and policymakers to leverage agricultural data responsibly, paving
the way for transformative advances in data-driven agriculture. By addressing
critical privacy challenges, this work supports secure data integration,
fostering innovation and sustainability in agricultural systems.

</details>


### [116] [ZKPROV: A Zero-Knowledge Approach to Dataset Provenance for Large Language Models](https://arxiv.org/abs/2506.20915)
*Mina Namazi,Alexander Nemecek,Erman Ayday*

Main category: cs.CR

TL;DR: ZKPROV是一个新型加密框架，通过零知识证明验证大语言模型（LLM）的数据来源，确保模型训练数据的可靠性而不泄露敏感信息。


<details>
  <summary>Details</summary>
Motivation: 在敏感领域（如医疗）部署LLM时，确保其计算来源的完整性至关重要，尤其是在数据使用受到严格监管的情况下。

Method: ZKPROV利用零知识证明和数据集签名元数据，将训练模型与其授权数据集绑定，避免验证每一步训练过程。

Result: 实验证明ZKPROV在生成和验证证明方面高效且可扩展，适用于实际部署。

Conclusion: ZKPROV在保护数据集机密性的同时，提供了可信的数据来源证明，为LLM的部署提供了实用解决方案。

Abstract: As the deployment of large language models (LLMs) grows in sensitive domains,
ensuring the integrity of their computational provenance becomes a critical
challenge, particularly in regulated sectors such as healthcare, where strict
requirements are applied in dataset usage. We introduce ZKPROV, a novel
cryptographic framework that enables zero-knowledge proofs of LLM provenance.
It allows users to verify that a model is trained on a reliable dataset without
revealing sensitive information about it or its parameters. Unlike prior
approaches that focus on complete verification of the training process
(incurring significant computational cost) or depend on trusted execution
environments, ZKPROV offers a distinct balance. Our method cryptographically
binds a trained model to its authorized training dataset(s) through
zero-knowledge proofs while avoiding proof of every training step. By
leveraging dataset-signed metadata and compact model parameter commitments,
ZKPROV provides sound and privacy-preserving assurances that the result of the
LLM is derived from a model trained on the claimed authorized and relevant
dataset. Experimental results demonstrate the efficiency and scalability of the
ZKPROV in generating this proof and verifying it, achieving a practical
solution for real-world deployments. We also provide formal security
guarantees, proving that our approach preserves dataset confidentiality while
ensuring trustworthy dataset provenance.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [117] [From On-chain to Macro: Assessing the Importance of Data Source Diversity in Cryptocurrency Market Forecasting](https://arxiv.org/abs/2506.21246)
*Giorgos Demosthenous,Chryssis Georgiou,Eliada Polydorou*

Main category: q-fin.PM

TL;DR: 研究探讨了数据源多样性对加密货币预测模型性能的影响，发现多样性显著提升预测准确性，尤其是链上指标对短期和长期预测的重要性。


<details>
  <summary>Details</summary>
Motivation: 加密货币市场复杂且波动大，传统预测模型依赖单一数据源，难以全面捕捉市场动态。研究旨在探索多源数据对预测性能的提升。

Method: 整合多种数据类别（如技术指标、链上指标、情绪指标等），提出新特征降维算法，构建Crypto100指数，并通过实验验证多样性效果。

Result: 数据源多样性显著提升模型预测性能，链上指标对短期和长期预测均关键，传统市场指标和宏观经济指标对长期预测更相关。

Conclusion: 多源数据整合能显著提升加密货币预测模型的准确性和稳健性，为未来模型开发提供了重要参考。

Abstract: This study investigates the impact of data source diversity on the
performance of cryptocurrency forecasting models by integrating various data
categories, including technical indicators, on-chain metrics, sentiment and
interest metrics, traditional market indices, and macroeconomic indicators. We
introduce the Crypto100 index, representing the top 100 cryptocurrencies by
market capitalization, and propose a novel feature reduction algorithm to
identify the most impactful and resilient features from diverse data sources.
Our comprehensive experiments demonstrate that data source diversity
significantly enhances the predictive performance of forecasting models across
different time horizons. Key findings include the paramount importance of
on-chain metrics for both short-term and long-term predictions, the growing
relevance of traditional market indices and macroeconomic indicators for
longer-term forecasts, and substantial improvements in model accuracy when
diverse data sources are utilized. These insights help demystify the short-term
and long-term driving factors of the cryptocurrency market and lay the
groundwork for developing more accurate and resilient forecasting models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [118] [Transferring disentangled representations: bridging the gap between synthetic and real images](https://arxiv.org/abs/2409.18017)
*Jacopo Dapueto,Nicoletta Noceti,Francesca Odone*

Main category: cs.CV

TL;DR: 研究了如何利用合成数据学习解耦表示，并将其迁移到真实数据，提出了一种新的度量方法。


<details>
  <summary>Details</summary>
Motivation: 解耦表示学习在真实图像上未充分发挥潜力，因生成因素相关、分辨率低及缺乏真实标签。

Method: 利用合成数据学习解耦表示，分析微调效果及迁移后保留的解耦特性，并提出新的干预度量方法。

Result: 实验表明，从合成数据到真实数据的表示迁移是可行且有效的。

Conclusion: 解耦表示的部分特性可在迁移后保留，为实际应用提供了可能性。

Abstract: Developing meaningful and efficient representations that separate the
fundamental structure of the data generation mechanism is crucial in
representation learning. However, Disentangled Representation Learning has not
fully shown its potential on real images, because of correlated generative
factors, their resolution and limited access to ground truth labels.
Specifically on the latter, we investigate the possibility of leveraging
synthetic data to learn general-purpose disentangled representations applicable
to real data, discussing the effect of fine-tuning and what properties of
disentanglement are preserved after the transfer. We provide an extensive
empirical study to address these issues. In addition, we propose a new
interpretable intervention-based metric, to measure the quality of factors
encoding in the representation. Our results indicate that some level of
disentanglement, transferring a representation from synthetic to real data, is
possible and effective.

</details>


### [119] [Step-by-Step Video-to-Audio Synthesis via Negative Audio Guidance](https://arxiv.org/abs/2506.20995)
*Akio Hayakawa,Masato Ishii,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 提出了一种逐步生成视频到音频的方法，通过分步生成特定声音事件的音频轨道，模仿传统Foley工作流程，利用文本提示和先前生成的音频进行引导，无需专用配对数据集。


<details>
  <summary>Details</summary>
Motivation: 模仿传统Foley工作流程，全面捕捉视频中的声音事件，并通过分步生成提高音频合成的质量。

Method: 采用逐步视频到音频合成任务，每步基于目标文本提示和先前生成的音频，利用预训练模型避免专用数据集需求。

Result: 实验表明，该方法能为单个视频生成多个语义不同的音频轨道，合成质量优于现有基线。

Conclusion: 该方法通过分步生成和引导合成，显著提升了视频到音频生成的质量和多样性。

Abstract: We propose a novel step-by-step video-to-audio generation method that
sequentially produces individual audio tracks, each corresponding to a specific
sound event in the video. Our approach mirrors traditional Foley workflows,
aiming to capture all sound events induced by a given video comprehensively.
Each generation step is formulated as a guided video-to-audio synthesis task,
conditioned on a target text prompt and previously generated audio tracks. This
design is inspired by the idea of concept negation from prior compositional
generation frameworks. To enable this guided generation, we introduce a
training framework that leverages pre-trained video-to-audio models and
eliminates the need for specialized paired datasets, allowing training on more
accessible data. Experimental results demonstrate that our method generates
multiple semantically distinct audio tracks for a single input video, leading
to higher-quality composite audio synthesis than existing baselines.

</details>


### [120] [HybridQ: Hybrid Classical-Quantum Generative Adversarial Network for Skin Disease Image Generation](https://arxiv.org/abs/2506.21015)
*Qingyue Jiao,Kangyu Zheng,Yiyu Shi,Zhiding Liang*

Main category: cs.CV

TL;DR: 提出了一种经典-量子混合的生成对抗网络（GAN），能够生成彩色医学图像，性能优于传统方法，且参数和训练时间大幅减少。


<details>
  <summary>Details</summary>
Motivation: 解决皮肤疾病数据集的类别不平衡、隐私问题和对象偏差，同时克服传统生成模型计算资源需求高和量子生成模型仅能生成低质量灰度图像的限制。

Method: 通过经典-量子潜在空间融合技术，开发了一种新型混合GAN，能够生成高质量彩色医学图像。

Result: 模型在图像生成质量和分类性能提升上优于传统方法，且参数和训练时间显著减少。在真实量子硬件上表现稳健。

Conclusion: 量子图像生成在量子硬件进步下有广阔前景，混合GAN为医学图像生成提供了高效解决方案。

Abstract: Machine learning-assisted diagnosis is gaining traction in skin disease
detection, but training effective models requires large amounts of high-quality
data. Skin disease datasets often suffer from class imbalance, privacy
concerns, and object bias, making data augmentation essential. While classical
generative models are widely used, they demand extensive computational
resources and lengthy training time. Quantum computing offers a promising
alternative, but existing quantum-based image generation methods can only yield
grayscale low-quality images. Through a novel classical-quantum latent space
fusion technique, our work overcomes this limitation and introduces the first
classical-quantum generative adversarial network (GAN) capable of generating
color medical images. Our model outperforms classical deep convolutional GANs
and existing hybrid classical-quantum GANs in both image generation quality and
classification performance boost when used as data augmentation. Moreover, the
performance boost is comparable with that achieved using state-of-the-art
classical generative models, yet with over 25 times fewer parameters and 10
times fewer training epochs. Such results suggest a promising future for
quantum image generation as quantum hardware advances. Finally, we demonstrate
the robust performance of our model on real IBM quantum machine with hardware
noise.

</details>


### [121] [Improving Diffusion-Based Image Editing Faithfulness via Guidance and Scheduling](https://arxiv.org/abs/2506.21045)
*Hansam Cho,Seoung Bum Kim*

Main category: cs.CV

TL;DR: 论文提出FGS方法，通过忠实性指导和调度策略，在图像编辑中平衡可编辑性和忠实性，实现高质量编辑。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导扩散模型在图像编辑中存在可编辑性与忠实性的固有权衡问题，难以同时优化两者。

Method: 提出FGS方法，结合忠实性指导和调度策略，增强输入图像信息的保留，并解决可编辑性与忠实性的错位问题。

Result: 实验表明，FGS在保持可编辑性的同时显著提升了忠实性，且兼容多种编辑方法。

Conclusion: FGS为图像编辑提供了一种高效平衡可编辑性和忠实性的解决方案，适用于多样化任务。

Abstract: Text-guided diffusion models have become essential for high-quality image
synthesis, enabling dynamic image editing. In image editing, two crucial
aspects are editability, which determines the extent of modification, and
faithfulness, which reflects how well unaltered elements are preserved.
However, achieving optimal results is challenging because of the inherent
trade-off between editability and faithfulness. To address this, we propose
Faithfulness Guidance and Scheduling (FGS), which enhances faithfulness with
minimal impact on editability. FGS incorporates faithfulness guidance to
strengthen the preservation of input image information and introduces a
scheduling strategy to resolve misalignment between editability and
faithfulness. Experimental results demonstrate that FGS achieves superior
faithfulness while maintaining editability. Moreover, its compatibility with
various editing methods enables precise, high-quality image edits across
diverse tasks.

</details>


### [122] [EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for Efficient Egocentric Perception](https://arxiv.org/abs/2506.21080)
*Sanjoy Chowdhury,Subrata Biswas,Sayan Nag,Tushar Nagarajan,Calvin Murdock,Ishwarya Ananthabhotla,Yijun Qian,Vamsi Krishna Ithapu,Dinesh Manocha,Ruohan Gao*

Main category: cs.CV

TL;DR: EgoAdapt框架通过跨模态蒸馏和策略学习，显著提升了多感知自我中心任务的效率，减少了计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现代感知模型在多感知自我中心任务中性能优异，但计算成本高，难以在资源受限环境中部署。

Method: 提出EgoAdapt框架，结合跨模态蒸馏和策略学习，适应不同任务的动作空间。

Result: 在三个数据集上测试，计算量减少89.09%，参数减少82.02%，能耗降低9.6倍，性能与或优于现有模型。

Conclusion: EgoAdapt在保持高性能的同时，显著提升了效率，适用于资源受限环境。

Abstract: Modern perception models, particularly those designed for multisensory
egocentric tasks, have achieved remarkable performance but often come with
substantial computational costs. These high demands pose challenges for
real-world deployment, especially in resource-constrained environments. In this
paper, we introduce EgoAdapt, a framework that adaptively performs cross-modal
distillation and policy learning to enable efficient inference across different
egocentric perception tasks, including egocentric action recognition, active
speaker localization, and behavior anticipation. Our proposed policy module is
adaptable to task-specific action spaces, making it broadly applicable.
Experimental results on three challenging egocentric datasets EPIC-Kitchens,
EasyCom, and Aria Everyday Activities demonstrate that our method significantly
enhances efficiency, reducing GMACs by up to 89.09%, parameters up to 82.02%,
and energy up to 9.6x, while still on-par and in many cases outperforming, the
performance of corresponding state-of-the-art models.

</details>


### [123] [Pushing Trade-Off Boundaries: Compact yet Effective Remote Sensing Change Detection](https://arxiv.org/abs/2506.21109)
*Luosheng Xu,Dalin Zhang,Zhaohui Song*

Main category: cs.CV

TL;DR: FlickCD是一种轻量级遥感变化检测模型，通过增强差异模块和多尺度特征融合，显著降低计算资源消耗，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型复杂度高但精度提升有限，FlickCD旨在平衡性能与资源消耗，适用于卫星实时处理。

Method: 提出增强差异模块（EDM）抑制无关变化，结合局部-全局融合块（SWSA和EGSA）捕捉多尺度语义信息。

Result: 在四个基准数据集上，FlickCD计算和存储开销降低一个数量级，性能达到或接近SOTA。

Conclusion: FlickCD在资源受限场景下实现了高效且高精度的变化检测，为遥感应用提供了实用解决方案。

Abstract: Remote sensing change detection is essential for monitoring urban expansion,
disaster assessment, and resource management, offering timely, accurate, and
large-scale insights into dynamic landscape transformations. While deep
learning has revolutionized change detection, the increasing complexity and
computational demands of modern models have not necessarily translated into
significant accuracy gains. Instead of following this trend, this study
explores a more efficient approach, focusing on lightweight models that
maintain high accuracy while minimizing resource consumption, which is an
essential requirement for on-satellite processing. To this end, we propose
FlickCD, which means quick flick then get great results, pushing the boundaries
of the performance-resource trade-off. FlickCD introduces an Enhanced
Difference Module (EDM) to amplify critical feature differences between
temporal phases while suppressing irrelevant variations such as lighting and
weather changes, thereby reducing computational costs in the subsequent change
decoder. Additionally, the FlickCD decoder incorporates Local-Global Fusion
Blocks, leveraging Shifted Window Self-Attention (SWSA) and Enhanced Global
Self-Attention (EGSA) to efficiently capture semantic information at multiple
scales, preserving both coarse- and fine-grained changes. Extensive experiments
on four benchmark datasets demonstrate that FlickCD reduces computational and
storage overheads by more than an order of magnitude while achieving
state-of-the-art (SOTA) performance or incurring only a minor (<1\% F1)
accuracy trade-off. The implementation code is publicly available at
https://github.com/xulsh8/FlickCD.

</details>


### [124] [A Comprehensive Dataset for Underground Miner Detection in Diverse Scenario](https://arxiv.org/abs/2506.21451)
*Cyrus Addy,Ajay Kumar Gurumadaiah,Yixiang Gao,Kwame Awuah-Offei*

Main category: cs.CV

TL;DR: 本文提出了一种专为地下采矿环境设计的热成像数据集，用于开发和验证矿工检测系统，以提升应急响应能力。


<details>
  <summary>Details</summary>
Motivation: 地下采矿作业面临重大安全挑战，需要可靠的矿工检测技术来支持机器人辅助的搜救行动，但目前缺乏适用于此类环境的训练数据集。

Method: 通过系统性地捕捉各种采矿活动和场景的热成像图像，创建了一个数据集，并评估了多种先进的目标检测算法（如YOLOv8、YOLOv10、YOLO11和RT-DETR）的性能。

Result: 研究证明了热成像用于矿工检测的可行性，并提供了未来研究的基础。

Conclusion: 该数据集是开发可靠热成像矿工检测系统的关键第一步，为实际应急场景的应用奠定了基础。

Abstract: Underground mining operations face significant safety challenges that make
emergency response capabilities crucial. While robots have shown promise in
assisting with search and rescue operations, their effectiveness depends on
reliable miner detection capabilities. Deep learning algorithms offer potential
solutions for automated miner detection, but require comprehensive training
datasets, which are currently lacking for underground mining environments. This
paper presents a novel thermal imaging dataset specifically designed to enable
the development and validation of miner detection systems for potential
emergency applications. We systematically captured thermal imagery of various
mining activities and scenarios to create a robust foundation for detection
algorithms. To establish baseline performance metrics, we evaluated several
state-of-the-art object detection algorithms including YOLOv8, YOLOv10, YOLO11,
and RT-DETR on our dataset. While not exhaustive of all possible emergency
situations, this dataset serves as a crucial first step toward developing
reliable thermal-based miner detection systems that could eventually be
deployed in real emergency scenarios. This work demonstrates the feasibility of
using thermal imaging for miner detection and establishes a foundation for
future research in this critical safety application.

</details>


### [125] [Evaluation of Traffic Signals for Daily Traffic Pattern](https://arxiv.org/abs/2506.21469)
*Mohammad Shokrolah Shirazi,Hung-Fu Chang*

Main category: cs.CV

TL;DR: 论文提出了动态、静态和混合三种基于转向运动计数（TMC）的交通信号配置方法，并通过仿真验证了不同信号设计在不同交通条件下的性能。


<details>
  <summary>Details</summary>
Motivation: 转向运动计数数据对交通信号设计、交叉口几何规划和拥堵分析至关重要，但现有方法未能充分适应交通流的变化。

Method: 开发了基于视觉的跟踪系统估计TMC，结合仿真工具SUMO评估信号设计，提出动态、静态和混合三种配置方法。

Result: 实验表明，90和120秒的信号周期效果最佳；动态配置在四个交叉口表现更好，混合方法在高峰和非高峰时段表现更优。

Conclusion: 混合信号方法能适应不同交通模式，尤其适用于交通流分布不均的交叉口。

Abstract: The turning movement count data is crucial for traffic signal design,
intersection geometry planning, traffic flow, and congestion analysis. This
work proposes three methods called dynamic, static, and hybrid configuration
for TMC-based traffic signals. A vision-based tracking system is developed to
estimate the TMC of six intersections in Las Vegas using traffic cameras. The
intersection design, route (e.g. vehicle movement directions), and signal
configuration files with compatible formats are synthesized and imported into
Simulation of Urban MObility for signal evaluation with realistic data. The
initial experimental results based on estimated waiting times indicate that the
cycle time of 90 and 120 seconds works best for all intersections. In addition,
four intersections show better performance for dynamic signal timing
configuration, and the other two with lower performance have a lower ratio of
total vehicle count to total lanes of the intersection leg. Since daily traffic
flow often exhibits a bimodal pattern, we propose a hybrid signal method that
switches between dynamic and static methods, adapting to peak and off-peak
traffic conditions for improved flow management. So, a built-in traffic
generator module creates vehicle routes for 4 hours, including peak hours, and
a signal design module produces signal schedule cycles according to static,
dynamic, and hybrid methods. Vehicle count distributions are weighted
differently for each zone (i.e., West, North, East, South) to generate diverse
traffic patterns. The extended experimental results for 6 intersections with 4
hours of simulation time imply that zone-based traffic pattern distributions
affect signal design selection. Although the static method works great for
evenly zone-based traffic distribution, the hybrid method works well for highly
weighted traffic at intersection pairs of the West-East and North-South zones.

</details>


### [126] [Towards Reliable Detection of Empty Space: Conditional Marked Point Processes for Object Detection](https://arxiv.org/abs/2506.21486)
*Tobias J. Riedlinger,Kira Maag,Hanno Gottschalk*

Main category: cs.CV

TL;DR: 提出了一种基于空间统计学的目标检测模型，用于解决现有模型在未检测区域不确定性量化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有目标检测模型的置信度估计常不准确，且无法量化未检测区域的不确定性，这在自动驾驶等应用中存在安全隐患。

Method: 采用空间统计学中的标记点过程，将边界框数据建模为空间点事件，通过基于似然的训练提供明确的置信度估计。

Result: 通过校准评估和性能测试验证了方法的有效性。

Conclusion: 提出的统计框架能够提供更准确的置信度估计，特别是在未检测区域的概率评估上表现优异。

Abstract: Deep neural networks have set the state-of-the-art in computer vision tasks
such as bounding box detection and semantic segmentation. Object detectors and
segmentation models assign confidence scores to predictions, reflecting the
model's uncertainty in object detection or pixel-wise classification. However,
these confidence estimates are often miscalibrated, as their architectures and
loss functions are tailored to task performance rather than probabilistic
foundation. Even with well calibrated predictions, object detectors fail to
quantify uncertainty outside detected bounding boxes, i.e., the model does not
make a probability assessment of whether an area without detected objects is
truly free of obstacles. This poses a safety risk in applications such as
automated driving, where uncertainty in empty areas remains unexplored. In this
work, we propose an object detection model grounded in spatial statistics.
Bounding box data matches realizations of a marked point process, commonly used
to describe the probabilistic occurrence of spatial point events identified as
bounding box centers, where marks are used to describe the spatial extension of
bounding boxes and classes. Our statistical framework enables a
likelihood-based training and provides well-defined confidence estimates for
whether a region is drivable, i.e., free of objects. We demonstrate the
effectiveness of our method through calibration assessments and evaluation of
performance.

</details>


### [127] [Maximal Matching Matters: Preventing Representation Collapse for Robust Cross-Modal Retrieval](https://arxiv.org/abs/2506.21538)
*Hani Alomari,Anushka Sivakumar,Andrew Zhang,Chris Thomas*

Main category: cs.CV

TL;DR: 论文提出了一种改进跨模态图像-文本检索的方法，通过最大化嵌入集之间的一对一匹配和引入两种损失函数来解决稀疏监督和集合崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 传统单向量嵌入方法难以捕捉跨模态的多样关联，而基于集合的方法虽能捕获更丰富关系，但仍面临稀疏监督和集合崩溃的挑战。

Method: 提出Maximal Pair Assignment Similarity优化嵌入集间的一对一匹配，并引入Global Discriminative Loss和Intra-Set Divergence Loss增强表示。

Result: 在MS-COCO和Flickr30k上实现了最先进性能，无需依赖外部数据。

Conclusion: 该方法有效解决了集合表示的局限性，提升了跨模态检索的性能。

Abstract: Cross-modal image-text retrieval is challenging because of the diverse
possible associations between content from different modalities. Traditional
methods learn a single-vector embedding to represent semantics of each sample,
but struggle to capture nuanced and diverse relationships that can exist across
modalities. Set-based approaches, which represent each sample with multiple
embeddings, offer a promising alternative, as they can capture richer and more
diverse relationships. In this paper, we show that, despite their promise,
these set-based representations continue to face issues including sparse
supervision and set collapse, which limits their effectiveness. To address
these challenges, we propose Maximal Pair Assignment Similarity to optimize
one-to-one matching between embedding sets which preserve semantic diversity
within the set. We also introduce two loss functions to further enhance the
representations: Global Discriminative Loss to enhance distinction among
embeddings, and Intra-Set Divergence Loss to prevent collapse within each set.
Our method achieves state-of-the-art performance on MS-COCO and Flickr30k
without relying on external data.

</details>


### [128] [HalluSegBench: Counterfactual Visual Reasoning for Segmentation Hallucination Evaluation](https://arxiv.org/abs/2506.21546)
*Xinzhuo Li,Adheesh Juvekar,Xingyou Liu,Muntasir Wahed,Kiet A. Nguyen,Ismini Lourentzou*

Main category: cs.CV

TL;DR: HalluSegBench是一个新基准，用于评估视觉语言分割模型中的幻觉问题，通过反事实视觉推理揭示模型在视觉驱动幻觉上的普遍性。


<details>
  <summary>Details</summary>
Motivation: 现有分割幻觉评估协议主要关注标签或文本幻觉，缺乏对视觉上下文的操控，无法诊断关键失败。

Method: 引入HalluSegBench，包含1340对反事实实例和281个对象类，以及新指标量化幻觉敏感性。

Result: 实验表明，视觉驱动幻觉比标签驱动更普遍，模型常持续错误分割。

Conclusion: 反事实推理对诊断视觉语言分割模型的真实性至关重要。

Abstract: Recent progress in vision-language segmentation has significantly advanced
grounded visual understanding. However, these models often exhibit
hallucinations by producing segmentation masks for objects not grounded in the
image content or by incorrectly labeling irrelevant regions. Existing
evaluation protocols for segmentation hallucination primarily focus on label or
textual hallucinations without manipulating the visual context, limiting their
capacity to diagnose critical failures. In response, we introduce
HalluSegBench, the first benchmark specifically designed to evaluate
hallucinations in visual grounding through the lens of counterfactual visual
reasoning. Our benchmark consists of a novel dataset of 1340 counterfactual
instance pairs spanning 281 unique object classes, and a set of newly
introduced metrics that quantify hallucination sensitivity under visually
coherent scene edits. Experiments on HalluSegBench with state-of-the-art
vision-language segmentation models reveal that vision-driven hallucinations
are significantly more prevalent than label-driven ones, with models often
persisting in false segmentation, highlighting the need for counterfactual
reasoning to diagnose grounding fidelity.

</details>


### [129] [Whole-Body Conditioned Egocentric Video Prediction](https://arxiv.org/abs/2506.21552)
*Yutong Bai,Danny Tran,Amir Bar,Yann LeCun,Trevor Darrell,Jitendra Malik*

Main category: cs.CV

TL;DR: 训练模型通过人体动作预测第一人称视频（PEVA），结合过去视频和3D身体姿态，利用扩散变换器学习动作对环境的影响。


<details>
  <summary>Details</summary>
Motivation: 研究如何从第一人称视角模拟人类动作对环境的影响，解决复杂现实环境建模的挑战。

Method: 使用自回归条件扩散变换器，基于大规模真实世界数据集Nymeria训练，结合身体姿态轨迹。

Result: 设计了分层评估协议，全面分析模型的预测和控制能力。

Conclusion: 这是从人类视角建模复杂环境和具身行为的初步尝试。

Abstract: We train models to Predict Ego-centric Video from human Actions (PEVA), given
the past video and an action represented by the relative 3D body pose. By
conditioning on kinematic pose trajectories, structured by the joint hierarchy
of the body, our model learns to simulate how physical human actions shape the
environment from a first-person point of view. We train an auto-regressive
conditional diffusion transformer on Nymeria, a large-scale dataset of
real-world egocentric video and body pose capture. We further design a
hierarchical evaluation protocol with increasingly challenging tasks, enabling
a comprehensive analysis of the model's embodied prediction and control
abilities. Our work represents an initial attempt to tackle the challenges of
modeling complex real-world environments and embodied agent behaviors with
video prediction from the perspective of a human.

</details>


<div id='q-bio.CB'></div>

# q-bio.CB [[Back]](#toc)

### [130] [scMamba: A Scalable Foundation Model for Single-Cell Multi-Omics Integration Beyond Highly Variable Feature Selection](https://arxiv.org/abs/2506.20697)
*Zhen Yuan,Shaoqing Jiao,Yihang Xiao,Jiajie Peng*

Main category: q-bio.CB

TL;DR: scMamba是一种无需预先特征选择即可整合单细胞多组学数据的基础模型，通过基于补丁的细胞标记化策略和对比学习方法，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单细胞多组学技术的出现为细胞研究提供了新视角，但现有方法在预处理中可能丢失重要生物信息，因此需要一种更有效的数据整合方法。

Method: scMamba采用基于补丁的细胞标记化策略，将基因组区域视为标记，细胞视为句子，并结合对比学习和余弦相似性正则化。

Result: scMamba在多个数据集上表现优异，显著优于现有方法，尤其在生物变异保留、组学层对齐和下游任务中表现突出。

Conclusion: scMamba是一种强大的单细胞多组学整合工具，适用于大规模图谱研究，推动生物学发现。

Abstract: The advent of single-cell multi-omics technologies has enabled the
simultaneous profiling of diverse omics layers within individual cells.
Integrating such multimodal data provides unprecedented insights into cellular
identity, regulatory processes, and disease mechanisms. However, it remains
challenging, as current methods often rely on selecting highly variable genes
or peaks during preprocessing, which may inadvertently discard crucial
biological information. Here, we present scMamba, a foundation model designed
to integrate single-cell multi-omics data without the need for prior feature
selection while preserving genomic positional information. scMamba introduces a
patch-based cell tokenization strategy that treats genomics regions as words
(tokens) and cells as sentences. Building upon the concept of state space
duality, scMamba distills rich biological insights from high-dimensional,
sparse single-cell multi-omics data. Additionally, our novel contrastive
learning approach, enhanced with cosine similarity regularization, enables
superior alignment across omics layers compared to traditional methods.
Systematic benchmarking across multiple datasets demonstrates that scMamba
significantly outperforms state-of-the-art methods in preserving biological
variation, aligning omics layers, and enhancing key downstream tasks such as
clustering, cell type annotation, and trajectory inference. Our findings
position scMamba as a powerful tool for large-scale single-cell multi-omics
integration, capable of handling large-scale atlases and advancing biological
discovery.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [131] [Transformer-Based Spatial-Temporal Counterfactual Outcomes Estimation](https://arxiv.org/abs/2506.21154)
*He Li,Haoang Chi,Mingyu Liu,Wanrong Huang,Liyang Xu,Wenjing Yang*

Main category: stat.ME

TL;DR: 本文提出了一种基于Transformer的新框架，用于估计具有时空属性的反事实结果，展示了更强的估计能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界具有时间和空间维度，但现有基于经典统计模型的方法在性能和泛化能力上存在局限。

Method: 使用Transformer构建框架，提出了一种一致且渐近正态的估计器。

Result: 模拟实验表明该估计器优于基线方法；真实数据实验揭示了冲突对哥伦比亚森林损失的因果效应。

Conclusion: 提出的框架在时空反事实结果估计中表现出色，为实际问题提供了有价值的结论。

Abstract: The real world naturally has dimensions of time and space. Therefore,
estimating the counterfactual outcomes with spatial-temporal attributes is a
crucial problem. However, previous methods are based on classical statistical
models, which still have limitations in performance and generalization. This
paper proposes a novel framework for estimating counterfactual outcomes with
spatial-temporal attributes using the Transformer, exhibiting stronger
estimation ability. Under mild assumptions, the proposed estimator within this
framework is consistent and asymptotically normal. To validate the
effectiveness of our approach, we conduct simulation experiments and real data
experiments. Simulation experiments show that our estimator has a stronger
estimation capability than baseline methods. Real data experiments provide a
valuable conclusion to the causal effect of conflicts on forest loss in
Colombia. The source code is available at
https://github.com/lihe-maxsize/DeppSTCI_Release_Version-master.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [132] [Spiking Neural Networks for SAR Interferometric Phase Unwrapping: A Theoretical Framework for Energy-Efficient Processing](https://arxiv.org/abs/2506.20782)
*Marc Bara*

Main category: cs.NE

TL;DR: 首次提出将脉冲神经网络（SNN）应用于合成孔径雷达（SAR）干涉相位解缠的理论框架，填补了现有方法的空白，并展示了SNN在能效和准确性上的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着地球观测数据量的指数增长（如NISAR任务预计两年内生成100PB数据），能效处理对可持续数据中心运营至关重要。SNN的事件驱动计算模型可节省30-100倍能源，同时保持可比准确性。

Method: 开发了针对缠绕相位数据的脉冲编码方案，提出了利用相位解缠空间传播特性的SNN架构，并分析了计算复杂性和收敛性。

Result: SNN的时间动态特性自然建模了相位解缠的空间连续性约束，为大规模InSAR处理提供了可持续的补充方法。

Conclusion: 该工作为神经形态计算与SAR干涉测量的交叉领域开辟了新研究方向，有望推动可持续的大规模InSAR处理。

Abstract: We present the first theoretical framework for applying spiking neural
networks (SNNs) to synthetic aperture radar (SAR) interferometric phase
unwrapping. Despite extensive research in both domains, our comprehensive
literature review confirms that SNNs have never been applied to phase
unwrapping, representing a significant gap in current methodologies. As Earth
observation data volumes continue to grow exponentially (with missions like
NISAR expected to generate 100PB in two years) energy-efficient processing
becomes critical for sustainable data center operations. SNNs, with their
event-driven computation model, offer potential energy savings of 30-100x
compared to conventional approaches while maintaining comparable accuracy. We
develop spike encoding schemes specifically designed for wrapped phase data,
propose SNN architectures that leverage the spatial propagation nature of phase
unwrapping, and provide theoretical analysis of computational complexity and
convergence properties. Our framework demonstrates how the temporal dynamics
inherent in SNNs can naturally model the spatial continuity constraints
fundamental to phase unwrapping. This work opens a new research direction at
the intersection of neuromorphic computing and SAR interferometry, offering a
complementary approach to existing algorithms that could enable more
sustainable large-scale InSAR processing.

</details>


### [133] [Stochastic Quantum Spiking Neural Networks with Quantum Memory and Local Learning](https://arxiv.org/abs/2506.21324)
*Jiechen Chen,Bipin Rajendran,Osvaldo Simeone*

Main category: cs.NE

TL;DR: 提出了一种结合神经形态计算和量子计算优势的随机量子脉冲（SQS）神经元模型，解决了现有量子脉冲模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 结合神经形态计算的时间序列处理效率和量子计算的指数级状态空间优势，推动量子脉冲神经网络的发展。

Method: 使用多量子比特电路实现具有内部量子记忆的脉冲单元，并通过硬件友好的局部学习规则训练SQS神经网络（SQSNN）。

Result: SQS神经元模型实现了单次事件驱动的概率脉冲生成，无需全局经典反向传播。

Conclusion: SQSNN模型为模块化、可扩展且可在量子硬件上训练的量子脉冲神经网络铺平了道路。

Abstract: Neuromorphic and quantum computing have recently emerged as promising
paradigms for advancing artificial intelligence, each offering complementary
strengths. Neuromorphic systems built on spiking neurons excel at processing
time-series data efficiently through sparse, event-driven computation,
consuming energy only upon input events. Quantum computing, on the other hand,
leverages superposition and entanglement to explore feature spaces that are
exponentially large in the number of qubits. Hybrid approaches combining these
paradigms have begun to show potential, but existing quantum spiking models
have important limitations. Notably, prior quantum spiking neuron
implementations rely on classical memory mechanisms on single qubits, requiring
repeated measurements to estimate firing probabilities, and they use
conventional backpropagation on classical simulators for training. Here we
propose a stochastic quantum spiking (SQS) neuron model that addresses these
challenges. The SQS neuron uses multi-qubit quantum circuits to realize a
spiking unit with internal quantum memory, enabling event-driven probabilistic
spike generation in a single shot. Furthermore, we outline how networks of SQS
neurons -- dubbed SQS neural networks (SQSNNs) -- can be trained via a
hardware-friendly local learning rule, eliminating the need for global
classical backpropagation. The proposed SQSNN model fuses the time-series
efficiency of neuromorphic computing with the exponentially large inner state
space of quantum computing, paving the way for quantum spiking neural networks
that are modular, scalable, and trainable on quantum hardware.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [134] [Structural System Identification via Validation and Adaptation](https://arxiv.org/abs/2506.20799)
*Cristian López,Keegan J. Moore*

Main category: math.DS

TL;DR: 提出了一种基于生成模型的结构系统识别方法，用于参数估计和模型验证。


<details>
  <summary>Details</summary>
Motivation: 结合实验数据与科学理论，估计复杂系统的动态参数，以理解和预测系统行为。

Method: 使用神经网络将随机噪声映射为物理参数，通过已知运动方程生成假加速度，并与真实数据比较；利用独立验证数据集和判别器网络验证参数。

Result: 分析和实验表明，该方法能准确估计参数并验证非线性结构系统模型。

Conclusion: 该方法有效实现了结构系统识别、不确定性量化和模型验证。

Abstract: Estimating the governing equation parameter values is essential for
integrating experimental data with scientific theory to understand, validate,
and predict the dynamics of complex systems. In this work, we propose a new
method for structural system identification (SI), uncertainty quantification,
and validation directly from data. Inspired by generative modeling frameworks,
a neural network maps random noise to physically meaningful parameters. These
parameters are then used in the known equation of motion to obtain fake
accelerations, which are compared to real training data via a mean square error
loss. To simultaneously validate the learned parameters, we use independent
validation datasets. The generated accelerations from these datasets are
evaluated by a discriminator network, which determines whether the output is
real or fake, and guides the parameter-generator network. Analytical and real
experiments show the parameter estimation accuracy and model validation for
different nonlinear structural systems.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [135] [When Servers Meet Species: A Fab-to-Grave Lens on Computing's Biodiversity Impact](https://arxiv.org/abs/2506.20442)
*Tianyao Shi,Ritbik Kumar,Inez Hua,Yi Ding*

Main category: cs.CY

TL;DR: 本文首次全面分析了计算系统对生物多样性的影响，提出了两个新指标（EBI和OBI）和建模框架FABRIC，强调在可持续计算设计中需考虑生物多样性。


<details>
  <summary>Details</summary>
Motivation: 生物多样性丧失是一个关键的行星边界问题，但计算领域对其影响的研究不足，现有可持续性努力主要关注碳和水。

Method: 引入Embodied Biodiversity Index (EBI)和Operational Biodiversity Index (OBI)指标，开发FABRIC建模框架，量化计算系统全生命周期的生物多样性影响。

Result: 评估表明，可持续计算设计和优化需同时考虑生物多样性、碳和水的影响。

Conclusion: 本文填补了计算领域对生物多样性影响的研究空白，提供了量化工具和框架，推动了可持续计算的发展。

Abstract: Biodiversity loss is a critical planetary boundary, yet its connection to
computing remains largely unexamined. Prior sustainability efforts in computing
have focused on carbon and water, overlooking biodiversity due to the lack of
appropriate metrics and modeling frameworks. This paper presents the first
end-to-end analysis of biodiversity impact from computing systems. We introduce
two new metrics--Embodied Biodiversity Index (EBI) and Operational Biodiversity
Index (OBI)--to quantify biodiversity impact across the lifecycle, and present
FABRIC, a modeling framework that links computing workloads to biodiversity
impacts. Our evaluation highlights the need to consider biodiversity alongside
carbon and water in sustainable computing design and optimization. The code is
available at https://github.com/TianyaoShi/FABRIC.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [136] [Thinning to improve two-sample discrepancy](https://arxiv.org/abs/2506.20932)
*Gleb Smirnov,Roman Vershynin*

Main category: math.PR

TL;DR: 论文提出了一种在线算法，通过丢弃少量点，将两个独立样本的差异从O(√n)降低到O(log²ᵈ n)。


<details>
  <summary>Details</summary>
Motivation: 研究两个独立样本之间的差异，并探索如何通过算法减少这种差异。

Method: 提出了一种简单的在线算法，通过选择性丢弃部分数据点来实现差异的降低。

Result: 算法成功将差异从O(√n)减少到O(log²ᵈ n)。

Conclusion: 该方法有效降低了样本差异，为相关领域提供了新的解决方案。

Abstract: The discrepancy between two independent samples \(X_1,\dots,X_n\) and
\(Y_1,\dots,Y_n\) drawn from the same distribution on $\mathbb{R}^d$ typically
has order \(O(\sqrt{n})\) even in one dimension. We give a simple online
algorithm that reduces the discrepancy to \(O(\log^{2d} n)\) by discarding a
small fraction of the points.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [137] [Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks](https://arxiv.org/abs/2506.21182)
*Isaac Chung,Imene Kerboua,Marton Kardos,Roman Solomatin,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 本文探讨了如何通过工程实践确保MTEB（大规模文本嵌入基准）的可重现性和可扩展性，包括持续集成、数据集验证和社区贡献管理。


<details>
  <summary>Details</summary>
Motivation: MTEB已成为文本嵌入模型的标准评估平台，但需要确保其长期的可重现性和可扩展性，以保持其相关性和质量。

Method: 通过设计稳健的持续集成流程、自动化测试执行、数据集完整性验证以及灵活处理社区贡献和新任务扩展。

Result: 成功提升了MTEB的全面性和质量，同时保持了其可重现性和可用性。

Conclusion: 本文的经验为机器学习评估框架的维护者提供了有价值的参考，强调了工程实践在确保基准长期有效性中的重要性。

Abstract: The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation
platform for text embedding models. While previous work has established the
core benchmark methodology, this paper focuses on the engineering aspects that
ensure MTEB's continued reproducibility and extensibility. We present our
approach to maintaining robust continuous integration pipelines that validate
dataset integrity, automate test execution, and assess benchmark results'
generalizability. We detail the design choices that collectively enhance
reproducibility and usability. Furthermore, we discuss our strategies for
handling community contributions and extending the benchmark with new tasks and
datasets. These engineering practices have been instrumental in scaling MTEB to
become more comprehensive while maintaining quality and, ultimately, relevance
to the field. Our experiences offer valuable insights for benchmark maintainers
facing similar challenges in ensuring reproducibility and usability in machine
learning evaluation frameworks. The MTEB repository is available at:
https://github.com/embeddings-benchmark/mteb

</details>


### [138] [The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas](https://arxiv.org/abs/2506.20803)
*Chenglei Si,Tatsunori Hashimoto,Diyi Yang*

Main category: cs.CL

TL;DR: 研究发现，尽管LLM生成的研究想法在初始阶段被认为更具新颖性，但在实际执行后，其评分显著下降，甚至低于人类专家的想法。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM生成的研究想法是否能在实际执行后产生更好的研究成果。

Method: 招募43名专家研究人员，随机分配执行LLM生成或人类专家提出的研究想法，并撰写4页短论文。所有项目由NLP专家盲审评分。

Result: LLM生成的想法在执行后评分显著下降，多项指标上人类想法反超LLM想法。

Conclusion: 当前LLM在生成真正有效的研究想法上存在局限，且缺乏执行结果时难以准确评估研究想法。

Abstract: Large Language Models (LLMs) have shown promise in accelerating the
scientific research pipeline. A key capability for this process is the ability
to generate novel research ideas, and prior studies have found settings in
which LLM-generated research ideas were judged as more novel than human-expert
ideas. However, a good idea should not simply appear to be novel, it should
also result in better research after being executed. To test whether
AI-generated ideas lead to better research outcomes, we conduct an execution
study by recruiting 43 expert researchers to execute randomly-assigned ideas,
either written by experts or generated by an LLM. Each expert spent over 100
hours implementing the idea and wrote a 4-page short paper to document the
experiments. All the executed projects are then reviewed blindly by expert NLP
researchers. Comparing the review scores of the same ideas before and after
execution, the scores of the LLM-generated ideas decrease significantly more
than expert-written ideas on all evaluation metrics (novelty, excitement,
effectiveness, and overall; p < 0.05), closing the gap between LLM and human
ideas observed at the ideation stage. When comparing the aggregated review
scores from the execution study, we even observe that for many metrics there is
a flip in rankings where human ideas score higher than LLM ideas. This
ideation-execution gap highlights the limitations of current LLMs in generating
truly effective research ideas and the challenge of evaluating research ideas
in the absence of execution outcomes.

</details>


### [139] [Can Gradient Descent Simulate Prompting?](https://arxiv.org/abs/2506.20989)
*Eric Zhang,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 论文探讨了通过梯度更新模拟提示效果的方法，提出了一种元训练语言模型的技术，使其能够通过单次梯度更新实现类似提示的性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过梯度更新（如微调）模拟提示的效果，以解决提示在某些任务中表现更优但参数更新更高效的问题。

Method: 采用基于梯度的元学习方法，利用语言模型自身的提示预测作为目标，无需真实标签，通过梯度下降训练模拟提示效果。

Result: 实验显示，该方法在单次梯度更新后能部分或完全恢复提示模型的性能，尤其在“反转诅咒”任务和文本问答任务中表现显著。

Conclusion: 研究表明，适当的初始化下梯度下降具有较强表达能力，为长上下文建模和梯度学习泛化能力提供了新思路。

Abstract: There are two primary ways of incorporating new information into a language
model (LM): changing its prompt or changing its parameters, e.g. via
fine-tuning. Parameter updates incur no long-term storage cost for model
changes. However, for many model updates, prompting is significantly more
effective: prompted models can generalize robustly from single examples and
draw logical inferences that do not occur under standard fine-tuning. Can
models be modified so that fine-tuning does emulate prompting? This paper
describes a method for meta-training LMs such that gradient updates emulate the
effects of conditioning on new information. Our approach uses tools from
gradient-based meta-learning but uses an LM's own prompted predictions as
targets, eliminating the need for ground-truth labels. Subsequent gradient
descent training recovers some (and occasionally all) of prompted model
performance -- showing improvement on the ``reversal curse'' tasks, and
answering questions about text passages after a single gradient update. These
results suggest that, with appropriate initialization, gradient descent can be
surprisingly expressive. Our results suggest new avenues for long-context
modeling and offer insight into the generalization capabilities of
gradient-based learning.

</details>


### [140] [Small Encoders Can Rival Large Decoders in Detecting Groundedness](https://arxiv.org/abs/2506.21288)
*Istabrak Abbes,Gabriele Prato,Quentin Fournier,Fernando Rodriguez,Alaa Boukhary,Adam Elwood,Sarath Chandar*

Main category: cs.CL

TL;DR: 该研究提出了一种轻量级模型，用于检测查询是否基于提供的上下文，以减少LLMs生成答案时的资源消耗和延迟。


<details>
  <summary>Details</summary>
Motivation: LLMs在上下文信息不足时容易产生不可靠的回答，因此需要一种机制来检测查询是否基于上下文，以提高回答的准确性和效率。

Method: 使用轻量级编码器模型（如RoBERTa和NomicBERT），并在精选数据集上进行微调，以检测查询的groundedness。

Result: 这些轻量级模型在groundedness检测上的准确性与最先进的LLMs（如Llama3 8B和GPT4o）相当，同时显著降低了推理延迟。

Conclusion: 通过轻量级模型检测查询的groundedness，可以有效减少LLMs的资源消耗和延迟，同时保持高准确性。

Abstract: Augmenting large language models (LLMs) with external context significantly
improves their performance in natural language processing (NLP) tasks. However,
LLMs struggle to answer queries reliably when the provided context lacks
information, often resorting to ungrounded speculation or internal knowledge.
Groundedness - generating responses strictly supported by the context - is
essential for ensuring factual consistency and trustworthiness. This study
focuses on detecting whether a given query is grounded in a document provided
in context before the costly answer generation by LLMs. Such a detection
mechanism can significantly reduce both inference time and resource
consumption. We show that lightweight, task specific encoder models such as
RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy
comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in
groundedness detection while reducing inference latency by orders of magnitude.
The code is available at : https://github.com/chandarlab/Hallucinate-less

</details>


### [141] [Aligning Spoken Dialogue Models from User Interactions](https://arxiv.org/abs/2506.21463)
*Anne Wu,Laurent Mazaré,Neil Zeghidour,Alexandre Défossez*

Main category: cs.CL

TL;DR: 提出了一种新颖的偏好对齐框架，用于通过用户交互改进实时对话模型，解决了现有方法在实时语音交互中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有偏好学习方法主要针对文本模型，难以适应实时语音交互的复杂性（如打断、插话等）。

Method: 构建了一个包含15万对偏好标注的大规模数据集，结合AI反馈，利用离线对齐方法微调全双工自回归语音模型。

Result: 实验表明，通用对话反馈能显著提升语音对话模型的准确性、安全性和上下文对齐性。

Conclusion: 研究强调了在实时语音对话系统中平衡多种动态因素的重要性。

Abstract: We propose a novel preference alignment framework for improving spoken
dialogue models on real-time conversations from user interactions. Current
preference learning methods primarily focus on text-based language models, and
are not directly suited to the complexities of real-time speech interactions,
with richer dynamics (e.g. interruption, interjection) and no explicit
segmentation between speaker turns.We create a large-scale dataset of more than
150,000 preference pairs from raw multi-turn speech conversations, annotated
with AI feedback, to cover preferences over both linguistic content and
temporal context variations. We leverage offline alignment methods to finetune
a full-duplex autoregressive speech-to-speech model. Extensive experiments
demonstrate that feedback on generic conversations can be consistently
effective in improving spoken dialogue models to produce more factual, safer
and more contextually aligned interactions. We deploy the finetuned model and
conduct holistic human evaluations to assess the impact beyond single-turn
conversations. Our findings shed light on the importance of a well-calibrated
balance among various dynamics, crucial for natural real-time speech dialogue
systems.

</details>


### [142] [skLEP: A Slovak General Language Understanding Benchmark](https://arxiv.org/abs/2506.21508)
*Marek Šuppa,Andrej Ridzik,Daniel Hládek,Tomáš Javůrek,Viktória Ondrejová,Kristína Sásiková,Martin Tamajka,Marián Šimko*

Main category: cs.CL

TL;DR: 介绍了skLEP，首个专为评估斯洛伐克自然语言理解（NLU）模型设计的综合基准，涵盖九项多样化任务，并提供了模型能力的全面评估。


<details>
  <summary>Details</summary>
Motivation: 填补斯洛伐克语NLU评估的空白，推动该领域的研究发展。

Method: 通过整理新的斯洛伐克语数据集和翻译英语NLU资源构建基准，并系统评估多种预训练语言模型。

Result: 发布了完整的基准数据、开源工具包和公共排行榜，促进可重复性和未来研究。

Conclusion: skLEP为斯洛伐克语NLU研究提供了重要资源，有望推动该领域的进一步发展。

Abstract: In this work, we introduce skLEP, the first comprehensive benchmark
specifically designed for evaluating Slovak natural language understanding
(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span
token-level, sentence-pair, and document-level challenges, thereby offering a
thorough assessment of model capabilities. To create this benchmark, we curated
new, original datasets tailored for Slovak and meticulously translated
established English NLU resources. Within this paper, we also present the first
systematic and extensive evaluation of a wide array of Slovak-specific,
multilingual, and English pre-trained language models using the skLEP tasks.
Finally, we also release the complete benchmark data, an open-source toolkit
facilitating both fine-tuning and evaluation of models, and a public
leaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering
reproducibility and drive future research in Slovak NLU.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [143] [Efficacy of Temporal Fusion Transformers for Runoff Simulation](https://arxiv.org/abs/2506.20831)
*Sinan Rasiya Koya,Tirthankar Roy*

Main category: physics.geo-ph

TL;DR: Temporal Fusion Transformers (TFTs) slightly outperform LSTM in rainfall-runoff modeling, especially for midsection and peak hydrographs, and handle longer sequences better. TFT also provides explainable insights. Both models show performance drops with the Caravan dataset, suggesting data quality issues.


<details>
  <summary>Details</summary>
Motivation: To compare the performance of TFT and LSTM in hydrological predictions and explore their strengths in handling different catchment attributes and datasets.

Method: Train ten randomly initialized TFT and LSTM models for 531 CAMELS catchments in the US, and repeat with five subsets of the Caravan dataset (US, Australia, Brazil, Great Britain, Chile). Assess performance, variability, and dataset differences.

Result: TFT slightly outperforms LSTM, especially in simulating midsection and peak hydrographs, and handles longer sequences better. TFT also identifies key variables. Both models perform poorly with the Caravan dataset.

Conclusion: TFT shows potential for improving hydrological modeling and understanding, but data quality is crucial for performance.

Abstract: Combining attention with recurrence has shown to be valuable in sequence
modeling, including hydrological predictions. Here, we explore the strength of
Temporal Fusion Transformers (TFTs) over Long Short-Term Memory (LSTM) networks
in rainfall-runoff modeling. We train ten randomly initialized models, TFT and
LSTM, for 531 CAMELS catchments in the US. We repeat the experiment with five
subsets of the Caravan dataset, each representing catchments in the US,
Australia, Brazil, Great Britain, and Chile. Then, the performance of the
models, their variability regarding the catchment attributes, and the
difference according to the datasets are assessed. Our findings show that TFT
slightly outperforms LSTM, especially in simulating the midsection and peak of
hydrographs. Furthermore, we show the ability of TFT to handle longer sequences
and why it can be a better candidate for higher or larger catchments. Being an
explainable AI technique, TFT identifies the key dynamic and static variables,
providing valuable scientific insights. However, both TFT and LSTM exhibit a
considerable drop in performance with the Caravan dataset, indicating possible
data quality issues. Overall, the study highlights the potential of TFT in
improving hydrological modeling and understanding.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [144] [Performance improvement of spatial semantic segmentation with enriched audio features and agent-based error correction for DCASE 2025 Challenge Task 4](https://arxiv.org/abs/2506.21174)
*Jongyeon Park,Joonhee Lee,Do-Hyeon Lim,Hong Kook Kim,Hyeongcheol Geum,Jeong Eun Lim*

Main category: eess.AS

TL;DR: 该技术报告介绍了DCASE 2025挑战赛任务4的提交系统，通过加入额外音频特征和标签校正系统，提升了音频分类性能。


<details>
  <summary>Details</summary>
Motivation: 混合音频中的细微线索难以仅通过梅尔频谱捕捉，因此需要额外特征提供多视角信息。

Method: 结合梅尔频谱特征与额外音频特征（如谱滚降和色度特征），并应用基于代理的标签校正系统，优化训练数据集。

Result: 实验显示，该系统相比基线在CA-SDRi指标上提升了14.7%。

Conclusion: 通过多特征融合和标签校正，显著提升了音频分类性能。

Abstract: This technical report presents submission systems for Task 4 of the DCASE
2025 Challenge. This model incorporates additional audio features (spectral
roll-off and chroma features) into the embedding feature extracted from the
mel-spectral feature to im-prove the classification capabilities of an
audio-tagging model in the spatial semantic segmentation of sound scenes (S5)
system. This approach is motivated by the fact that mixed audio often contains
subtle cues that are difficult to capture with mel-spectrograms alone. Thus,
these additional features offer alterna-tive perspectives for the model.
Second, an agent-based label correction system is applied to the outputs
processed by the S5 system. This system reduces false positives, improving the
final class-aware signal-to-distortion ratio improvement (CA-SDRi) metric.
Finally, we refine the training dataset to enhance the classi-fication accuracy
of low-performing classes by removing irrele-vant samples and incorporating
external data. That is, audio mix-tures are generated from a limited number of
data points; thus, even a small number of out-of-class data points could
degrade model performance. The experiments demonstrate that the submit-ted
systems employing these approaches relatively improve CA-SDRi by up to 14.7%
compared to the baseline of DCASE 2025 Challenge Task 4.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [145] [Exploring Adapter Design Tradeoffs for Low Resource Music Generation](https://arxiv.org/abs/2506.21298)
*Atharva Mehta,Shivam Chauhan,Monojit Choudhury*

Main category: cs.SD

TL;DR: 论文研究了在低资源音乐流派中，如何通过参数高效微调（PEFT）技术优化MusicGen和Mustango模型的适配器设计，发现卷积和变压器适配器各有优势，并分析了计算资源与性能的权衡。


<details>
  <summary>Details</summary>
Motivation: 解决大规模音乐生成模型微调时的高计算成本问题，探索适配器设计对模型性能的影响。

Method: 通过实验比较不同架构、位置和大小的适配器在MusicGen和Mustango模型上的表现，分析其对Hindustani Classical和Turkish Makam音乐流派的影响。

Result: 卷积适配器擅长捕捉局部音乐细节，变压器适配器更适合长程依赖；Mustango生成多样性更高但稳定性差，MusicGen训练更快且质量更高。

Conclusion: 适配器设计需根据具体需求选择，中规模适配器在表达力和质量间取得最佳平衡。

Abstract: Fine-tuning large-scale music generation models, such as MusicGen and
Mustango, is a computationally expensive process, often requiring updates to
billions of parameters and, therefore, significant hardware resources.
Parameter-Efficient Fine-Tuning (PEFT) techniques, particularly adapter-based
methods, have emerged as a promising alternative, enabling adaptation with
minimal trainable parameters while preserving model performance. However, the
design choices for adapters, including their architecture, placement, and size,
are numerous, and it is unclear which of these combinations would produce
optimal adapters and why, for a given case of low-resource music genre. In this
paper, we attempt to answer this question by studying various adapter
configurations for two AI music models, MusicGen and Mustango, on two genres:
Hindustani Classical and Turkish Makam music.
  Our findings reveal distinct trade-offs: convolution-based adapters excel in
capturing fine-grained local musical details such as ornamentations and short
melodic phrases, while transformer-based adapters better preserve long-range
dependencies crucial for structured improvisation. Additionally, we analyze
computational resource requirements across different adapter scales,
demonstrating how mid-sized adapters (40M parameters) achieve an optimal
balance between expressivity and quality. Furthermore, we find that Mustango, a
diffusion-based model, generates more diverse outputs with better adherence to
the description in the input prompt while lacking in providing stability in
notes, rhythm alignment, and aesthetics. Also, it is computationally intensive
and requires significantly more time to train. In contrast, autoregressive
models like MusicGen offer faster training and are more efficient, and can
produce better quality output in comparison, but have slightly higher
redundancy in their generations.

</details>


### [146] [Learnable Adaptive Time-Frequency Representation via Differentiable Short-Time Fourier Transform](https://arxiv.org/abs/2506.21440)
*Maxime Leiber,Yosra Marnissi,Axel Barrau,Sylvain Meignen,Laurent Massoulié*

Main category: cs.SD

TL;DR: 提出了一种可微分的STFT方法，通过梯度优化参数，解决了传统STFT参数调优的局限性，并展示了其在提升时频表示和下游任务中的效果。


<details>
  <summary>Details</summary>
Motivation: 传统STFT参数调优依赖手动或启发式方法，效果不佳且计算量大，需要一种更高效的方法。

Method: 提出统一的可微分STFT框架，支持基于梯度的参数优化，并可无缝集成到神经网络中。

Result: 实验证明，该方法能有效提升时频表示质量，并改善下游任务性能。

Conclusion: 可微分STFT为参数优化提供了高效解决方案，具有实际应用潜力。

Abstract: The short-time Fourier transform (STFT) is widely used for analyzing
non-stationary signals. However, its performance is highly sensitive to its
parameters, and manual or heuristic tuning often yields suboptimal results. To
overcome this limitation, we propose a unified differentiable formulation of
the STFT that enables gradient-based optimization of its parameters. This
approach addresses the limitations of traditional STFT parameter tuning
methods, which often rely on computationally intensive discrete searches. It
enables fine-tuning of the time-frequency representation (TFR) based on any
desired criterion. Moreover, our approach integrates seamlessly with neural
networks, allowing joint optimization of the STFT parameters and network
weights. The efficacy of the proposed differentiable STFT in enhancing TFRs and
improving performance in downstream tasks is demonstrated through experiments
on both simulated and real-world data.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [147] [EraRAG: Efficient and Incremental Retrieval Augmented Generation for Growing Corpora](https://arxiv.org/abs/2506.20963)
*Fangyuan Zhang,Zhengjun Huang,Yingli Zhou,Qintian Guo,Zhixun Li,Wensheng Luo,Di Jiang,Yixiang Fang,Xiaofang Zhou*

Main category: cs.IR

TL;DR: EraRAG提出了一种支持动态更新的多层Graph-RAG框架，通过基于超平面的LSH技术高效组织语料库，显著减少更新时间和资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有Graph-RAG方法假设静态语料库，动态更新成本高，限制了其可扩展性。

Method: 利用基于超平面的LSH技术分层组织语料库，支持局部化数据插入，避免全局重构。

Result: 实验显示，EraRAG在更新时间和资源消耗上比现有系统减少一个数量级，同时保持高检索精度。

Conclusion: EraRAG为动态增长的语料库提供了高效的RAG解决方案，平衡了检索效率和适应性。

Abstract: Graph-based Retrieval-Augmented Generation (Graph-RAG) enhances large
language models (LLMs) by structuring retrieval over an external corpus.
However, existing approaches typically assume a static corpus, requiring
expensive full-graph reconstruction whenever new documents arrive, limiting
their scalability in dynamic, evolving environments. To address these
limitations, we introduce EraRAG, a novel multi-layered Graph-RAG framework
that supports efficient and scalable dynamic updates. Our method leverages
hyperplane-based Locality-Sensitive Hashing (LSH) to partition and organize the
original corpus into hierarchical graph structures, enabling efficient and
localized insertions of new data without disrupting the existing topology. The
design eliminates the need for retraining or costly recomputation while
preserving high retrieval accuracy and low latency. Experiments on large-scale
benchmarks demonstrate that EraRag achieves up to an order of magnitude
reduction in update time and token consumption compared to existing Graph-RAG
systems, while providing superior accuracy performance. This work offers a
practical path forward for RAG systems that must operate over continually
growing corpora, bridging the gap between retrieval efficiency and
adaptability. Our code and data are available at
https://github.com/EverM0re/EraRAG-Official.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [148] [On Uniform Weighted Deep Polynomial approximation](https://arxiv.org/abs/2506.21306)
*Kingsley Yeon,Steven B. Damelin*

Main category: math.NA

TL;DR: 该论文提出了一种加权深度多项式逼近方法，用于处理具有不对称行为的函数，优于传统多项式逼近方法。


<details>
  <summary>Details</summary>
Motivation: 传统有理逼近方法对非光滑或奇异函数有效，但多项式逼近仅能实现代数收敛。本文旨在通过加权深度多项式逼近方法提升逼近效率。

Method: 引入一类加权深度多项式逼近器，结合可学习的深度多项式和单侧权重，以捕捉局部非光滑性和全局增长性。

Result: 数值实验表明，该方法在相同参数数量下优于泰勒、切比雪夫和标准深度多项式逼近器。

Conclusion: 通过稳定的图参数化策略优化逼近器，该方法在理论和实践中均表现出色。

Abstract: It is a classical result in rational approximation theory that certain
non-smooth or singular functions, such as $|x|$ and $x^{1/p}$, can be
efficiently approximated using rational functions with root-exponential
convergence in terms of degrees of freedom \cite{Sta, GN}. In contrast,
polynomial approximations admit only algebraic convergence by Jackson's theorem
\cite{Lub2}. Recent work shows that composite polynomial architectures can
recover exponential approximation rates even without smoothness \cite{KY}. In
this work, we introduce and analyze a class of weighted deep polynomial
approximants tailored for functions with asymmetric behavior-growing unbounded
on one side and decaying on the other. By multiplying a learnable deep
polynomial with a one-sided weight, we capture both local non-smoothness and
global growth. We show numerically that this framework outperforms Taylor,
Chebyshev, and standard deep polynomial approximants, even when all use the
same number of parameters. To optimize these approximants in practice, we
propose a stable graph-based parameterization strategy building on \cite{Jar}.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [149] [MegaFold: System-Level Optimizations for Accelerating Protein Structure Prediction Models](https://arxiv.org/abs/2506.20686)
*Hoa La,Ahan Gupta,Alex Morehead,Jianlin Cheng,Minjia Zhang*

Main category: q-bio.BM

TL;DR: MegaFold是一个跨平台系统，旨在加速AlphaFold3（AF3）的训练，通过缓存、高效内核和深度融合技术显著提升性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: AF3等蛋白质结构预测模型在计算和内存需求上成本高昂，限制了其训练的可扩展性。

Method: MegaFold采用提前缓存、Triton内核和深度融合技术优化AF3训练。

Result: 在NVIDIA H200和AMD MI250 GPU上，MegaFold将峰值内存使用降低1.23倍，训练时间提升1.73倍和1.62倍，并支持更长的序列训练。

Conclusion: MegaFold显著提升了AF3的训练效率和可扩展性，代码已开源。

Abstract: Protein structure prediction models such as AlphaFold3 (AF3) push the
frontier of biomolecular modeling by incorporating science-informed
architectural changes to the transformer architecture. However, these advances
come at a steep system cost, introducing: compute- and memory-intensive
operators, 2D attention mechanisms, and retrieval-augmented data pipelines,
which collectively hinder the scalability of AF3 training. In this work, we
present MegaFold, a cross-platform system to accelerate AF3 training. MegaFold
tackles key bottlenecks through ahead-of-time caching to eliminate GPU idle
time from the retrieval-augmented data pipeline, Triton-based kernels for
memory-efficient EvoAttention on heterogeneous devices, and deep fusion for
common and critical small operators in AF3. Evaluation on both NVIDIA H200 and
AMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by
up to 1.23$\times$ and improves per-iteration training time by up-to
1.73$\times$ and 1.62$\times$ respectively. More importantly, MegaFold enables
training on 1.35$\times$ longer sequence lengths compared to PyTorch baselines
without running out-of-memory, significantly improving the scalability of
modern protein folding models. We open source our code at
https://github.com/Supercomputing-System-AI-Lab/MegaFold/.

</details>


### [150] [CovDocker: Benchmarking Covalent Drug Design with Tasks, Datasets, and Solutions](https://arxiv.org/abs/2506.21085)
*Yangzhe Peng,Kaiyuan Gao,Liang He,Yuheng Cong,Haiguang Liu,Kun He,Lijun Wu*

Main category: q-bio.BM

TL;DR: 论文提出了一种名为CovDocker的共价对接基准，用于解决现有方法难以处理共价键形成及其结构变化的问题。


<details>
  <summary>Details</summary>
Motivation: 共价相互作用在药物设计中具有重要价值，但现有对接方法和深度学习模型很少考虑共价键的形成及其结构变化。

Method: 将共价对接过程分解为三个任务：反应位点预测、共价反应预测和共价对接，并采用Uni-Mol和Chemformer等先进模型建立基线性能。

Result: 基准测试能准确预测相互作用位点并建模共价结合中的分子转化，验证了其作为研究框架的严谨性。

Conclusion: CovDocker为共价药物设计提供了数据驱动的研究框架，有望加速选择性共价抑制剂的发现。

Abstract: Molecular docking plays a crucial role in predicting the binding mode of
ligands to target proteins, and covalent interactions, which involve the
formation of a covalent bond between the ligand and the target, are
particularly valuable due to their strong, enduring binding nature. However,
most existing docking methods and deep learning approaches hardly account for
the formation of covalent bonds and the associated structural changes. To
address this gap, we introduce a comprehensive benchmark for covalent docking,
CovDocker, which is designed to better capture the complexities of covalent
binding. We decompose the covalent docking process into three main tasks:
reactive location prediction, covalent reaction prediction, and covalent
docking. By adapting state-of-the-art models, such as Uni-Mol and Chemformer,
we establish baseline performances and demonstrate the effectiveness of the
benchmark in accurately predicting interaction sites and modeling the molecular
transformations involved in covalent binding. These results confirm the role of
the benchmark as a rigorous framework for advancing research in covalent drug
design. It underscores the potential of data-driven approaches to accelerate
the discovery of selective covalent inhibitors and addresses critical
challenges in therapeutic development.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [151] [Rational Miner Behaviour, Protocol Stability, and Time Preference: An Austrian and Game-Theoretic Analysis of Bitcoin's Incentive Environment](https://arxiv.org/abs/2506.20965)
*Craig Steven Wright*

Main category: econ.GN

TL;DR: 论文结合奥地利资本理论和重复博弈论，分析区块链系统中矿工在不同制度条件下的策略行为，发现可变协议会提高时间偏好，破坏长期合作均衡。


<details>
  <summary>Details</summary>
Motivation: 探讨区块链系统中矿工行为如何受协议可变性影响，揭示制度设计对网络稳定性的重要性。

Method: 结合形式博弈论分析和奥地利经济学原理，研究矿工激励变化。

Result: 可变协议导致矿工从生产性投资转向政治寻租和影响力博弈，而固定协议（如比特币）能降低时间偏好，促进战略一致性和网络均衡。

Conclusion: 协议不可变性对恢复战略一致性、企业家信心和可持续网络均衡至关重要。

Abstract: This paper integrates Austrian capital theory with repeated game theory to
examine strategic miner behaviour under different institutional conditions in
blockchain systems. It shows that when protocol rules are mutable, effective
time preference rises, undermining rational long-term planning and cooperative
equilibria. Using formal game-theoretic analysis and Austrian economic
principles, the paper demonstrates how mutable protocols shift miner incentives
from productive investment to political rent-seeking and influence games. The
original Bitcoin protocol is interpreted as an institutional anchor: a fixed
rule-set enabling calculability and low time preference. Drawing on the work of
Bohm-Bawerk, Mises, and Hayek, the argument is made that protocol immutability
is essential for restoring strategic coherence, entrepreneurial confidence, and
sustainable network equilibrium.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [152] [The final solution of the Hitchhiker's problem #5](https://arxiv.org/abs/2506.20672)
*Matjaž Omladič,Martin Vuk,Aljaž Zalar*

Main category: stat.ML

TL;DR: 本文通过解析方法解决了关于多元拟协变量质量分布极值的开放问题，并推翻了相关猜想。


<details>
  <summary>Details</summary>
Motivation: 尽管拟协变量缺乏统计解释，但其在依赖建模社区中的重要性日益增加。本文旨在解决《Hitchhiker's Guide》中的开放问题。

Method: 采用解析方法，解决了线性规划方法在维度限制下的不足。

Result: 完全解决了原始问题，并推翻了最近的一个猜想。

Conclusion: 本文为拟协变量的理论研究提供了重要进展，解决了开放问题。

Abstract: A recent survey, nicknamed "Hitchhiker's Guide", J.J. Arias-Garc{\i}a, R.
Mesiar, and B. De Baets, A hitchhiker's guide to quasi-copulas, Fuzzy Sets and
Systems 393 (2020) 1-28, has raised the rating of quasi-copula problems in the
dependence modeling community in spite of the lack of statistical
interpretation of quasi-copulas. In our previous work (arXiv:2410.19339,
accepted in Fuzzy Sets and Systems), we addressed the question of extreme
values of the mass distribution associated with multivariate quasi-copulas.
Using a linear programming approach, we were able to solve Open Problem 5 of
the "Guide" up to dimension d = 17 and disprove a recent conjecture on the
solution to that problem. In this paper, we use an analytical approach to
provide a complete answer to the original question.

</details>


### [153] [Stable Minima of ReLU Neural Networks Suffer from the Curse of Dimensionality: The Neural Shattering Phenomenon](https://arxiv.org/abs/2506.20779)
*Tongtong Liang,Dan Qiao,Yu-Xiang Wang,Rahul Parhi*

Main category: stat.ML

TL;DR: 本文研究了平坦性/低曲率在两层过参数化ReLU网络中的隐式偏差及其对泛化的影响，发现平坦解在高维输入下泛化性能会指数级下降。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于梯度下降训练中的极小值稳定性和边缘稳定性现象，现有工作仅关注单变量输入或需要插值条件。

Method: 针对多变量输入，分析了平坦解在两种自然设置下的泛化性能：(1)平坦解的泛化差距，(2)稳定极小值在非参数函数估计中的均方误差。

Result: 理论结果表明，平坦解虽然能泛化，但其收敛速率随输入维度增加而指数级下降，与低范数解形成鲜明对比。

Conclusion: 平坦解在高维下可能因神经元激活稀疏但权重高而性能不佳，首次系统解释了平坦极小值在高维中泛化失败的原因。

Abstract: We study the implicit bias of flatness / low (loss) curvature and its effects
on generalization in two-layer overparameterized ReLU networks with
multivariate inputs -- a problem well motivated by the minima stability and
edge-of-stability phenomena in gradient-descent training. Existing work either
requires interpolation or focuses only on univariate inputs. This paper
presents new and somewhat surprising theoretical results for multivariate
inputs. On two natural settings (1) generalization gap for flat solutions, and
(2) mean-squared error (MSE) in nonparametric function estimation by stable
minima, we prove upper and lower bounds, which establish that while flatness
does imply generalization, the resulting rates of convergence necessarily
deteriorate exponentially as the input dimension grows. This gives an
exponential separation between the flat solutions vis-\`a-vis low-norm
solutions (i.e., weight decay), which knowingly do not suffer from the curse of
dimensionality. In particular, our minimax lower bound construction, based on a
novel packing argument with boundary-localized ReLU neurons, reveals how flat
solutions can exploit a kind of ''neural shattering'' where neurons rarely
activate, but with high weight magnitudes. This leads to poor performance in
high dimensions. We corroborate these theoretical findings with extensive
numerical simulations. To the best of our knowledge, our analysis provides the
first systematic explanation for why flat minima may fail to generalize in high
dimensions.

</details>


### [154] [Active Learning for Manifold Gaussian Process Regression](https://arxiv.org/abs/2506.20928)
*Yuanxing Cheng,Lulu Kang,Yiwei Wang,Chun Liu*

Main category: stat.ML

TL;DR: 提出了一种结合流形学习和主动学习的高斯过程回归框架，通过优化降维网络和潜空间回归器提升高维数据准确性。


<details>
  <summary>Details</summary>
Motivation: 解决高维空间中复杂、不连续函数的回归问题，提升预测精度。

Method: 联合优化降维神经网络和潜空间高斯过程回归器，采用主动学习准则最小化全局预测误差。

Result: 在合成数据上表现优于随机顺序学习，高效处理复杂函数且保持计算可行性。

Conclusion: 框架具有科学和工程应用价值，未来将关注可扩展性和不确定性感知的流形学习。

Abstract: This paper introduces an active learning framework for manifold Gaussian
Process (GP) regression, combining manifold learning with strategic data
selection to improve accuracy in high-dimensional spaces. Our method jointly
optimizes a neural network for dimensionality reduction and a Gaussian process
regressor in the latent space, supervised by an active learning criterion that
minimizes global prediction error. Experiments on synthetic data demonstrate
superior performance over randomly sequential learning. The framework
efficiently handles complex, discontinuous functions while preserving
computational tractability, offering practical value for scientific and
engineering applications. Future work will focus on scalability and
uncertainty-aware manifold learning.

</details>


### [155] [Lower Bounds on the Size of Markov Equivalence Classes](https://arxiv.org/abs/2506.20933)
*Erik Jahn,Frederick Eberhardt,Leonard J. Schulman*

Main category: stat.ML

TL;DR: 论文研究了因果发现算法中马尔可夫等价类的规模问题，发现当放宽无环性、因果充分性或均匀模型先验假设时，等价类规模会指数级增长。


<details>
  <summary>Details</summary>
Motivation: 探索因果发现算法在放宽假设条件下马尔可夫等价类规模的变化，揭示其理论限制。

Method: 通过理论分析，证明了在稀疏随机有向无环图、均匀随机无环混合图和均匀随机有向循环图中，马尔可夫等价类的期望规模呈指数级增长。

Result: 在三种放宽假设的场景下，马尔可夫等价类的规模均呈指数级增长。

Conclusion: 放宽假设会导致马尔可夫等价类规模显著增大，表明因果发现算法的局限性在更广泛条件下依然存在。

Abstract: Causal discovery algorithms typically recover causal graphs only up to their
Markov equivalence classes unless additional parametric assumptions are made.
The sizes of these equivalence classes reflect the limits of what can be
learned about the underlying causal graph from purely observational data. Under
the assumptions of acyclicity, causal sufficiency, and a uniform model prior,
Markov equivalence classes are known to be small on average. In this paper, we
show that this is no longer the case when any of these assumptions is relaxed.
Specifically, we prove exponentially large lower bounds for the expected size
of Markov equivalence classes in three settings: sparse random directed acyclic
graphs, uniformly random acyclic directed mixed graphs, and uniformly random
directed cyclic graphs.

</details>


### [156] [Forecasting Geopolitical Events with a Sparse Temporal Fusion Transformer and Gaussian Process Hybrid: A Case Study in Middle Eastern and U.S. Conflict Dynamics](https://arxiv.org/abs/2506.20935)
*Hsin-Hsiung Huang,Hayden Hampton*

Main category: stat.ML

TL;DR: STFT-VNNGP是一种混合架构，结合TFT和VNNGP，解决了地缘政治冲突预测中数据稀疏性和突发性问题，在2023年ATD竞赛中获胜。


<details>
  <summary>Details</summary>
Motivation: 地缘政治冲突预测对国家安防至关重要，但现有深度学习模型（如TFT）在稀疏、突发和过度分散的数据上表现不可靠。

Method: 采用两阶段方法：TFT捕捉复杂时间动态生成多分位数预测，VNNGP进行时空平滑和不确定性量化。

Result: 在中东和美国的冲突动态预测中，STFT-VNNGP表现优于单独TFT，尤其在长期预测突发事件的时机和规模上。

Conclusion: STFT-VNNGP为挑战性事件数据提供了更可靠的预测框架，代码公开以确保可重复性。

Abstract: Forecasting geopolitical conflict from data sources like the Global Database
of Events, Language, and Tone (GDELT) is a critical challenge for national
security. The inherent sparsity, burstiness, and overdispersion of such data
cause standard deep learning models, including the Temporal Fusion Transformer
(TFT), to produce unreliable long-horizon predictions. We introduce STFT-VNNGP,
a hybrid architecture that won the 2023 Algorithms for Threat Detection (ATD)
competition by overcoming these limitations. Designed to bridge this gap, our
model employs a two-stage process: first, a TFT captures complex temporal
dynamics to generate multi-quantile forecasts. These quantiles then serve as
informed inputs for a Variational Nearest Neighbor Gaussian Process (VNNGP),
which performs principled spatiotemporal smoothing and uncertainty
quantification. In a case study forecasting conflict dynamics in the Middle
East and the U.S., STFT-VNNGP consistently outperforms a standalone TFT,
showing a superior ability to predict the timing and magnitude of bursty event
periods, particularly at long-range horizons. This work offers a robust
framework for generating more reliable and actionable intelligence from
challenging event data, with all code and workflows made publicly available to
ensure reproducibility.

</details>


### [157] [Homogenization of Multi-agent Learning Dynamics in Finite-state Markov Games](https://arxiv.org/abs/2506.21079)
*Yann Kerzreho*

Main category: stat.ML

TL;DR: 本文提出了一种新方法，通过调整学习率和更新频率来近似多智能体强化学习在有限状态马尔可夫博弈中的动态。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体强化学习在复杂环境中的动态行为，提供一个可处理的确定性近似方法。

Method: 通过同时减小学习率和增加更新频率，将智能体参数视为慢变量，受快速混合的游戏状态影响。

Result: 证明了在温和假设下，该方法收敛到一个常微分方程（ODE），为学习动态提供了确定性近似。

Conclusion: 该方法为多智能体强化学习的动态分析提供了有效的工具，代码已开源。

Abstract: This paper introduces a new approach for approximating the learning dynamics
of multiple reinforcement learning (RL) agents interacting in a finite-state
Markov game. The idea is to rescale the learning process by simultaneously
reducing the learning rate and increasing the update frequency, effectively
treating the agent's parameters as a slow-evolving variable influenced by the
fast-mixing game state. Under mild assumptions-ergodicity of the state process
and continuity of the updates-we prove the convergence of this rescaled process
to an ordinary differential equation (ODE). This ODE provides a tractable,
deterministic approximation of the agent's learning dynamics. An implementation
of the framework is available at\,:
https://github.com/yannKerzreho/MarkovGameApproximation

</details>


### [158] [Hyperspherical Variational Autoencoders Using Efficient Spherical Cauchy Distribution](https://arxiv.org/abs/2506.21278)
*Lukas Sablica,Kurt Hornik*

Main category: stat.ML

TL;DR: 提出了一种基于球形柯西（spCauchy）潜在分布的变分自编码器（VAE）架构，优于传统高斯或vMF分布，提供更自然的超球面表示，避免数值不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 传统高斯或vMF分布在捕捉方向性数据时存在局限性，且vMF因涉及贝塞尔函数计算而数值不稳定。spCauchy能更好地表示方向性数据，同时避免这些问题。

Method: 采用spCauchy潜在分布，利用Möbius变换实现高效重参数化，并通过快速收敛的幂级数计算KL散度。

Result: spCauchy提供了更灵活且高效的潜在空间表示，避免了数值不稳定问题，提升了高维生成模型的性能。

Conclusion: spCauchy是VAE的一种理论优越且实际高效的替代方案，适用于高维生成建模。

Abstract: We propose a novel variational autoencoder (VAE) architecture that employs a
spherical Cauchy (spCauchy) latent distribution. Unlike traditional Gaussian
latent spaces or the widely used von Mises-Fisher (vMF) distribution, spCauchy
provides a more natural hyperspherical representation of latent variables,
better capturing directional data while maintaining flexibility. Its
heavy-tailed nature prevents over-regularization, ensuring efficient latent
space utilization while offering a more expressive representation.
Additionally, spCauchy circumvents the numerical instabilities inherent to vMF,
which arise from computing normalization constants involving Bessel functions.
Instead, it enables a fully differentiable and efficient reparameterization
trick via M\"obius transformations, allowing for stable and scalable training.
The KL divergence can be computed through a rapidly converging power series,
eliminating concerns of underflow or overflow associated with evaluation of
ratios of hypergeometric functions. These properties make spCauchy a compelling
alternative for VAEs, offering both theoretical advantages and practical
efficiency in high-dimensional generative modeling.

</details>


### [159] [Wild refitting for black box prediction](https://arxiv.org/abs/2506.21460)
*Martin J. Wainwright*

Main category: stat.ML

TL;DR: 提出了一种高效的重拟合方法，用于计算惩罚非参数估计的实例均方预测误差的高概率上界。


<details>
  <summary>Details</summary>
Motivation: 解决在单一数据集和黑盒预测方法下，高效计算预测误差上界的问题。

Method: 通过计算残差、对称化和缩放残差（使用预因子ρ），并定义和求解以当前估计为中心的修正预测问题。

Result: 在噪声异质性条件下，该方法能高概率地提供预测误差的上界。

Conclusion: 该方法适用于多种问题，如非刚性结构恢复、图像修复和核方法随机草图。

Abstract: We describe and analyze a computionally efficient refitting procedure for
computing high-probability upper bounds on the instance-wise mean-squared
prediction error of penalized nonparametric estimates based on least-squares
minimization. Requiring only a single dataset and black box access to the
prediction method, it consists of three steps: computing suitable residuals,
symmetrizing and scaling them with a pre-factor $\rho$, and using them to
define and solve a modified prediction problem recentered at the current
estimate. We refer to it as wild refitting, since it uses Rademacher residual
symmetrization as in a wild bootstrap variant. Under relatively mild conditions
allowing for noise heterogeneity, we establish a high probability guarantee on
its performance, showing that the wild refit with a suitably chosen wild noise
scale $\rho$ gives an upper bound on prediction error. This theoretical
analysis provides guidance into the design of such procedures, including how
the residuals should be formed, the amount of noise rescaling in the wild
sub-problem needed for upper bounds, and the local stability properties of the
block-box procedure. We illustrate the applicability of this procedure to
various problems, including non-rigid structure-from-motion recovery with
structured matrix penalties; plug-and-play image restoration with deep neural
network priors; and randomized sketching with kernel methods.

</details>


### [160] [Gaussian Invariant Markov Chain Monte Carlo](https://arxiv.org/abs/2506.21511)
*Michalis K. Titsias,Angelos Alexopoulos,Siran Liu,Petros Dellaportas*

Main category: stat.ML

TL;DR: 论文提出了基于高斯不变性的采样方法，包括RWM、MALA和二阶Hessian或流形MALA的变体，展示了其在统计效率上的优势。


<details>
  <summary>Details</summary>
Motivation: 传统RWM和MALA方法在统计效率上存在不足，而高斯不变性为解决这一问题提供了新思路。

Method: 开发了高斯不变版本的RWM、MALA和二阶Hessian或流形MALA，利用高斯不变性解析泊松方程，构建高效控制变量。

Result: 新方法在高维潜在高斯模型中表现优异，达到先进水平，并提供了几何遍历性和最优缩放的理论分析。

Conclusion: 高斯不变性采样方法显著提升了统计效率，适用于复杂目标分布，具有理论和实际应用价值。

Abstract: We develop sampling methods, which consist of Gaussian invariant versions of
random walk Metropolis (RWM), Metropolis adjusted Langevin algorithm (MALA) and
second order Hessian or Manifold MALA. Unlike standard RWM and MALA we show
that Gaussian invariant sampling can lead to ergodic estimators with improved
statistical efficiency. This is due to a remarkable property of Gaussian
invariance that allows us to obtain exact analytical solutions to the Poisson
equation for Gaussian targets. These solutions can be used to construct
efficient and easy to use control variates for variance reduction of estimators
under any intractable target. We demonstrate the new samplers and estimators in
several examples, including high dimensional targets in latent Gaussian models
where we compare against several advanced methods and obtain state-of-the-art
results. We also provide theoretical results regarding geometric ergodicity,
and an optimal scaling analysis that shows the dependence of the optimal
acceptance rate on the Gaussianity of the target.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [161] [Control and optimization for Neural Partial Differential Equations in Supervised Learning](https://arxiv.org/abs/2506.20764)
*Alain Bensoussan,Minh-Binh Tran,Bangjie Wang*

Main category: math.OC

TL;DR: 本文提出了一种新的视角，将神经网络解释为偏微分方程（PDEs），并将控制问题从常微分方程（ODEs）重新表述为针对抛物型和双曲型算子系数的PDE控制问题。


<details>
  <summary>Details</summary>
Motivation: 现有文献对抛物型和双曲型系统的控制和优化问题已有大量研究，但对相关算子系数的控制和优化问题尚未深入探讨。本文旨在填补这一空白，特别是在神经网络和监督学习的背景下。

Method: 提出了一种双系统公式，用于抛物型PDE的控制和优化问题，并提供了理论证明，表明该问题存在极小值。此外，研究了双曲型PDE的控制问题，并证明了近似控制问题的解的存在性。

Result: 证明了抛物型PDE的控制和优化问题存在极小值，并展示了双曲型PDE近似控制问题的解的存在性。

Conclusion: 本文为PDE控制理论中的新研究方向奠定了基础，特别是在神经网络和监督学习的应用中，为未来高效数值方案的发展提供了理论支持。

Abstract: Although there is a substantial body of literature on control and
optimization problems for parabolic and hyperbolic systems, the specific
problem of controlling and optimizing the coefficients of the associated
operators within such systems has not yet been thoroughly explored. In this
work, we aim to initiate a line of research in control theory focused on
optimizing and controlling the coefficients of these operators-a problem that
naturally arises in the context of neural networks and supervised learning.
  In supervised learning, the primary objective is to transport initial data
toward target data through the layers of a neural network. We propose a novel
perspective: neural networks can be interpreted as partial differential
equations (PDEs). From this viewpoint, the control problem traditionally
studied in the context of ordinary differential equations (ODEs) is
reformulated as a control problem for PDEs, specifically targeting the
optimization and control of coefficients in parabolic and hyperbolic operators.
To the best of our knowledge, this specific problem has not yet been
systematically addressed in the control theory of PDEs.
  To this end, we propose a dual system formulation for the control and
optimization problem associated with parabolic PDEs, laying the groundwork for
the development of efficient numerical schemes in future research. We also
provide a theoretical proof showing that the control and optimization problem
for parabolic PDEs admits minimizers. Finally, we investigate the control
problem associated with hyperbolic PDEs and prove the existence of solutions
for a corresponding approximated control problem.

</details>


### [162] [Faster Fixed-Point Methods for Multichain MDPs](https://arxiv.org/abs/2506.20910)
*Matthew Zurek,Yudong Chen*

Main category: math.OC

TL;DR: 研究多链MDPs的平均奖励问题，提出改进的VI算法以解决导航子问题，提升收敛速度和复杂度度量。


<details>
  <summary>Details</summary>
Motivation: 多链MDPs的平均奖励问题因缺乏收缩性和解的非唯一性而具有挑战性，需同时解决导航子问题和优化长期性能。

Method: 开发改进的VI算法，结合折扣问题和平均奖励问题的新联系，扩展Banach空间中的固定点方法。

Result: 获得更快的收敛速度和更精确的复杂度度量，提升了折扣和平均奖励问题的性能。

Conclusion: 改进的VI算法在多链MDPs中表现更优，扩展了VI方法的理论基础。

Abstract: We study value-iteration (VI) algorithms for solving general (a.k.a.
multichain) Markov decision processes (MDPs) under the average-reward
criterion, a fundamental but theoretically challenging setting. Beyond the
difficulties inherent to all average-reward problems posed by the lack of
contractivity and non-uniqueness of solutions to the Bellman operator, in the
multichain setting an optimal policy must solve the navigation subproblem of
steering towards the best connected component, in addition to optimizing
long-run performance within each component. We develop algorithms which better
solve this navigational subproblem in order to achieve faster convergence for
multichain MDPs, obtaining improved rates of convergence and sharper measures
of complexity relative to prior work. Many key components of our results are of
potential independent interest, including novel connections between
average-reward and discounted problems, optimal fixed-point methods for
discounted VI which extend to general Banach spaces, new sublinear convergence
rates for the discounted value error, and refined suboptimality decompositions
for multichain MDPs. Overall our results yield faster convergence rates for
discounted and average-reward problems and expand the theoretical foundations
of VI approaches.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [163] [Uncertainty-Aware Machine-Learning Framework for Predicting Dislocation Plasticity and Stress-Strain Response in FCC Alloys](https://arxiv.org/abs/2506.20839)
*Jing Luo,Yejun Gu,Yanfei Wang,Xiaolong Ma,Jaafar. A El-Awady*

Main category: cond-mat.mtrl-sci

TL;DR: 本文提出了一种基于混合密度网络（MDN）的方法，通过整合文献中的实验数据，预测位错密度和应力分布，并结合统计参数改进塑性模型，实现高精度和不确定性量化的应力-应变预测。


<details>
  <summary>Details</summary>
Motivation: 机器学习在结构材料领域的应用日益重要，但现有数据整合和预测模型中的不确定性量化仍需改进。本研究旨在通过新方法提升机械性能预测的准确性和可靠性。

Method: 使用混合密度网络（MDN）模型，基于大量实验数据预测位错密度（作为潜变量）和应力分布，并将统计参数整合到位错介导的塑性模型中。

Result: 该方法能够高精度预测应力-应变关系，并明确量化不确定性，显著提升了机械性能预测的可靠性。

Conclusion: 该策略不仅优化了合金设计，还为快速发展的材料行业提供了新材料的开发支持。

Abstract: Machine learning has significantly advanced the understanding and application
of structural materials, with an increasing emphasis on integrating existing
data and quantifying uncertainties in predictive modeling. This study presents
a comprehensive methodology utilizing a mixed density network (MDN) model,
trained on extensive experimental data from literature. This approach uniquely
predicts the distribution of dislocation density, inferred as a latent
variable, and the resulting stress distribution at the grain level. The
incorporation of statistical parameters of those predicted distributions into a
dislocation-mediated plasticity model allows for accurate stress-strain
predictions with explicit uncertainty quantification. This strategy not only
improves the accuracy and reliability of mechanical property predictions but
also plays a vital role in optimizing alloy design, thereby facilitating the
development of new materials in a rapidly evolving industry.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [164] [Benchmarking and Parallelization of Electrostatic Particle-In-Cell for low-temperature Plasma Simulation by particle-thread Binding](https://arxiv.org/abs/2506.21524)
*Libn Varghese,Bhaskar Chaudhury,Miral Shah,Mainak Bandyopadhyay*

Main category: physics.comp-ph

TL;DR: 提出了一种基于粒子-线程绑定策略的新方法，显著提升了PIC模拟中电荷沉积（CD）子程序的可扩展性和性能，同时保持传统数据结构。


<details>
  <summary>Details</summary>
Motivation: 解决2D和3D设备级PIC模拟中电荷沉积子程序因频繁粒子-网格交互成为瓶颈的问题，传统方法因生成私有网格而面临可扩展性挑战。

Method: 采用粒子-线程绑定策略，仅需每个节点四个私有网格（分布式内存系统）或四个私有网格（共享内存系统），通过附加功能和标志避免线程冲突。

Result: 在共享内存和分布式内存系统（1000核）上的性能评估显示方法具有高可扩展性和低硬件依赖性。

Conclusion: 该方法显著提升PIC模拟的可扩展性和性能，且对现有代码改动极小。

Abstract: The Particle-In-Cell (PIC) method for plasma simulation tracks particle phase
space information using particle and grid data structures. High computational
costs in 2D and 3D device-scale PIC simulations necessitate parallelization,
with the Charge Deposition (CD) subroutine often becoming a bottleneck due to
frequent particle-grid interactions. Conventional methods mitigate dependencies
by generating private grids for each core, but this approach faces scalability
issues. We propose a novel approach based on a particle-thread binding strategy
that requires only four private grids per node in distributed memory systems or
four private grids in shared memory systems, enhancing CD scalability and
performance while maintaining conventional data structures and requiring
minimal changes to existing PIC codes. This method ensures complete
accessibility of grid data structure for concurrent threads and avoids
simultaneous access to particles within the same cell using additional
functions and flags. Performance evaluations using a PIC benchmark for
low-temperature partially magnetized E x B discharge simulation on a shared
memory as well as a distributed memory system (1000 cores) demonstrate the
method's scalability, and additionally, we show the method has little hardware
dependency.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [165] [IXAII: An Interactive Explainable Artificial Intelligence Interface for Decision Support Systems](https://arxiv.org/abs/2506.21310)
*Pauline Speckmann,Mario Nadj,Christian Janiesch*

Main category: cs.AI

TL;DR: IXAII是一个交互式可解释AI系统，整合了LIME、SHAP、Anchors和DiCE四种方法，针对不同用户群体提供定制化解释，提升透明度和用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI方法多为静态且忽视用户视角，限制了其实际效果。

Method: 开发IXAII系统，结合四种可解释AI方法，提供交互式解释和多种可视化选项。

Result: 专家和普通用户评价显示IXAII能有效提升透明度。

Conclusion: IXAII为可解释AI实践和人机交互提供了新视角。

Abstract: Although several post-hoc methods for explainable AI have been developed,
most are static and neglect the user perspective, limiting their effectiveness
for the target audience. In response, we developed the interactive explainable
intelligent system called IXAII that offers explanations from four explainable
AI methods: LIME, SHAP, Anchors, and DiCE. Our prototype provides tailored
views for five user groups and gives users agency over the explanations'
content and their format. We evaluated IXAII through interviews with experts
and lay users. Our results indicate that IXAII, which provides different
explanations with multiple visualization options, is perceived as helpful to
increase transparency. By bridging the gaps between explainable AI methods,
interactivity, and practical implementation, we provide a novel perspective on
AI explanation practices and human-AI interaction.

</details>


### [166] [Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?](https://arxiv.org/abs/2506.21215)
*Haoang Chi,He Li,Wenjing Yang,Feng Liu,Long Lan,Xiaoguang Ren,Tongliang Liu,Bo Han*

Main category: cs.AI

TL;DR: 论文探讨了大语言模型（LLMs）在因果推理能力上的局限性，提出了一种新方法G^2-Reasoner以提升其能力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示LLMs在因果推理上的不足，并探索如何提升其能力以接近人类水平。

Method: 通过分析LLMs的自回归机制，提出新基准CausalProbe-2024，并设计G^2-Reasoner方法结合通用知识和目标导向提示。

Result: 实验表明G^2-Reasoner显著提升了LLMs在新鲜和反事实情境中的因果推理能力。

Conclusion: G^2-Reasoner为LLMs实现更高级别的因果推理提供了新路径。

Abstract: Causal reasoning capability is critical in advancing large language models
(LLMs) toward strong artificial intelligence. While versatile LLMs appear to
have demonstrated capabilities in understanding contextual causality and
providing responses that obey the laws of causality, it remains unclear whether
they perform genuine causal reasoning akin to humans. However, current evidence
indicates the contrary. Specifically, LLMs are only capable of performing
shallow (level-1) causal reasoning, primarily attributed to the causal
knowledge embedded in their parameters, but they lack the capacity for genuine
human-like (level-2) causal reasoning. To support this hypothesis,
methodologically, we delve into the autoregression mechanism of
transformer-based LLMs, revealing that it is not inherently causal.
Empirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,
whose corpora are fresh and nearly unseen for the studied LLMs. The LLMs
exhibit a significant performance drop on CausalProbe-2024 compared to earlier
benchmarks, indicating the fact that they primarily engage in level-1 causal
reasoning. To bridge the gap towards level-2 causal reasoning, we draw
inspiration from the fact that human reasoning is usually facilitated by
general knowledge and intended goals. We propose G^2-Reasoner, a method that
incorporates general knowledge and goal-oriented prompts into LLMs' causal
reasoning processes. Experiments demonstrate that G^2-Reasoner significantly
enhances LLMs' causal reasoning capability, particularly in fresh and
counterfactual contexts. This work sheds light on a new path for LLMs to
advance towards genuine causal reasoning, going beyond level-1 and making
strides towards level-2.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [167] [Quantum Reinforcement Learning Trading Agent for Sector Rotation in the Taiwan Stock Market](https://arxiv.org/abs/2506.20930)
*Chi-Sheng Chen,Xinyu Zhang,Ya-Chuan Chen*

Main category: quant-ph

TL;DR: 提出了一种混合量子-经典强化学习框架，用于台湾股市的行业轮动。结合PPO算法与经典（LSTM、Transformer）和量子增强模型（QNN、QRWKV、QASA），发现量子模型在训练奖励上表现更好，但在实际投资指标（如累计收益和夏普比率）上不如经典模型。


<details>
  <summary>Details</summary>
Motivation: 探索量子增强模型在金融强化学习中的应用，解决行业轮动问题。

Method: 使用PPO算法，结合经典和量子模型作为策略和价值网络，自动化特征工程提取金融指标。

Result: 量子模型训练奖励高，但实际投资表现不如经典模型，揭示了奖励信号与真实目标的不匹配问题。

Conclusion: 当前奖励设计可能导致过拟合短期波动，量子电路在NISQ约束下的不稳定性加剧了问题。未来需改进奖励设计、模型正则化和验证方法。

Abstract: We propose a hybrid quantum-classical reinforcement learning framework for
sector rotation in the Taiwan stock market. Our system employs Proximal Policy
Optimization (PPO) as the backbone algorithm and integrates both classical
architectures (LSTM, Transformer) and quantum-enhanced models (QNN, QRWKV,
QASA) as policy and value networks. An automated feature engineering pipeline
extracts financial indicators from capital share data to ensure consistent
model input across all configurations. Empirical backtesting reveals a key
finding: although quantum-enhanced models consistently achieve higher training
rewards, they underperform classical models in real-world investment metrics
such as cumulative return and Sharpe ratio. This discrepancy highlights a core
challenge in applying reinforcement learning to financial domains -- namely,
the mismatch between proxy reward signals and true investment objectives. Our
analysis suggests that current reward designs may incentivize overfitting to
short-term volatility rather than optimizing risk-adjusted returns. This issue
is compounded by the inherent expressiveness and optimization instability of
quantum circuits under Noisy Intermediate-Scale Quantum (NISQ) constraints. We
discuss the implications of this reward-performance gap and propose directions
for future improvement, including reward shaping, model regularization, and
validation-based early stopping. Our work offers a reproducible benchmark and
critical insights into the practical challenges of deploying quantum
reinforcement learning in real-world finance.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [168] [U-R-VEDA: Integrating UNET, Residual Links, Edge and Dual Attention, and Vision Transformer for Accurate Semantic Segmentation of CMRs](https://arxiv.org/abs/2506.20689)
*Racheal Mukisa,Arvind K. Bansal*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的增强UNet模型U-R-Veda，用于心脏磁共振图像的自动语义分割，结合了多种技术以提高准确性。


<details>
  <summary>Details</summary>
Motivation: 自动化心脏图像分割是心脏疾病诊断和管理的关键步骤，现有方法在准确性和信息保留方面存在不足。

Method: U-R-Veda整合了卷积变换、视觉变换器、残差链接、通道和空间注意力，以及基于边缘检测的跳跃连接。

Result: 模型在DSC指标上平均准确率达到95.2%，优于其他模型，尤其在右心室和左心室心肌的分割上表现突出。

Conclusion: U-R-Veda显著提升了心脏磁共振图像的语义分割效果，为医学图像分析提供了更准确的工具。

Abstract: Artificial intelligence, including deep learning models, will play a
transformative role in automated medical image analysis for the diagnosis of
cardiac disorders and their management. Automated accurate delineation of
cardiac images is the first necessary initial step for the quantification and
automated diagnosis of cardiac disorders. In this paper, we propose a deep
learning based enhanced UNet model, U-R-Veda, which integrates convolution
transformations, vision transformer, residual links, channel-attention, and
spatial attention, together with edge-detection based skip-connections for an
accurate fully-automated semantic segmentation of cardiac magnetic resonance
(CMR) images. The model extracts local-features and their interrelationships
using a stack of combination convolution blocks, with embedded channel and
spatial attention in the convolution block, and vision transformers. Deep
embedding of channel and spatial attention in the convolution block identifies
important features and their spatial localization. The combined edge
information with channel and spatial attention as skip connection reduces
information-loss during convolution transformations. The overall model
significantly improves the semantic segmentation of CMR images necessary for
improved medical image analysis. An algorithm for the dual attention module
(channel and spatial attention) has been presented. Performance results show
that U-R-Veda achieves an average accuracy of 95.2%, based on DSC metrics. The
model outperforms the accuracy attained by other models, based on DSC and HD
metrics, especially for the delineation of right-ventricle and
left-ventricle-myocardium.

</details>


### [169] [Exploring the Design Space of 3D MLLMs for CT Report Generation](https://arxiv.org/abs/2506.21535)
*Mohammed Baharoon,Jun Ma,Congyu Fang,Augustin Toma,Bo Wang*

Main category: eess.IV

TL;DR: 本文系统研究了3D多模态大语言模型（MLLMs）在放射学报告生成（RRG）中的设计空间，包括视觉输入表示、投影器、大语言模型（LLMs）和微调技术，并提出了两种基于知识的报告增强方法，性能提升达10%。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用3D MLLMs自动化生成放射学报告，并优化其设计以提高性能。

Method: 研究了视觉输入表示、投影器、LLMs和微调技术，并引入了两种知识增强方法。

Result: 在MICCAI 2024 AMOS-MM挑战赛中排名第二，性能提升10%；RRG性能与LLM大小无关；更大的体积尺寸不一定提升性能；使用分割掩码可提升性能。

Conclusion: 3D MLLMs在RRG中具有潜力，设计选择和知识增强方法显著提升性能，代码已开源。

Abstract: Multimodal Large Language Models (MLLMs) have emerged as a promising way to
automate Radiology Report Generation (RRG). In this work, we systematically
investigate the design space of 3D MLLMs, including visual input
representation, projectors, Large Language Models (LLMs), and fine-tuning
techniques for 3D CT report generation. We also introduce two knowledge-based
report augmentation methods that improve performance on the GREEN score by up
to 10\%, achieving the 2nd place on the MICCAI 2024 AMOS-MM challenge. Our
results on the 1,687 cases from the AMOS-MM dataset show that RRG is largely
independent of the size of LLM under the same training protocol. We also show
that larger volume size does not always improve performance if the original ViT
was pre-trained on a smaller volume size. Lastly, we show that using a
segmentation mask along with the CT volume improves performance. The code is
publicly available at https://github.com/bowang-lab/AMOS-MM-Solution

</details>
