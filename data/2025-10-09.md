<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.SE](#cs.SE) [Total: 11]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.LG](#cs.LG) [Total: 101]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [quant-ph](#quant-ph) [Total: 14]
- [q-bio.TO](#q-bio.TO) [Total: 1]
- [math.CO](#math.CO) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.AI](#cs.AI) [Total: 8]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [econ.EM](#econ.EM) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.CL](#cs.CL) [Total: 18]
- [eess.IV](#eess.IV) [Total: 3]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.CR](#cs.CR) [Total: 6]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.CV](#cs.CV) [Total: 14]
- [hep-ph](#hep-ph) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.IT](#cs.IT) [Total: 2]
- [stat.ML](#stat.ML) [Total: 10]
- [physics.ao-ph](#physics.ao-ph) [Total: 2]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Bridging Imperative Process Models and Process Data Queries-Translation and Relaxation](https://arxiv.org/abs/2510.06414)
*Abdur Rehman Anwar Qureshi,Adrian Rebmann,Timotheus Kampik,Matthias Weidlich,Mathias Weske*

Main category: cs.DB

TL;DR: 将命令式流程模型转换为可在关系数据库上执行的SQL查询，用于一致性检查，弥合传统流程建模与数据驱动流程分析之间的差距。


<details>
  <summary>Details</summary>
Motivation: 解决命令式流程模型（如Petri网）与关系数据库中结构化流程执行数据之间的不匹配问题，避免现有流程模型未被充分利用的情况。

Method: 提供将命令式模型转换为宽松流程数据查询（特别是SQL查询）的方法，基于行为足迹和其他声明性方法。

Result: 展示了命令式流程模型在数据驱动流程管理中的持续相关性，以及行为足迹在整合基于模型和数据驱动的流程管理中的重要性。

Conclusion: 成功弥合了传统流程建模与数据驱动流程分析之间的差距，证明了命令式模型在数据驱动流程管理中的价值。

Abstract: Business process management is increasingly practiced using data-driven
approaches. Still, classical imperative process models, which are typically
formalized using Petri nets, are not straightforwardly applicable to the
relational databases that contain much of the available structured process
execution data. This creates a gap between the traditional world of process
modeling and recent developments around data-driven process analysis,
ultimately leading to the under-utilization of often readily available process
models. In this paper, we close this gap by providing an approach for
translating imperative models into relaxed process data queries, specifically
SQL queries executable on relational databases, for conformance checking. Our
results show the continued relevance of imperative process models to
data-driven process management, as well as the importance of behavioral
footprints and other declarative approaches for integrating model-based and
data-driven process management.

</details>


### [2] [Automated Discovery of Test Oracles for Database Management Systems Using LLMs](https://arxiv.org/abs/2510.06663)
*Qiuyang Mang,Runyuan He,Suyang Zhong,Xiaoxuan Liu,Huanchen Zhang,Alvin Cheung*

Main category: cs.DB

TL;DR: 本文提出Argus框架，利用大语言模型自动发现和实例化数据库测试预言，通过约束抽象查询和SQL等价性验证，在5个广泛测试的DBMS中发现40个未知bug。


<details>
  <summary>Details</summary>
Motivation: 当前DBMS自动化测试中的测试预言设计需要人工参与，成为完全自动化测试的瓶颈。虽然LLMs具有创造力，但存在幻觉问题且成本高昂，需要限制调用次数。

Method: 引入Argus框架，基于约束抽象查询概念（包含占位符及其实例化条件的SQL骨架），使用LLMs生成语义等价的骨架对，通过SQL等价求解器进行形式化验证，最后用LLMs合成的具体SQL片段实例化占位符。

Result: 在5个广泛测试的DBMS中发现了40个先前未知的bug，其中35个是逻辑bug，36个已确认，26个已被开发者修复。

Conclusion: Argus框架成功利用LLMs自动化测试预言发现和实例化，解决了DBMS完全自动化测试的关键瓶颈，同时通过形式化验证确保可靠性并控制成本。

Abstract: Since 2020, automated testing for Database Management Systems (DBMSs) has
flourished, uncovering hundreds of bugs in widely-used systems. A cornerstone
of these techniques is test oracle, which typically implements a mechanism to
generate equivalent query pairs, thereby identifying bugs by checking the
consistency between their results. However, while applying these oracles can be
automated, their design remains a fundamentally manual endeavor. This paper
explores the use of large language models (LLMs) to automate the discovery and
instantiation of test oracles, addressing a long-standing bottleneck towards
fully automated DBMS testing. Although LLMs demonstrate impressive creativity,
they are prone to hallucinations that can produce numerous false positive bug
reports. Furthermore, their significant monetary cost and latency mean that LLM
invocations should be limited to ensure that bug detection is efficient and
economical.
  To this end, we introduce Argus, a novel framework built upon the core
concept of the Constrained Abstract Query - a SQL skeleton containing
placeholders and their associated instantiation conditions (e.g., requiring a
placeholder to be filled by a boolean column). Argus uses LLMs to generate
pairs of these skeletons that are asserted to be semantically equivalent. This
equivalence is then formally proven using a SQL equivalence solver to ensure
soundness. Finally, the placeholders within the verified skeletons are
instantiated with concrete, reusable SQL snippets that are also synthesized by
LLMs to efficiently produce complex test cases. We implemented Argus and
evaluated it on five extensively tested DBMSs, discovering 40 previously
unknown bugs, 35 of which are logic bugs, with 36 confirmed and 26 already
fixed by the developers.

</details>


### [3] [Relational Database Distillation: From Structured Tables to Condensed Graph Data](https://arxiv.org/abs/2510.06980)
*Xinyi Gao,Jingxi Zhang,Lijian Chen,Tong Chen,Lizhen Cui,Hongzhi Yin*

Main category: cs.DB

TL;DR: 提出关系数据库蒸馏(RDD)方法，将大规模关系数据库压缩为紧凑的异质图，同时保持预测能力，解决现有图表示学习方法存储开销大、训练时间长的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于图表示学习的方法在处理大规模关系数据库时面临存储开销过大和训练时间过长的问题，主要原因是数据库规模庞大和跨表消息传递的计算负担。

Method: 将关系数据库蒸馏为紧凑的异质图，通过节点特征保留多模态列信息，通过异质边编码主外键关系。设计了基于核岭回归和伪标签的目标函数，避免传统的低效双层蒸馏框架。

Result: 在多个真实世界关系数据库上的实验表明，该方法显著减小数据规模，同时在分类和回归任务上保持有竞争力的性能。

Conclusion: 关系数据库蒸馏为可扩展的关系数据库学习提供了有效途径，在保持性能的同时大幅降低数据规模。

Abstract: Relational databases (RDBs) underpin the majority of global data management
systems, where information is structured into multiple interdependent tables.
To effectively use the knowledge within RDBs for predictive tasks, recent
advances leverage graph representation learning to capture complex inter-table
relations as multi-hop dependencies. Despite achieving state-of-the-art
performance, these methods remain hindered by the prohibitive storage overhead
and excessive training time, due to the massive scale of the database and the
computational burden of intensive message passing across interconnected tables.
To alleviate these concerns, we propose and study the problem of Relational
Database Distillation (RDD). Specifically, we aim to distill large-scale RDBs
into compact heterogeneous graphs while retaining the predictive power (i.e.,
utility) required for training graph-based models. Multi-modal column
information is preserved through node features, and primary-foreign key
relations are encoded via heterogeneous edges, thereby maintaining both data
fidelity and relational structure. To ensure adaptability across diverse
downstream tasks without engaging the traditional, inefficient bi-level
distillation framework, we further design a kernel ridge regression-guided
objective with pseudo-labels, which produces quality features for the distilled
graph. Extensive experiments on multiple real-world RDBs demonstrate that our
solution substantially reduces the data size while maintaining competitive
performance on classification and regression tasks, creating an effective
pathway for scalable learning with RDBs.

</details>


### [4] [On the Expressiveness of Languages for Querying Property Graphs in Relational Databases](https://arxiv.org/abs/2510.07062)
*Hadar Rotschield,Liat Peterfreund*

Main category: cs.DB

TL;DR: SQL/PGQ标准在查询属性图时，其表达能力形成严格层次结构：只读片段弱于读写片段，读写片段低于NL复杂度类。扩展视图定义后可精确捕获NL，在有序结构上层次结构会塌陷。


<details>
  <summary>Details</summary>
Motivation: 研究SQL/PGQ标准在不同片段下的表达能力，特别关注图创建操作对表达力的影响，以及视图定义在属性图查询中的核心作用。

Method: 形式化分析SQL/PGQ三个片段的表达能力：只读核心、读写扩展、以及具有更丰富视图定义的扩展变体。

Result: 只读片段严格弱于读写片段，读写片段仍低于NL复杂度类。扩展视图定义后可精确捕获NL，形成严格层次结构，在有序结构上该层次会塌陷。

Conclusion: 图创建在决定SQL/PGQ表达能力中起核心作用，视图构造在属性图查询中具有基础性地位，扩展视图定义可达到完全表达能力。

Abstract: SQL/PGQ is the emerging ISO standard for querying property graphs defined as
views over relational data. We formalize its expressive power across three
fragments: the read-only core, the read-write extension, and an extended
variant with richer view definitions. Our results show that graph creation
plays a central role in determining the expressiveness. The read-only fragment
is strictly weaker than the read-write fragment, and the latter is still below
the complexity class NL. Extending view definitions with arbitrary arity
identifiers closes this gap: the extended fragment captures exactly NL. This
yields a strict hierarchy of SQL/PGQ fragments, whose union covers all NL
queries. On ordered structures the hierarchy collapses: once arity-2
identifiers are allowed, higher arities add no power, mirroring the classical
transitive-closure collapse and underscoring the central role of view
construction in property graph querying.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [DiLi: A Lock-Free Asynchronously Distributable Linked List](https://arxiv.org/abs/2510.06387)
*Raaghav Ravishankar,Sandeep Kulkarni,Sathya Peri,Gokarna Sharma*

Main category: cs.DC

TL;DR: DiLi是一个条件无锁、可线性化、可分布式的链表数据结构，支持动态分区和负载均衡，在多机环境下吞吐量线性扩展


<details>
  <summary>Details</summary>
Motivation: 解决现代数据库在高吞吐需求下，单机容量不足时无法简单替换硬件或静态分区的问题，需要支持动态分布式扩展

Method: 提出条件无锁概念，扩展传统无锁计算以适应进程间通信；采用二进制搜索分区方案和有限线性遍历的搜索方法；提供动态分区和负载均衡原语

Result: 单机性能与最先进的无锁并发搜索结构相当；多机环境下吞吐量随机器数量线性增长

Conclusion: DiLi成功实现了可动态分区和负载均衡的分布式无锁数据结构，在保持单机性能的同时实现多机线性扩展

Abstract: Modern databases use dynamic search structures that store a huge amount of
data, and often serve them using multi-threaded algorithms to support the
ever-increasing throughput needs. When this throughput need exceeds the
capacity of the machine hosting the structure, one either needs to replace the
underlying hardware (an option that is typically not viable and introduces a
long down time) or make the data structure distributed. Static partitioning of
the data structure for distribution is not desirable, as it is prone to uneven
load distribution over time, and having to change the partitioning scheme later
will require downtime.
  Since a distributed data structure, inherently, relies on communication
support from the network stack and operating systems, we introduce the notion
of conditional lock-freedom that extends the notion of lock-free computation
with reasonable assumptions about communication between processes. We present
DiLi, a conditional lock-free, linearizable, and distributable linked list that
can be asynchronously and dynamically (1) partitioned into multiple sublists
and (2) load balanced by distributing sublists across multiple machines. DiLi
contains primitives for these that also maintain the lock-free property of the
underlying search structure that supports find, remove, and insert of a key as
the client operations.
  Searching for an item in DiLi is by a novel traversal that involves a binary
search on the partitioning scheme, and then a linear traversal on a limitable
number of linked nodes. As a result, we are able to empirically show that DiLi
performs as well as the state-of-the-art lock-free concurrent search structures
that are based off of a linked list when executed on a single-machine. We also
show that the throughput of DiLi scales linearly with the number of machines
that host it.

</details>


### [6] [Adaptive Protein Design Protocols and Middleware](https://arxiv.org/abs/2510.06396)
*Aymen Alsaadi,Jonathan Ash,Mikhail Titov,Matteo Turilli,Andre Merzky,Shantenu Jha,Sagar Khare*

Main category: cs.DC

TL;DR: IMPRESS是一个集成机器学习和高性能计算的蛋白质结构设计平台，通过自适应协议和动态资源分配提高蛋白质设计质量和效率


<details>
  <summary>Details</summary>
Motivation: 蛋白质序列和结构空间极其庞大，传统计算方法需要大量计算资源进行采样，难以实现生成结构与预测结构之间的收敛

Method: 开发IMPRESS平台，耦合AI与高性能计算任务，实现自适应蛋白质设计协议，支持动态资源分配和异步工作负载执行

Result: 提高了蛋白质设计质量的一致性，增强了蛋白质设计的吞吐量

Conclusion: IMPRESS通过整合机器学习和先进计算系统，为蛋白质设计提供了高效的计算基础设施

Abstract: Computational protein design is experiencing a transformation driven by
AI/ML. However, the range of potential protein sequences and structures is
astronomically vast, even for moderately sized proteins. Hence, achieving
convergence between generated and predicted structures demands substantial
computational resources for sampling. The Integrated Machine-learning for
Protein Structures at Scale (IMPRESS) offers methods and advanced computing
systems for coupling AI to high-performance computing tasks, enabling the
ability to evaluate the effectiveness of protein designs as they are developed,
as well as the models and simulations used to generate data and train models.
This paper introduces IMPRESS and demonstrates the development and
implementation of an adaptive protein design protocol and its supporting
computing infrastructure. This leads to increased consistency in the quality of
protein design and enhanced throughput of protein design due to dynamic
resource allocation and asynchronous workload execution.

</details>


### [7] [MuFASA -- Asynchronous Checkpoint for Weakly Consistent Fully Replicated Databases](https://arxiv.org/abs/2510.06404)
*Raaghav Ravishankar,Sandeep Kulkarni,Nitin H Vaidya*

Main category: cs.DC

TL;DR: 提出了一种用于完全复制弱一致性分布式数据库的最小化检查点算法DTCS，通过O(n)消息开销和单计数器实现强一致性快照，解决了传统检查点的不一致性和高开销问题。


<details>
  <summary>Details</summary>
Motivation: 弱一致性分布式数据库中的检查点问题很重要但具有挑战性：最终一致性会产生用户未预期的异常，传统检查点会导致显著开销或不一致性。

Method: 定义了完全复制数据库的大小最小化检查点概念，提出仅需O(n)新消息和单个计数器的检查点算法。

Result: 该算法提供最小化检查点开销，相比现有分布式系统和主存数据库检查点算法有显著优势。

Conclusion: DTCS通过一系列强一致性快照总结计算过程，当最终一致性系统出现异常时，可以专注于异常时间点周围的快照进行分析。

Abstract: We focus on the problem of checkpointing in fully replicated weakly
consistent distributed databases, which we refer to as Distributed Transaction
Consistent Snapshot (DTCS). A typical example of such a system is a main-memory
database that provides strong eventual consistency. This problem is important
and challenging for several reasons: (1) eventual consistency often creates
anomalies that the users do not anticipate. Hence, frequent checkpoints to
ascertain desired invariants is highly beneficial in their use, and (2)
traditional checkpoints lead to significant overhead and/or inconsistencies. By
showing that the traditional checkpoint leads to inconsistencies or excessive
overhead, we define the notion of size-minimal checkpointing for fully
replicated databases. We present an algorithm for checkpointing with minimal
checkpointing overhead (only O(n) new messages and addition of a single counter
for existing messages). It also provides a significant benefit over existing
checkpointing algorithms for distributed systems and main-memory databases.
  A key benefit of DTCS is that it summarizes the computation by a sequence of
snapshots that are strongly consistent even though the underlying computation
is weakly consistent. In essence, when anomalies arise in an eventually
consistent system, DTCS enables one to concentrate solely on the snapshots
surrounding the time point of the anomaly.

</details>


### [8] [REACH: Reinforcement Learning for Adaptive Microservice Rescheduling in the Cloud-Edge Continuum](https://arxiv.org/abs/2510.06675)
*Xu Bai,Muhammed Tawfiqul Islam,Rajkumar Buyya,Adel N. Toosi*

Main category: cs.DC

TL;DR: REACH是一种基于强化学习的微服务动态重调度算法，用于在云边连续体中优化微服务放置，以应对资源波动和性能变化，显著降低端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 云计算难以满足新兴延迟敏感应用的实时需求，云边连续体结合了边缘资源的响应性和云的可扩展性，但异构动态计算资源给微服务最优放置带来挑战。

Method: 提出REACH算法，使用强化学习实时动态调整微服务放置，以适应分布式基础设施中波动的资源可用性和性能变化。

Result: 在真实世界测试平台上的广泛实验表明，REACH在三个基准MSA应用中分别将平均端到端延迟降低了7.9%、10%和8%，同时有效缓解了延迟波动和峰值。

Conclusion: REACH算法能够有效优化云边连续体中的微服务放置，显著提升延迟敏感应用的性能表现。

Abstract: Cloud computing, despite its advantages in scalability, may not always fully
satisfy the low-latency demands of emerging latency-sensitive pervasive
applications. The cloud-edge continuum addresses this by integrating the
responsiveness of edge resources with cloud scalability. Microservice
Architecture (MSA) characterized by modular, loosely coupled services, aligns
effectively with this continuum. However, the heterogeneous and dynamic
computing resource poses significant challenges to the optimal placement of
microservices. We propose REACH, a novel rescheduling algorithm that
dynamically adapts microservice placement in real time using reinforcement
learning to react to fluctuating resource availability, and performance
variations across distributed infrastructures. Extensive experiments on a
real-world testbed demonstrate that REACH reduces average end-to-end latency by
7.9%, 10%, and 8% across three benchmark MSA applications, while effectively
mitigating latency fluctuations and spikes.

</details>


### [9] [Multi-Dimensional Autoscaling of Stream Processing Services on Edge Devices](https://arxiv.org/abs/2510.06882)
*Boris Sedlak,Philipp Raith,Andrea Morichetta,Víctor Casamayor Pujol,Schahram Dustdar*

Main category: cs.DC

TL;DR: 提出了MUDAP多维度自动扩展平台，支持服务级和资源级的细粒度垂直扩展，通过RASK智能体基于结构知识回归分析优化服务执行，在边缘设备上相比基线方法减少了28%的SLO违规。


<details>
  <summary>Details</summary>
Motivation: 边缘设备资源有限，现有自动扩展机制仅关注资源扩展，无法满足竞争服务的SLO需求，需要支持多维度扩展来维持服务质量。

Method: 开发MUDAP平台支持服务级和资源级垂直扩展，使用基于结构知识回归分析(RASK)的智能体探索解决方案空间并学习连续回归模型，推断最优扩展动作。

Result: RASK仅需20次迭代(约200秒处理时间)即可推断出准确回归模型，相比Kubernetes VPA和强化学习智能体，在单个边缘设备上扩展9个服务时，RASK能承受最高请求负载且SLO违规减少28%。

Conclusion: 多维度自动扩展方法能有效提升边缘设备上竞争服务的性能，RASK智能体通过快速学习环境模型实现了更优的服务质量保障。

Abstract: Edge devices have limited resources, which inevitably leads to situations
where stream processing services cannot satisfy their needs. While existing
autoscaling mechanisms focus entirely on resource scaling, Edge devices require
alternative ways to sustain the Service Level Objectives (SLOs) of competing
services. To address these issues, we introduce a Multi-dimensional Autoscaling
Platform (MUDAP) that supports fine-grained vertical scaling across both
service- and resource-level dimensions. MUDAP supports service-specific scaling
tailored to available parameters, e.g., scale data quality or model size for a
particular service. To optimize the execution across services, we present a
scaling agent based on Regression Analysis of Structural Knowledge (RASK). The
RASK agent efficiently explores the solution space and learns a continuous
regression model of the processing environment for inferring optimal scaling
actions. We compared our approach with two autoscalers, the Kubernetes VPA and
a reinforcement learning agent, for scaling up to 9 services on a single Edge
device. Our results showed that RASK can infer an accurate regression model in
merely 20 iterations (i.e., observe 200s of processing). By increasingly adding
elasticity dimensions, RASK sustained the highest request load with 28% less
SLO violations, compared to baselines.

</details>


### [10] [GROMACS Unplugged: How Power Capping and Frequency Shapes Performance on GPUs](https://arxiv.org/abs/2510.06902)
*Ayesha Afzal,Anna Kahler,Georg Hager,Gerhard Wellein*

Main category: cs.DC

TL;DR: 该论文分析了四种NVIDIA GPU加速器在GROMACS分子动力学模拟中的性能表现，研究了频率缩放和功率限制对性能的影响，为硬件选择和性能优化提供指导。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟性能严重依赖硬件选择和配置，需要了解不同GPU在GROMACS工作负载下的性能特征，特别是在频率缩放和功率限制条件下的表现。

Method: 使用四种NVIDIA GPU（A40、A100、L4、L40）测试六种代表性GROMACS生物分子工作负载和两个合成基准测试（Pi Solver计算绑定、STREAM Triad内存绑定），研究GPU图形时钟频率缩放和功率限制对性能的影响。

Result: 小型GROMACS系统表现出强烈的频率敏感性，而大型系统快速饱和并变得内存受限；在功率限制下，性能在达到特定阈值前保持稳定，高端GPU如A100即使在降低功率预算下也能维持接近最大性能。

Conclusion: 研究结果为在功率约束下选择GPU硬件和优化大规模分子动力学工作流程的GROMACS性能提供了实用指导。

Abstract: Molecular dynamics simulations are essential tools in computational
biophysics, but their performance depend heavily on hardware choices and
configuration. In this work, we presents a comprehensive performance analysis
of four NVIDIA GPU accelerators -- A40, A100, L4, and L40 -- using six
representative GROMACS biomolecular workloads alongside two synthetic
benchmarks: Pi Solver (compute bound) and STREAM Triad (memory bound). We
investigate how performance scales with GPU graphics clock frequency and how
workloads respond to power capping. The two synthetic benchmarks define the
extremes of frequency scaling: Pi Solver shows ideal compute scalability, while
STREAM Triad reveals memory bandwidth limits -- framing GROMACS's performance
in context. Our results reveal distinct frequency scaling behaviors: Smaller
GROMACS systems exhibit strong frequency sensitivity, while larger systems
saturate quickly, becoming increasingly memory bound. Under power capping,
performance remains stable until architecture- and workload-specific thresholds
are reached, with high-end GPUs like the A100 maintaining near-maximum
performance even under reduced power budgets. Our findings provide practical
guidance for selecting GPU hardware and optimizing GROMACS performance for
large-scale MD workflows under power constraints.

</details>


### [11] [Evaluating Rapid Makespan Predictions for Heterogeneous Systems with Programmable Logic](https://arxiv.org/abs/2510.06998)
*Martin Wilhelm,Franz Freitag,Max Tzschoppe,Thilo Pionteck*

Main category: cs.DC

TL;DR: 本文提出了一个用于异构计算系统的灵活评估框架，能够基于抽象任务图描述收集真实世界的makespan结果，帮助开发快速makespan预测算法。


<details>
  <summary>Details</summary>
Motivation: 异构计算系统中任务映射对整体makespan的影响预测非常困难。现有模拟器需要完整任务实现，而纯分析方法虽然快速但过于抽象。需要弥合理论与实践之间的差距。

Method: 开发了一个高度灵活的评估框架，支持CPU、GPU和FPGA异构系统，能够基于抽象任务图描述收集真实makespan结果。

Result: 分析了现有分析方法预测实际makespan的程度，并识别了异构系统中由数据传输开销和设备拥塞等高层特性带来的常见挑战。

Conclusion: 该框架有助于开发更准确的快速makespan预测算法，解决了异构计算系统任务映射优化中的关键挑战。

Abstract: Heterogeneous computing systems, which combine general-purpose processors
with specialized accelerators, are increasingly important for optimizing the
performance of modern applications. A central challenge is to decide which
parts of an application should be executed on which accelerator or, more
generally, how to map the tasks of an application to available devices.
Predicting the impact of a change in a task mapping on the overall makespan is
non-trivial. While there are very capable simulators, these generally require a
full implementation of the tasks in question, which is particularly
time-intensive for programmable logic. A promising alternative is to use a
purely analytical function, which allows for very fast predictions, but
abstracts significantly from reality. Bridging the gap between theory and
practice poses a significant challenge to algorithm developers. This paper aims
to aid in the development of rapid makespan prediction algorithms by providing
a highly flexible evaluation framework for heterogeneous systems consisting of
CPUs, GPUs and FPGAs, which is capable of collecting real-world makespan
results based on abstract task graph descriptions. We analyze to what extent
actual makespans can be predicted by existing analytical approaches.
Furthermore, we present common challenges that arise from high-level
characteristics such as data transfer overhead and device congestion in
heterogeneous systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [12] [Leveraging Large Language Models for Cybersecurity Risk Assessment -- A Case from Forestry Cyber-Physical Systems](https://arxiv.org/abs/2510.06343)
*Fikret Mert Gültekin,Oscar Lilja,Ranim Khojah,Rebekka Wohlrab,Marvin Damschen,Mazen Mohamad*

Main category: cs.SE

TL;DR: 使用本地部署的大型语言模型结合检索增强生成技术，支持林业领域网络安全风险评估，满足数据保护要求，通过专家访谈和调查验证有效性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键软件系统中，网络安全专家稀缺导致工作负荷高，需要工具支持工程师进行风险评估，同时满足数据隐私保护要求。

Method: 采用设计科学研究方法，通过12位专家的访谈、互动会议和问卷调查，在大型项目中验证本地LLM结合RAG技术的应用。

Result: LLM能够协助生成初始风险评估、识别威胁并进行冗余检查，但需要人工监督确保准确性和合规性，专家愿意在特定评估和辅助角色中使用LLM。

Conclusion: LLM代理可以支持安全关键领域网络物理系统的风险评估过程，但需结合人类监督，为相关应用提供实践见解。

Abstract: In safety-critical software systems, cybersecurity activities become
essential, with risk assessment being one of the most critical. In many
software teams, cybersecurity experts are either entirely absent or represented
by only a small number of specialists. As a result, the workload for these
experts becomes high, and software engineers would need to conduct
cybersecurity activities themselves. This creates a need for a tool to support
cybersecurity experts and engineers in evaluating vulnerabilities and threats
during the risk assessment process. This paper explores the potential of
leveraging locally hosted large language models (LLMs) with retrieval-augmented
generation to support cybersecurity risk assessment in the forestry domain
while complying with data protection and privacy requirements that limit
external data sharing. We performed a design science study involving 12 experts
in interviews, interactive sessions, and a survey within a large-scale project.
The results demonstrate that LLMs can assist cybersecurity experts by
generating initial risk assessments, identifying threats, and providing
redundancy checks. The results also highlight the necessity for human oversight
to ensure accuracy and compliance. Despite trust concerns, experts were willing
to utilize LLMs in specific evaluation and assistance roles, rather than solely
relying on their generative capabilities. This study provides insights that
encourage the use of LLM-based agents to support the risk assessment process of
cyber-physical systems in safety-critical domains.

</details>


### [13] [Improving Assignment Submission in Higher Education through a Git-Enabled System: An Iterative Case Study](https://arxiv.org/abs/2510.06363)
*Ololade Babatunde,Tomisin Ayodabo,Raqibul Raqibul*

Main category: cs.SE

TL;DR: 该研究通过引入定制化的Git提交系统，解决了高等教育中传统作业提交方法的挑战，显著提升了作业跟踪、协作和提交效率。


<details>
  <summary>Details</summary>
Motivation: 解决高等教育中传统作业提交方法在跟踪、协作和效率方面的不足，探索分布式版本控制在教育环境中的应用价值。

Method: 采用迭代软件开发和以用户为中心的设计方法，将系统集成到真实大学环境中，通过可用性测试和学生反馈进行实证评估。

Result: 85%的教师认为Git系统更易用，84%的学生偏好该系统，提交和评审时间减少38%，存储需求降低48%，学生报告了更好的学习成果和减轻的管理负担。

Conclusion: Git提交系统为教育环境中集成分布式版本控制提供了实用见解，增强了教师监督和学生参与度，尽管初期采用和学习曲线存在挑战但可通过迭代改进缓解。

Abstract: This study addresses challenges in traditional assignment submission methods
used in higher education by introducing and evaluating a customized Git-based
submission system. Employing iterative software development and user-centered
design methodologies, the system was integrated within a real-world university
environment. Empirical evaluation, including usability testing and student
feedback, indicated significant improvements in assignment tracking,
collaboration, and submission efficiency. Students reported positive
experiences using distributed version control workflows, highlighting improved
learning outcomes and reduced administrative burden. Challenges related to
initial adoption and student learning curves were identified and mitigated
through iterative improvements. The proposed system contributes practical
insights for integrating distributed version control into educational settings,
enhancing both instructor oversight and student engagement in software
engineering and related disciplines. Based on our results, the research showed
that 85% of instructors found the git based system easier to use, with 84% of
students preferring it over traditional methods, as it provides a 38% reduction
in time taken for submission and review, while also leading to a 48% reduction
in storage requirements.

</details>


### [14] [Addressing Visual Impairments with Model-Driven Engineering: A Systematic Literature Review](https://arxiv.org/abs/2510.06483)
*Judith Michael,Lukas Netz,Bernhard Rumpe,Ingo Müller,John Grundy,Shavindra Wickramathilaka,Hourieh Khalajzadeh*

Main category: cs.SE

TL;DR: 本文通过系统文献综述分析了模型驱动工程(MDE)如何解决视觉障碍的可访问性问题，发现当前研究在方法细节、用户参与和实证验证方面存在不足，并提出了改进研究议程。


<details>
  <summary>Details</summary>
Motivation: 软件应用经常对具有可访问性需求(如视觉障碍)的用户构成障碍。模型驱动工程(MDE)通过系统化的代码推导，提供了将可访问性关注点集成到软件开发中的系统方法，同时减少人工工作量。

Method: 对447篇初步识别论文进行系统文献综述，最终30篇主要研究符合纳入标准。分析这些研究中如何建模用户界面结构、交互导航、用户能力、需求和上下文信息。

Result: 约三分之二的研究参考了WCAG指南，但项目特定适配和最终用户验证阻碍了在MDE中的更广泛采用。研究缺乏具体的建模技术细节、转换规则或代码模板，影响了可重用性、通用性和可重现性。用户参与不足和开发者可访问性专业知识有限导致实证验证薄弱。

Conclusion: 当前MDE研究对视觉相关可访问性的支持不足。需要更有效地将视觉障碍支持嵌入MDE流程，包括改进方法细节、加强用户参与和实证验证。

Abstract: Software applications often pose barriers for users with accessibility needs,
e.g., visual impairments. Model-driven engineering (MDE), with its systematic
nature of code derivation, offers systematic methods to integrate accessibility
concerns into software development while reducing manual effort. This paper
presents a systematic literature review on how MDE addresses accessibility for
vision impairments. From 447 initially identified papers, 30 primary studies
met the inclusion criteria. About two-thirds reference the Web Content
Accessibility Guidelines (WCAG), yet their project-specific adaptions and
end-user validations hinder wider adoption in MDE. The analyzed studies model
user interface structures, interaction and navigation, user capabilities,
requirements, and context information. However, only few specify concrete
modeling techniques on how to incorporate accessibility needs or demonstrate
fully functional systems. Insufficient details on MDE methods, i.e.,
transformation rules or code templates, hinder the reuse, generalizability, and
reproducibility. Furthermore, limited involvement of affected users and limited
developer expertise in accessibility contribute to weak empirical validation.
Overall, the findings indicate that current MDE research insufficiently
supports vision-related accessibility. Our paper concludes with a research
agenda outlining how support for vision impairments can be more effectively
embedded in MDE processes.

</details>


### [15] [Beyond More Context: How Granularity and Order Drive Code Completion Quality](https://arxiv.org/abs/2510.06606)
*Uswat Yusuf,Genevieve Caumartin,Diego Elias Costa*

Main category: cs.SE

TL;DR: 本文提出了一种基于静态分析的代码块检索方法，用于改进代码补全中的上下文收集策略，在ASE 2025挑战中显著提升了LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 在大规模代码库中，为代码补全任务构建相关上下文面临两个挑战：LLM的上下文长度限制使得无法包含所有文件；生成代码质量对噪声或无关上下文高度敏感。

Method: 开发并评估了文件和代码块级别的检索策略，重点研究上下文大小和文件排序对LLM性能的影响，引入了基于静态分析的代码块检索方法。

Result: 基于静态分析的代码块检索比最佳文件检索策略提升了6%，比无上下文基线在Python中提升了16%。结果表明上下文的数量、排序和检索粒度对模型性能有显著影响。

Conclusion: 检索粒度、排序策略和混合方法对于为真实开发场景构建有效的上下文收集管道至关重要。

Abstract: Context plays an important role in the quality of code completion, as Large
Language Models (LLMs) require sufficient and relevant information to assist
developers in code generation tasks. However, composing a relevant context for
code completion poses challenges in large repositories: First, the limited
context length of LLMs makes it impractical to include all repository files.
Second, the quality of generated code is highly sensitive to noisy or
irrelevant context. In this paper, we present our approach for the ASE 2025
Context Collection Challenge. The challenge entails outperforming JetBrains
baselines by designing effective retrieval and context collection strategies.
We develop and evaluate a series of experiments that involve retrieval
strategies at both the file and chunk levels. We focus our initial experiments
on examining the impact of context size and file ordering on LLM performance.
Our results show that the amount and order of context can significantly
influence the performance of the models. We introduce chunk-based retrieval
using static analysis, achieving a 6% improvement over our best file-retrieval
strategy and a 16% improvement over the no-context baseline for Python in the
initial phase of the competition. Our results highlight the importance of
retrieval granularity, ordering and hybrid strategies in developing effective
context collection pipelines for real-world development scenarios.

</details>


### [16] [AISysRev -- LLM-based Tool for Title-abstract Screening](https://arxiv.org/abs/2510.06708)
*Aleksi Huotala,Miikka Kuutila,Olli-Pekka Turtio,Mika Mäntylä*

Main category: cs.SE

TL;DR: 开发了一个基于LLM的系统综述筛选工具AiSysRev，通过自动化筛选减少人工工作量，支持多种LLM模型和零样本/少样本学习，在边界案例中仍需人工干预。


<details>
  <summary>Details</summary>
Motivation: 系统综述在软件工程中很重要，但筛选阶段工作量巨大。LLM虽然不能完全替代人工，但可以显著减轻评估大量科学文献的负担。

Method: 开发了AiSysRev工具，作为Docker容器中的Web应用，接受包含论文标题和摘要的CSV文件，用户指定纳入和排除标准，通过OpenRouter支持多种LLM，提供零样本和少样本筛选以及手动筛选界面。

Result: 对137篇论文的试验研究表明，论文可分为四类：简单纳入、简单排除、边界纳入和边界排除。LLM在边界案例中容易出错，需要人工干预。

Conclusion: LLM不能替代系统综述中的人类判断，但可以显著减少评估大量科学文献的负担，特别是在快速综述中很有帮助。

Abstract: Systematic reviews are a standard practice for summarizing the state of
evidence in software engineering. Conducting systematic reviews is laborious,
especially during the screening or study selection phase, where the number of
papers can be overwhelming. During this phase, papers are assessed against
inclusion and exclusion criteria based on their titles and abstracts. Recent
research has demonstrated that large language models (LLMs) can perform
title-abstract screening at a level comparable to that of a master's student.
While LLMs cannot be fully trusted, they can help, for example, in Rapid
Reviews, which try to expedite the review process. Building on recent research,
we developed AiSysRev, an LLM-based screening tool implemented as a web
application running in a Docker container. The tool accepts a CSV file
containing paper titles and abstracts. Users specify inclusion and exclusion
criteria. One can use multiple LLMs for screening via OpenRouter. AiSysRev
supports both zero-shot and few-shot screening, and also allows for manual
screening through interfaces that display LLM results as guidance for human
reviewers.We conducted a trial study with 137 papers using the tool. Our
findings indicate that papers can be classified into four categories: Easy
Includes, Easy Excludes, Boundary Includes, and Boundary Excludes. The Boundary
cases, where LLMs are prone to errors, highlight the need for human
intervention. While LLMs do not replace human judgment in systematic reviews,
they can significantly reduce the burden of assessing large volumes of
scientific literature. Video: https://www.youtube.com/watch?v=jVbEj4Y4tQI Tool:
https://github.com/EvoTestOps/AISysRev

</details>


### [17] [LLM Company Policies and Policy Implications in Software Organizations](https://arxiv.org/abs/2510.06718)
*Ranim Khojah,Mazen Mohamad,Linda Erlenhov,Francisco Gomes de Oliveira Neto,Philipp Leitner*

Main category: cs.SE

TL;DR: 研究11家公司如何制定大语言模型聊天机器人使用政策及其影响因素，帮助管理者安全地将聊天机器人集成到开发工作流程中


<details>
  <summary>Details</summary>
Motivation: 软件组织采用大语言模型聊天机器人存在风险，需要明确的政策来指导使用

Method: 研究11家公司的政策制定过程和影响因素

Result: 分析了企业制定LLM聊天机器人政策的方法和关键因素

Conclusion: 研究结果有助于管理者在开发工作流程中安全集成聊天机器人

Abstract: The risks associated with adopting large language model (LLM) chatbots in
software organizations highlight the need for clear policies. We examine how 11
companies create these policies and the factors that influence them, aiming to
help managers safely integrate chatbots into development workflows.

</details>


### [18] [Oops!... I did it again. Conclusion (In-)Stability in Quantitative Empirical Software Engineering: A Large-Scale Analysis](https://arxiv.org/abs/2510.06844)
*Nicole Hoess,Carlos Paradis,Rick Kazman,Wolfgang Mauerer*

Main category: cs.SE

TL;DR: 本研究通过复制三个软件演化分析研究，评估了四种独立挖掘工具在数据提取、分析结果和结论方面的一致性，发现工具设计和实现中的技术细节会导致显著差异。


<details>
  <summary>Details</summary>
Motivation: 软件仓库挖掘工具在研究和实践中被广泛应用，但其局限性和一致性往往未被充分理解，这威胁到软件演化分析的有效性。

Method: 通过轻量级文献综述选择三个关于协作协调、软件维护和软件质量的研究，使用四种独立选择的挖掘工具进行正式复制，定量和定性比较提取的数据、分析结果和结论。

Result: 工具设计和实现中的众多技术细节在复杂挖掘管道中累积，可能导致提取的基线数据、其衍生数据、统计分析结果以及特定情况下的结论出现显著差异。

Conclusion: 用户必须仔细选择工具并评估其局限性，以适当方式评估有效性范围。建议重用工具，研究人员和工具作者可通过复制包和比较研究促进可重用性并减少不确定性。

Abstract: Context: Mining software repositories is a popular means to gain insights
into a software project's evolution, monitor project health, support decisions
and derive best practices. Tools supporting the mining process are commonly
applied by researchers and practitioners, but their limitations and agreement
are often not well understood.
  Objective: This study investigates some threats to validity in complex tool
pipelines for evolutionary software analyses and evaluates the tools' agreement
in terms of data, study outcomes and conclusions for the same research
questions.
  Method: We conduct a lightweight literature review to select three studies on
collaboration and coordination, software maintenance and software quality from
high-ranked venues, which we formally replicate with four independent,
systematically selected mining tools to quantitatively and qualitatively
compare the extracted data, analysis results and conclusions.
  Results: We find that numerous technical details in tool design and
implementation accumulate along the complex mining pipelines and can cause
substantial differences in the extracted baseline data, its derivatives,
subsequent results of statistical analyses and, under specific circumstances,
conclusions.
  Conclusions: Users must carefully choose tools and evaluate their limitations
to assess the scope of validity in an adequate way. Reusing tools is
recommended. Researchers and tool authors can promote reusability and help
reducing uncertainties by reproduction packages and comparative studies
following our approach.

</details>


### [19] [An empirical study on declined proposals: why are these proposals declined?](https://arxiv.org/abs/2510.06984)
*Masanari Kondo,Mahmoud Alfadel,Shane McIntosh,Yasutaka Kamei,Naoyasu Ubayashi*

Main category: cs.SE

TL;DR: 该研究分析了Go编程语言项目的提案特征和结果，发现提案被拒绝的比例高于接受，且解决时间通常超过一个月。通过定性分析识别了9个主要拒绝原因，并证明GPT模型可以在讨论早期预测拒绝决策。


<details>
  <summary>Details</summary>
Motivation: 开源软件项目的设计决策提案过程资源密集且常导致贡献者挫败感，特别是当提案被拒绝而缺乏明确反馈时。目前对提案拒绝原因的理解有限，限制了流程优化和有效指导贡献者的机会。

Method: 对Go项目的1,091个提案进行混合方法实证研究，量化提案结果，建立拒绝原因分类法，并评估大型语言模型预测这些结果的能力。

Result: 提案被拒绝的比例高于接受，解决时间中位数超过一个月。仅14.7%的被拒绝提案会重新提交。识别出9个主要拒绝原因，包括重复、用例有限或违反项目原则等。GPT模型在部分评论情况下可达到0.71的F1分数来预测拒绝决策。

Conclusion: 研究揭示了提案过程中的低效性，并强调了通过早期分诊和基于结构化拒绝原因理解指导贡献者来改善贡献者体验和审查者工作量的可行机会。

Abstract: Design-level decisions in open-source software (OSS) projects are often made
through structured mechanisms such as proposals, which require substantial
community discussion and review. Despite their importance, the proposal process
is resource-intensive and often leads to contributor frustration, especially
when proposals are declined without clear feedback. Yet, the reasons behind
proposal rejection remain poorly understood, limiting opportunities to
streamline the process or guide contributors effectively. This study
investigates the characteristics and outcomes of proposals in the Go
programming language to understand why proposals are declined and how such
outcomes might be anticipated. We conduct a mixed-method empirical study on
1,091 proposals submitted to the Go project. We quantify proposal outcomes,
build a taxonomy of decline reasons, and evaluate large language models (LLMs)
for predicting these outcomes. We find that proposals are more often declined
than accepted, and resolution typically takes over a month. Only 14.7% of
declined proposals are ever resubmitted. Through qualitative coding, we
identify nine key reasons for proposal decline, such as duplication, limited
use cases, or violations of project principles. This taxonomy can help
contributors address issues in advance, e.g., checking for existing
alternatives can reduce redundancy. We also demonstrate that GPT-based models
can predict decline decisions early in the discussion (F1 score = 0.71 with
partial comments), offering a practical tool for prioritizing review effort.
Our findings reveal inefficiencies in the proposal process and highlight
actionable opportunities for improving both contributor experience and reviewer
workload by enabling early triage and guiding contributors to strengthen their
proposals using a structured understanding of past decline reasons.

</details>


### [20] [Human-aligned AI Model Cards with Weighted Hierarchy Architecture](https://arxiv.org/abs/2510.06989)
*Pengyue Yang,Haolin Jin,Qingwen Zeng,Jiawen Wen,Harry Rao,Huaming Chen*

Main category: cs.SE

TL;DR: 提出了CRAI-MCF框架，从静态披露转向可操作、以人为本的文档，解决大型语言模型发现和采用中的挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生态系统中模型发现和采用面临挑战，现有文档框架如Model Cards和FactSheets存在静态、定性为主、缺乏定量比较机制等问题。

Method: 基于价值敏感设计，通过对240个开源项目的实证分析，提炼出217个参数，构建了包含八个模块的价值对齐架构，并引入了定量充分性标准。

Result: 开发了CRAI-MCF框架，能够实现严格的跨模型比较，平衡技术、伦理和运营维度。

Conclusion: CRAI-MCF框架使从业者能够更高效地评估、选择和采用大型语言模型，提高信心和运营完整性。

Abstract: The proliferation of Large Language Models (LLMs) has led to a burgeoning
ecosystem of specialized, domain-specific models. While this rapid growth
accelerates innovation, it has simultaneously created significant challenges in
model discovery and adoption. Users struggle to navigate this landscape due to
inconsistent, incomplete, and imbalanced documentation across platforms.
Existing documentation frameworks, such as Model Cards and FactSheets, attempt
to standardize reporting but are often static, predominantly qualitative, and
lack the quantitative mechanisms needed for rigorous cross-model comparison.
This gap exacerbates model underutilization and hinders responsible adoption.
To address these shortcomings, we introduce the Comprehensive Responsible AI
Model Card Framework (CRAI-MCF), a novel approach that transitions from static
disclosures to actionable, human-aligned documentation. Grounded in Value
Sensitive Design (VSD), CRAI-MCF is built upon an empirical analysis of 240
open-source projects, distilling 217 parameters into an eight-module,
value-aligned architecture. Our framework introduces a quantitative sufficiency
criterion to operationalize evaluation and enables rigorous cross-model
comparison under a unified scheme. By balancing technical, ethical, and
operational dimensions, CRAI-MCF empowers practitioners to efficiently assess,
select, and adopt LLMs with greater confidence and operational integrity.

</details>


### [21] [Building an Open AIBOM Standard in the Wild](https://arxiv.org/abs/2510.07070)
*Gopi Krishnan Rajbahadur,Keheliya Gallaba,Elyas Rashno,Arthit Suriyawongkul,Karen Bennet,Kate Stewart,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文介绍了AI物料清单(AIBOM)规范的开发经验，这是SPDX标准的扩展，用于记录AI组件如数据集和训练工件。通过行动研究方法，联合90多名贡献者开发，并通过法规对齐、行业用例映射、访谈和案例研究进行验证。


<details>
  <summary>Details</summary>
Motivation: 探索在快速发展的AI系统领域，如何创建开放社区驱动的标准，填补AI组件标准化管理的空白。

Method: 采用行动研究方法，进行全球多利益相关方协作，包括90多名贡献者参与的结构化AR周期。通过四种互补方法验证：法规标准对齐、行业用例映射、半结构化从业者访谈和工业案例研究。

Result: 开发出经过验证的AIBOM规范，能够有效捕获AI组件，并与主要法规和行业需求保持一致。

Conclusion: 不仅交付了验证过的AIBOM规范，还记录了在真实环境中构建规范的过程，提炼的经验可为软件工程社区未来的标准化工作提供参考。

Abstract: Modern software engineering increasingly relies on open, community-driven
standards, yet how such standards are created in fast-evolving domains like
AI-powered systems remains underexplored. This paper presents a detailed
experience report on the development of the AI Bill of Materials AIBOM
specification, an extension of the ISO/IEC 5962:2021 Software Package Data
Exchange (SPDX) software bill of materials (SBOM) standard, which captures AI
components such as datasets and iterative training artifacts. Framed through
the lens of Action Research (AR), we document a global, multi-stakeholder
effort involving over 90 contributors and structured AR cycles. The resulting
specification was validated through four complementary approaches: alignment
with major regulations and ethical standards (e.g., EU AI Act and IEEE 7000
standards), systematic mapping to six industry use cases, semi-structured
practitioner interviews, and an industrial case study. Beyond delivering a
validated artefact, our paper documents the process of building the AIBOM
specification in the wild, and reflects on how it aligns with the AR cycle, and
distills lessons that can inform future standardization efforts in the software
engineering community.

</details>


### [22] [Prompt, Synthesize, Fine-Tune: A Secure Code Generation Recipe](https://arxiv.org/abs/2510.07189)
*Junjie Li,Fazle Rabbi,Bo Yang,Song Wang,Jinqiu Yang*

Main category: cs.SE

TL;DR: Secure-Instruct是一个自动合成高质量漏洞和安全代码示例的框架，通过指令微调提升LLM生成安全代码的能力，在安全性和功能性方面都有显著改进。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型生成的代码往往存在安全隐患，现有方法（如SafeCoder）受限于有限且不平衡的数据集，影响了效果和泛化能力。

Method: 提出Secure-Instruct框架，自动合成漏洞和安全代码示例，生成微调指令，并对LLM进行指令微调，使任务描述与安全代码生成能力对齐。

Result: 在CWEBench上，Secure-Instruct将安全代码生成能力平均提升14.3%，比SafeCoder高出7.6%；在CWEval上，Func-Sec@1指标分别提升14%（CodeLlama-7B）和5.8%（Mistral-7B），比SafeCoder分别高出15.8%和6.8%。

Conclusion: Secure-Instruct不仅能提高生成代码的安全性，还能提升其功能正确性，为安全代码生成提供了有效的解决方案。

Abstract: Although Large Language Models (LLMs) show promising solutions to automated
code generation, they often produce insecure code that threatens software
security. Current approaches (e.g., SafeCoder) to improve secure code
generation suffer from limited and imbalanced datasets, reducing their
effectiveness and generalizability. In this work, we present Secure-Instruct, a
novel framework that automatically synthesizes high-quality vulnerable and
secure code examples, generates fine-tuning instructions, and instruction-tunes
LLMs to align task description and secure code generation abilities. We
evaluate Secure-Instruct on four representative LLMs using two benchmarks: our
own CWEBench and the existing CWEval. CWEBench comprises 93 scenarios on 44
CWEs, all without overlap with Secure-Instruct's synthetic instruction-tuning
dataset, while CWEval covers 31 CWEs with 119 manually verified
security-critical tasks. We find that Secure-Instruct improves not only the
security but also the functional correctness of the generated code. On
CWEBench, Secure-Instruct substantially improves secure code generation, giving
a 14.3% average increase in secure ratio over the pretrained models and
outperforms SafeCoder by 7.6%. On CWEval, Secure-Instruct achieves a 14%
increase for CodeLlama-7B and 5.8% for Mistral-7B in Func-Sec@1 over pretrained
models, and surpasses SafeCoder by 15.8% and 6.8% respectively.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [23] [Adaptive Semantic Communication for UAV/UGV Cooperative Path Planning](https://arxiv.org/abs/2510.06901)
*Fangzhou Zhao,Yao Sun,Jianglin Lan,Lan Zhang,Xuesong Liu,Muhammad Ali Imran*

Main category: cs.NI

TL;DR: 提出了一种语义通信框架来增强无人机与无人地面车辆在不可靠无线条件下的协同路径规划，通过传输关键语义信息而非原始数据来降低传输量并保持规划精度。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中，无人机和无人地面车辆的无线通信常因干扰和非视距条件而不稳定，难以支持及时准确的协同路径规划。

Method: 通过定义路径规划的关键语义并设计专用收发器，构建语义通信框架，仅传输路径规划所需的关键信息。

Result: 仿真结果表明，相比传统语义通信收发器，所提方法显著降低了数据传输量，同时保持了路径规划精度，提高了系统协作效率。

Conclusion: 语义通信框架能有效解决无人机与无人地面车辆在复杂环境中的协同路径规划通信问题，在保持精度的同时大幅减少通信开销。

Abstract: Effective path planning is fundamental to the coordination of unmanned aerial
vehicles (UAVs) and unmanned ground vehicles (UGVs) systems, particularly in
applications such as surveillance, navigation, and emergency response.
Combining UAVs' broad field of view with UGVs' ground-level operational
capability greatly improve the likelihood of successfully achieving task
objectives such as locating victims, monitoring target areas, or navigating
hazardous terrain. In complex environments, UAVs need to provide precise
environmental perception information for UGVs to optimize their routing policy.
However, due to severe interference and non-line-of-sight conditions, wireless
communication is often unstable in such complex environments, making it
difficult to support timely and accurate path planning for UAV-UGV
coordination. To this end, this paper proposes a semantic communication
(SemCom) framework to enhance UAV/UGV cooperative path planning under
unreliable wireless conditions. Unlike traditional methods that transmit raw
data, SemCom transmits only the key information for path planning, reducing
transmission volume without sacrificing accuracy. The proposed framework is
developed by defining key semantics for path planning and designing a
transceiver for meeting the requirements of UAV-UGV cooperative path planning.
Simulation results show that, compared to conventional SemCom transceivers, the
proposed transceiver significantly reduces data transmission volume while
maintaining path planning accuracy, thereby enhancing system collaboration
efficiency.

</details>


### [24] [Dynamic Control Aware Semantic Communication Enabled Image Transmission for Lunar Landing](https://arxiv.org/abs/2510.06916)
*Fangzhou Zhao,Yao Sun,Jianglin Lan,Muhammad Ali Imran*

Main category: cs.NI

TL;DR: 提出了一种基于语义通信的月球着陆器图像传输框架，通过动态调整传输策略来提升自主着陆控制系统的精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决月球着陆任务中本地控制系统处理高动态条件能力有限的问题，利用月球轨道卫星运行高性能着陆算法，但传统通信方式在月球恶劣环境下不可靠。

Method: 设计语义通信编码器-解码器框架，根据着陆控制算法的实时反馈动态调整传输策略，确保关键图像特征的准确传输。

Result: 理论分析表明该框架能提高控制算法精度并减少端到端传输时间，仿真结果显示相比传统通信方法显著提升了自主着陆性能。

Conclusion: 语义通信框架在月球恶劣通信环境下具有明显优势，能有效提升自主着陆的精度和可靠性。

Abstract: The primary challenge in autonomous lunar landing missions lies in the
unreliable local control system, which has limited capacity to handle
high-dynamic conditions, severely affecting landing precision and safety.
Recent advancements in lunar satellite communication make it possible to
establish a wireless link between lunar orbit satellites and the lunar lander.
This enables satellites to run high-performance autonomous landing algorithms,
improving landing accuracy while reducing the lander's computational and
storage load. Nevertheless, traditional communication paradigms are not
directly applicable due to significant temperature fluctuations on the lunar
surface, intense solar radiation, and severe interference caused by lunar dust
on hardware. The emerging technique of semantic communication (SemCom) offers
significant advantages in robustness and resource efficiency, particularly
under harsh channel conditions. In this paper, we introduce a novel SemCom
framework for transmitting images from the lander to satellites operating the
remote landing control system. The proposed encoder-decoder dynamically adjusts
the transmission strategy based on real-time feedback from the lander's control
algorithm, ensuring the accurate delivery of critical image features and
enhancing control reliability. We provide a rigorous theoretical analysis of
the conditions that improve the accuracy of the control algorithm and reduce
end-to-end transmission time under the proposed framework. Simulation results
demonstrate that our SemCom method significantly enhances autonomous landing
performance compared to traditional communication methods.

</details>


### [25] [A Genetic Algorithm Approach to Anti-Jamming UAV Swarm Behavior](https://arxiv.org/abs/2510.07292)
*Tiago Silva,António Grilo*

Main category: cs.NI

TL;DR: 使用遗传算法联合优化无人机群编队、波束成形天线和流量路由，以减轻主协调信道中的干扰影响。


<details>
  <summary>Details</summary>
Motivation: 无人机群在军事应用中面临通信干扰的威胁，现有抗干扰技术虽多，但结合智能群行为来增强抗干扰能力仍是开放研究问题。

Method: 采用遗传算法同时优化无人机群编队、波束成形天线和流量路由，假设存在一个更稳健的低数据率信道用于编队管理信令。

Result: 仿真结果表明所提方法有效，但计算成本较高。

Conclusion: 该方法能有效缓解干扰问题，但计算复杂度高，为后续研究提供了方向。

Abstract: In recent years, Unmanned Aerial Vehicles (UAVs) have brought a new true
revolution to military tactics. While UAVs already constitute an advantage when
operating alone, multi-UAV swarms expand the available possibilities, allowing
the UAVs to collaborate and support each other as a team to carry out a given
task. This entails the capability to exchange information related with
situation awareness and action coordination by means of a suitable wireless
communication technology. In such scenario, the adversary is expected to
disrupt communications by jamming the communication channel. The latter becomes
the Achilles heel of the swarm. While anti-jamming techniques constitute a well
covered topic in the literature, the use of intelligent swarm behaviors to
leverage those techniques is still an open research issue.
  This paper explores the use of Genetic Algorithms (GAs) to jointly optimize
UAV swarm formation, beam-steering antennas and traffic routing in order to
mitigate the effect of jamming in the main coordination channel, under the
assumption that a more robust and low data rate channel is used for formation
management signaling. Simulation results show the effectiveness of proposed
approach. However, the significant computational cost paves the way for further
research.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [26] [Making and Evaluating Calibrated Forecasts](https://arxiv.org/abs/2510.06388)
*Yuxuan Lu,Yifan Wu,Jason Hartline,Lunjia Hu*

Main category: cs.LG

TL;DR: 提出了第一个用于多类预测任务的完美真实校准度量，解决了现有校准度量的非真实性问题，并证明了该度量在鲁棒性方面的优越表现。


<details>
  <summary>Details</summary>
Motivation: 现有校准度量存在非真实性问题，即会激励预测器通过撒谎来显得更校准。虽然Hartline等人为二元预测任务引入了第一个完美真实校准度量，但多类预测任务仍缺乏此类度量。

Method: 将Hartline等人的二元预测真实校准度量推广到多类预测，研究了从二元到多类校准度量的常见扩展方法，识别了哪些方法能保持真实性。

Result: 成功构建了多类预测的完美真实校准度量，数学证明和实证验证表明该度量具有优越的鲁棒性，能稳健地保持主导和被主导预测器之间的排序关系。

Conclusion: 提出的多类真实校准度量不仅解决了真实性问题，还解决了分箱ECE的非鲁棒性问题，为多类预测的校准评估提供了可靠工具。

Abstract: Calibrated predictions can be reliably interpreted as probabilities. An
important step towards achieving better calibration is to design an appropriate
calibration measure to meaningfully assess the miscalibration level of a
predictor. A recent line of work initiated by Haghtalab et al. [2024] studies
the design of truthful calibration measures: a truthful measure is minimized
when a predictor outputs the true probabilities, whereas a non-truthful measure
incentivizes the predictor to lie so as to appear more calibrated. All previous
calibration measures were non-truthful until Hartline et al. [2025] introduced
the first perfectly truthful calibration measures for binary prediction tasks
in the batch setting.
  We introduce a perfectly truthful calibration measure for multi-class
prediction tasks, generalizing the work of Hartline et al. [2025] beyond binary
prediction. We study common methods of extending calibration measures from
binary to multi-class prediction and identify ones that do or do not preserve
truthfulness. In addition to truthfulness, we mathematically prove and
empirically verify that our calibration measure exhibits superior robustness:
it robustly preserves the ordering between dominant and dominated predictors,
regardless of the choice of hyperparameters (bin sizes). This result addresses
the non-robustness issue of binned ECE, which has been observed repeatedly in
prior work.

</details>


### [27] [Relational Transformer: Toward Zero-Shot Foundation Models for Relational Data](https://arxiv.org/abs/2510.06377)
*Rishabh Ranjan,Valter Hudovernik,Mark Znidar,Charilaos Kanatsoulis,Roshan Upendra,Mahmoud Mohammadi,Joe Meyer,Tom Palczewski,Carlos Guestrin,Jure Leskovec*

Main category: cs.LG

TL;DR: 提出了关系型Transformer（RT）架构，能够在不同关系数据库上预训练，并直接应用于未见过的数据集和任务，无需特定微调或上下文示例检索。


<details>
  <summary>Details</summary>
Motivation: 解决关系型领域缺乏能够跨数据集和任务迁移的架构问题，核心挑战是关系数据的多样性，包括异构模式、图结构和功能依赖。

Method: RT架构通过（i）使用表/列元数据对单元格进行标记化，（ii）通过掩码标记预测进行预训练，（iii）采用新型关系注意力机制覆盖列、行和主外键链接。

Result: 在RelBench数据集上预训练后，RT在零样本设置下达到强性能，在二元分类任务中平均达到完全监督AUROC的94%，而27B LLM仅为84%。微调后能以高样本效率获得最先进结果。

Conclusion: RT为零样本迁移利用了任务-表上下文、关系注意力模式和模式语义，为关系数据的基础模型提供了实用路径。

Abstract: Pretrained transformers readily adapt to new sequence modeling tasks via
zero-shot prompting, but relational domains still lack architectures that
transfer across datasets and tasks. The core challenge is the diversity of
relational data, with varying heterogeneous schemas, graph structures and
functional dependencies. In this paper, we present the Relational Transformer
(RT) architecture, which can be pretrained on diverse relational databases and
directly applied to unseen datasets and tasks without task- or dataset-specific
fine-tuning, or retrieval of in-context examples. RT (i) tokenizes cells with
table/column metadata, (ii) is pretrained via masked token prediction, and
(iii) utilizes a novel \textit{Relational Attention} mechanism over columns,
rows, and primary-foreign key links. Pretrained on RelBench datasets spanning
tasks such as churn and sales forecasting, RT attains strong zero-shot
performance, averaging 94% of fully supervised AUROC on binary classification
tasks with a single forward pass of a 22M parameter model, as opposed to 84%
for a 27B LLM. Fine-tuning yields state-of-the-art results with high sample
efficiency. Our experiments show that RT's zero-shot transfer harnesses
task-table context, relational attention patterns and schema semantics.
Overall, RT provides a practical path toward foundation models for relational
data.

</details>


### [28] [Context-Aware Inference via Performance Forecasting in Decentralized Learning Networks](https://arxiv.org/abs/2510.06444)
*Joel Pfeffer,J. M. Diederik Kruijssen,Clément Gossart,Mélanie Chevance,Diego Campo Millan,Florian Stecker,Steven N. Longmore*

Main category: cs.LG

TL;DR: 提出了一种在去中心化学习网络中使用机器学习预测模型性能的方法，通过前瞻性地分配权重来提高网络推理的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的线性池化方法（从简单平均到动态权重更新）存在反应性限制，难以快速适应环境变化。需要一种能够预测模型性能的前瞻性方法。

Method: 在去中心化学习网络中增加性能预测工作节点，使用机器学习模型预测每个模型在时间序列中的性能，基于预测结果分配更高的权重给可能更准确的模型。

Result: 预测遗憾或遗憾z分数的模型比预测损失的模型表现更好，能够超越简单的网络推理（历史加权平均）。性能预测模型对特征集选择和训练周期数敏感。

Conclusion: 性能预测方法为预测组合提供了一种前瞻性而非反应性的模型加权策略，不仅适用于去中心化学习网络，也可用于任何需要预测性模型加权的场景。

Abstract: In decentralized learning networks, predictions from many participants are
combined to generate a network inference. While many studies have demonstrated
performance benefits of combining multiple model predictions, existing
strategies using linear pooling methods (ranging from simple averaging to
dynamic weight updates) face a key limitation. Dynamic prediction combinations
that rely on historical performance to update weights are necessarily reactive.
Due to the need to average over a reasonable number of epochs (with moving
averages or exponential weighting), they tend to be slow to adjust to changing
circumstances (phase or regime changes). In this work, we develop a model that
uses machine learning to forecast the performance of predictions by models at
each epoch in a time series. This enables `context-awareness' by assigning
higher weight to models that are likely to be more accurate at a given time. We
show that adding a performance forecasting worker in a decentralized learning
network, following a design similar to the Allora network, can improve the
accuracy of network inferences. Specifically, we find forecasting models that
predict regret (performance relative to the network inference) or regret
z-score (performance relative to other workers) show greater improvement than
models predicting losses, which often do not outperform the naive network
inference (historically weighted average of all inferences). Through a series
of optimization tests, we show that the performance of the forecasting model
can be sensitive to choices in the feature set and number of training epochs.
These properties may depend on the exact problem and should be tailored to each
domain. Although initially designed for a decentralized learning network, using
performance forecasting for prediction combination may be useful in any
situation where predictive rather than reactive model weighting is needed.

</details>


### [29] [Vectorized FlashAttention with Low-cost Exponential Computation in RISC-V Vector Processors](https://arxiv.org/abs/2510.06834)
*Vasileios Titopoulos,Kosmas Alexandridis,Giorgos Dimitrakopoulos*

Main category: cs.LG

TL;DR: 本文首次在RISC-V向量处理器上实现了FlashAttention算法的向量化，通过低成本的指数函数近似和分块策略优化，显著提升了注意力机制的计算性能。


<details>
  <summary>Details</summary>
Motivation: 注意力机制是众多机器学习和人工智能模型的核心操作，但现有实现存在计算复杂度高、标量代码多的问题，需要针对向量处理器进行优化。

Method: 采用FlashAttention算法，在RISC-V向量处理器上实现向量化，使用低成本的浮点指数函数近似方法，避免扩展向量指令集，并探索合适的分块策略来改善内存局部性。

Result: 实验结果显示该方法具有良好的可扩展性，在实际应用的注意力层处理中，向量化实现带来了显著的性能提升。

Conclusion: 提出的向量化FlashAttention方法有效降低了计算复杂度，无需扩展指令集即可实现高性能的注意力计算，为向量处理器上的注意力机制加速提供了可行方案。

Abstract: Attention is a core operation in numerous machine learning and artificial
intelligence models. This work focuses on the acceleration of attention kernel
using FlashAttention algorithm, in vector processors, particularly those based
on the RISC-V instruction set architecture (ISA). This work represents the
first effort to vectorize FlashAttention, minimizing scalar code and
simplifying the computational complexity of evaluating exponentials needed by
softmax used in attention. By utilizing a low-cost approximation for
exponentials in floating-point arithmetic, we reduce the cost of computing the
exponential function without the need to extend baseline vector ISA with new
custom instructions. Also, appropriate tiling strategies are explored with the
goal to improve memory locality. Experimental results highlight the scalability
of our approach, demonstrating significant performance gains with the
vectorized implementations when processing attention layers in practical
applications.

</details>


### [30] [DPMM-CFL: Clustered Federated Learning via Dirichlet Process Mixture Model Nonparametric Clustering](https://arxiv.org/abs/2510.07132)
*Mariona Jaramillo-Civill,Peng Wu,Pau Closas*

Main category: cs.LG

TL;DR: 提出DPMM-CFL算法，使用狄利克雷过程先验自动推断聚类数量和客户端分配，解决传统聚类联邦学习需要预先指定聚类数的问题。


<details>
  <summary>Details</summary>
Motivation: 传统聚类联邦学习方法需要预先固定聚类数量K，这在潜在结构未知时不可行。需要一种能自动推断聚类数量的方法。

Method: 在聚类参数分布上放置狄利克雷过程先验，通过非参数贝叶斯推断联合推断聚类数量和客户端分配，同时优化每个聚类的联邦目标。

Result: 在Dirichlet和类分割非IID分区下的基准数据集上验证了算法有效性。

Conclusion: DPMM-CFL成功解决了聚类联邦学习中需要预先指定聚类数的问题，实现了自动聚类推断。

Abstract: Clustered Federated Learning (CFL) improves performance under non-IID client
heterogeneity by clustering clients and training one model per cluster, thereby
balancing between a global model and fully personalized models. However, most
CFL methods require the number of clusters K to be fixed a priori, which is
impractical when the latent structure is unknown. We propose DPMM-CFL, a CFL
algorithm that places a Dirichlet Process (DP) prior over the distribution of
cluster parameters. This enables nonparametric Bayesian inference to jointly
infer both the number of clusters and client assignments, while optimizing
per-cluster federated objectives. This results in a method where, at each
round, federated updates and cluster inferences are coupled, as presented in
this paper. The algorithm is validated on benchmark datasets under Dirichlet
and class-split non-IID partitions.

</details>


### [31] [Spiral Model Technique For Data Science & Machine Learning Lifecycle](https://arxiv.org/abs/2510.06987)
*Rohith Mahadevan*

Main category: cs.LG

TL;DR: 提出了一种新的螺旋技术来改进数据科学生命周期，强调在具有明确目标的企业问题中的灵活性、敏捷性和迭代方法。


<details>
  <summary>Details</summary>
Motivation: 传统数据科学生命周期通常被描述为线性或循环模型，但企业需要更灵活、迭代的方法来处理具有明确目标的数据依赖项目，以提高生产力和竞争力。

Method: 引入螺旋技术，这是一种新的数据科学生命周期方法，强调在业务过程中的多样性、敏捷性和迭代方法。

Result: 螺旋技术为企业数据科学项目提供了更灵活和迭代的生命周期框架，能够更好地适应具有明确目标的企业问题。

Conclusion: 螺旋技术是对传统数据科学生命周期的改进，为企业数据科学项目提供了更有效的框架，特别适用于具有明确目标的问题场景。

Abstract: Analytics play an important role in modern business. Companies adapt data
science lifecycles to their culture to seek productivity and improve their
competitiveness among others. Data science lifecycles are fairly an important
contributing factor to start and end a project that are data dependent. Data
science and Machine learning life cycles comprises of series of steps that are
involved in a project. A typical life cycle states that it is a linear or
cyclical model that revolves around. It is mostly depicted that it is possible
in a traditional data science life cycle to start the process again after
reaching the end of cycle. This paper suggests a new technique to incorporate
data science life cycle to business problems that have a clear end goal. A new
technique called spiral technique is introduced to emphasize versatility,
agility and iterative approach to business processes.

</details>


### [32] [RareGraph-Synth: Knowledge-Guided Diffusion Models for Generating Privacy-Preserving Synthetic Patient Trajectories in Ultra-Rare Diseases](https://arxiv.org/abs/2510.06267)
*Khartik Uppalapati,Shakeel Abdulkareem,Bora Yimenicioglu*

Main category: cs.LG

TL;DR: RareGraph-Synth是一个基于知识图谱引导的连续时间扩散框架，用于生成真实且保护隐私的超罕见疾病电子健康记录轨迹。该方法通过整合多个公共资源构建异质知识图谱，利用元路径分数调节扩散过程，在保持模型稳定性的同时生成生物学上合理的医疗数据。


<details>
  <summary>Details</summary>
Motivation: 为超罕见疾病研究生成真实且保护隐私的合成电子健康记录数据，解决罕见疾病数据稀缺和隐私保护之间的矛盾。

Method: 整合Orphanet/Orphadata、HPO、GARD、PrimeKG和FAERS五个公共资源构建包含约800万条边的异质知识图谱，使用元路径分数调节扩散模型的前向噪声调度，引导生成生物学上合理的实验室-药物-不良事件共现模式。

Result: 在模拟超罕见疾病队列上，相比无引导扩散基线，RareGraph-Synth将分类最大均值差异降低了40%，相比GAN对应方法降低了60%以上，同时保持下游预测效用。黑盒成员推理评估显示AUROC约为0.53，远低于0.55的安全发布阈值。

Conclusion: 将生物医学知识图谱直接整合到扩散噪声调度中可以同时提高保真度和隐私保护，为罕见疾病研究实现更安全的数据共享。

Abstract: We propose RareGraph-Synth, a knowledge-guided, continuous-time diffusion
framework that generates realistic yet privacy-preserving synthetic
electronic-health-record (EHR) trajectories for ultra-rare diseases.
RareGraph-Synth unifies five public resources: Orphanet/Orphadata, the Human
Phenotype Ontology (HPO), the GARD rare-disease KG, PrimeKG, and the FDA
Adverse Event Reporting System (FAERS) into a heterogeneous knowledge graph
comprising approximately 8 M typed edges. Meta-path scores extracted from this
8-million-edge KG modulate the per-token noise schedule in the forward
stochastic differential equation, steering generation toward biologically
plausible lab-medication-adverse-event co-occurrences while retaining
score-based diffusion model stability. The reverse denoiser then produces
timestamped sequences of lab-code, medication-code, and adverse-event-flag
triples that contain no protected health information. On simulated
ultra-rare-disease cohorts, RareGraph-Synth lowers categorical Maximum Mean
Discrepancy by 40 percent relative to an unguided diffusion baseline and by
greater than 60 percent versus GAN counterparts, without sacrificing downstream
predictive utility. A black-box membership-inference evaluation using the
DOMIAS attacker yields AUROC approximately 0.53, well below the 0.55
safe-release threshold and substantially better than the approximately 0.61
plus or minus 0.03 observed for non-KG baselines, demonstrating strong
resistance to re-identification. These results suggest that integrating
biomedical knowledge graphs directly into diffusion noise schedules can
simultaneously enhance fidelity and privacy, enabling safer data sharing for
rare-disease research.

</details>


### [33] [A Multi-Agent Framework for Stateful Inference-Time Search](https://arxiv.org/abs/2510.07147)
*Arshika Lalan,Rajat Ghosh,Aditya Kolsur,Debojyoti Dutta*

Main category: cs.LG

TL;DR: 提出了一种基于状态保持的多智能体进化搜索框架，用于自动生成单元测试用例，相比无状态方法在代码覆盖率上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的无状态推理方法在多步骤任务上表现不佳，缺乏持久状态；而任务特定的微调方法在需要深度推理和长程依赖的任务上仍然脆弱。

Method: 结合持久推理状态、对抗性变异和进化保留，通过专门的智能体序列化地提出、变异和评分候选用例，控制器在代际间维护持久状态。

Result: 在HumanEval和TestGenEvalMini等基准测试上，使用Llama、Gemma和GPT三种LLM家族，该框架在代码覆盖率上相比无状态单步基线方法有显著提升。

Conclusion: 将持久推理状态与进化搜索相结合，能够显著改进单元测试生成的质量和覆盖率。

Abstract: Recent work explores agentic inference-time techniques to perform structured,
multi-step reasoning. However, stateless inference often struggles on
multi-step tasks due to the absence of persistent state. Moreover,
task-specific fine-tuning or instruction-tuning often achieve surface-level
code generation but remain brittle on tasks requiring deeper reasoning and
long-horizon dependencies. To address these limitations, we propose stateful
multi-agent evolutionary search, a training-free framework that departs from
prior stateless approaches by combining (i) persistent inference-time state,
(ii) adversarial mutation, and (iii) evolutionary preservation. We demonstrate
its effectiveness in automated unit test generation through the generation of
edge cases. We generate robust edge cases using an evolutionary search process,
where specialized agents sequentially propose, mutate, and score candidates. A
controller maintains persistent state across generations, while evolutionary
preservation ensures diversity and exploration across all possible cases. This
yields a generalist agent capable of discovering robust, high-coverage edge
cases across unseen codebases. Experiments show our stateful multi-agent
inference framework achieves substantial gains in coverage over stateless
single-step baselines, evaluated on prevalent unit-testing benchmarks such as
HumanEval and TestGenEvalMini and using three diverse LLM families - Llama,
Gemma, and GPT. These results indicate that combining persistent inference-time
state with evolutionary search materially improves unit-test generation.

</details>


### [34] [MCCE: A Framework for Multi-LLM Collaborative Co-Evolution](https://arxiv.org/abs/2510.06270)
*Nian Ran,Zhongzheng Li,Yue Wang,Qingsong Ran,Xiaoyuan Zhang,Shikun Feng,Richard Allmendinger,Xiaoguang Zhao*

Main category: cs.LG

TL;DR: 提出MCCE框架，结合冻结的闭源大语言模型和轻量级可训练模型，通过强化学习持续优化小模型，实现多目标离散优化问题的协同进化。


<details>
  <summary>Details</summary>
Motivation: 解决多目标离散优化问题（如分子设计）中传统进化算法易陷局部最优，以及闭源LLM无法更新参数、开源小模型缺乏广泛知识和推理能力的问题。

Method: MCCE框架：维护搜索过程轨迹记忆，通过强化学习逐步精炼小模型，两个模型在全局探索中相互支持和补充，不同于模型蒸馏的相互启发增强。

Result: 在多目标药物设计基准测试中，MCCE实现了最先进的Pareto前沿质量，并持续优于基线方法。

Conclusion: MCCE展示了一种新的混合LLM系统持续进化范式，结合了知识驱动探索和经验驱动学习。

Abstract: Multi-objective discrete optimization problems, such as molecular design,
pose significant challenges due to their vast and unstructured combinatorial
spaces. Traditional evolutionary algorithms often get trapped in local optima,
while expert knowledge can provide crucial guidance for accelerating
convergence. Large language models (LLMs) offer powerful priors and reasoning
ability, making them natural optimizers when expert knowledge matters. However,
closed-source LLMs, though strong in exploration, cannot update their
parameters and thus cannot internalize experience. Conversely, smaller open
models can be continually fine-tuned but lack broad knowledge and reasoning
strength. We introduce Multi-LLM Collaborative Co-evolution (MCCE), a hybrid
framework that unites a frozen closed-source LLM with a lightweight trainable
model. The system maintains a trajectory memory of past search processes; the
small model is progressively refined via reinforcement learning, with the two
models jointly supporting and complementing each other in global exploration.
Unlike model distillation, this process enhances the capabilities of both
models through mutual inspiration. Experiments on multi-objective drug design
benchmarks show that MCCE achieves state-of-the-art Pareto front quality and
consistently outperforms baselines. These results highlight a new paradigm for
enabling continual evolution in hybrid LLM systems, combining knowledge-driven
exploration with experience-driven learning.

</details>


### [35] [RVFL-X: A Novel Randomized Network Based on Complex Transformed Real-Valued Tabular Datasets](https://arxiv.org/abs/2510.06278)
*M. Sajid,Mushir Akhtar,A. Quadir,M. Tanveer*

Main category: cs.LG

TL;DR: 提出了RVFL-X，一种复数版本的随机向量函数链接网络，通过两种方法将实值数据集转换为复数表示，在80个UCI数据集上表现优于原始RVFL和最先进的随机神经网络变体。


<details>
  <summary>Details</summary>
Motivation: 复数神经网络具有更强的表示能力，但在随机神经网络中的应用受限，因为缺乏将实值表格数据集转换为复数表示的有效方法。

Method: 提出了两种生成复数表示的方法：自然变换和自编码器驱动方法，并在此基础上构建了RVFL-X网络，该网络使用复数输入、权重和激活函数处理复数表示，但输出实值结果。

Result: 在80个真实UCI数据集上的综合评估显示，RVFL-X始终优于原始RVFL和最先进的随机神经网络变体，展示了其在不同应用领域的鲁棒性和有效性。

Conclusion: RVFL-X成功地将复数表示集成到随机神经网络中，同时保持了原始RVFL架构的简单性和效率，为复数神经网络在表格数据上的应用提供了有效解决方案。

Abstract: Recent advancements in neural networks, supported by foundational theoretical
insights, emphasize the superior representational power of complex numbers.
However, their adoption in randomized neural networks (RNNs) has been limited
due to the lack of effective methods for transforming real-valued tabular
datasets into complex-valued representations. To address this limitation, we
propose two methods for generating complex-valued representations from
real-valued datasets: a natural transformation and an autoencoder-driven
method. Building on these mechanisms, we propose RVFL-X, a complex-valued
extension of the random vector functional link (RVFL) network. RVFL-X
integrates complex transformations into real-valued datasets while maintaining
the simplicity and efficiency of the original RVFL architecture. By leveraging
complex components such as input, weights, and activation functions, RVFL-X
processes complex representations and produces real-valued outputs.
Comprehensive evaluations on 80 real-valued UCI datasets demonstrate that
RVFL-X consistently outperforms both the original RVFL and state-of-the-art
(SOTA) RNN variants, showcasing its robustness and effectiveness across diverse
application domains.

</details>


### [36] [On knot detection via picture recognition](https://arxiv.org/abs/2510.06284)
*Anne Dranowski,Yura Kabkov,Daniel Tubbenhauer*

Main category: cs.LG

TL;DR: 本文提出了一种结合机器学习和传统算法的方法，旨在通过照片自动识别绳结，使用CNN和transformer从图像预测交叉数，并计划结合符号重建进行更精确的绳结分类。


<details>
  <summary>Details</summary>
Motivation: 目标是开发能够通过手机拍摄绳结照片并自动识别绳结的系统，将现代机器学习方法与传统拓扑不变量计算相结合。

Method: 使用卷积神经网络和transformer架构从图像直接预测交叉数，并计划结合符号重建生成平面图代码，用于计算Jones多项式等量子不变量。

Result: 实验表明即使是轻量级的CNN和transformer架构也能从图像中恢复有意义的绳结结构信息。

Conclusion: 这种两阶段方法展示了机器学习处理噪声视觉数据与传统不变量强制执行严格拓扑区分之间的互补性，为稳健的绳结分类提供了可行路径。

Abstract: Our goal is to one day take a photo of a knot and have a phone automatically
recognize it. In this expository work, we explain a strategy to approximate
this goal, using a mixture of modern machine learning methods (in particular
convolutional neural networks and transformers for image recognition) and
traditional algorithms (to compute quantum invariants like the Jones
polynomial). We present simple baselines that predict crossing number directly
from images, showing that even lightweight CNN and transformer architectures
can recover meaningful structural information. The longer-term aim is to
combine these perception modules with symbolic reconstruction into planar
diagram (PD) codes, enabling downstream invariant computation for robust knot
classification. This two-stage approach highlights the complementarity between
machine learning, which handles noisy visual data, and invariants, which
enforce rigorous topological distinctions.

</details>


### [37] [Traj-Transformer: Diffusion Models with Transformer for GPS Trajectory Generation](https://arxiv.org/abs/2510.06291)
*Zhiyang Zhang,Ningcong Chen,Xin Zhang,Yanhua Li,Shen Su,Hui Lu,Jun Luo*

Main category: cs.LG

TL;DR: 提出Trajectory Transformer模型，使用transformer架构进行轨迹生成，相比基于卷积的方法能更好地保持街道级细节并减少偏差。


<details>
  <summary>Details</summary>
Motivation: 现有基于卷积架构的扩散模型在轨迹生成中存在明显偏差和细粒度街道细节丢失问题，需要更强大的模型容量。

Method: 使用transformer骨干网络进行条件信息嵌入和噪声预测，探索了两种GPS坐标嵌入策略（位置嵌入和经纬度嵌入），并在不同尺度分析模型性能。

Result: 在两个真实世界数据集上的实验表明，Trajectory Transformer显著提高了生成质量，有效缓解了先前方法的偏差问题。

Conclusion: Transformer架构在轨迹生成任务中比卷积架构表现更优，能够更好地保持轨迹的细节和准确性。

Abstract: The widespread use of GPS devices has driven advances in spatiotemporal data
mining, enabling machine learning models to simulate human decision making and
generate realistic trajectories, addressing both data collection costs and
privacy concerns. Recent studies have shown the promise of diffusion models for
high-quality trajectory generation. However, most existing methods rely on
convolution based architectures (e.g. UNet) to predict noise during the
diffusion process, which often results in notable deviations and the loss of
fine-grained street-level details due to limited model capacity. In this paper,
we propose Trajectory Transformer, a novel model that employs a transformer
backbone for both conditional information embedding and noise prediction. We
explore two GPS coordinate embedding strategies, location embedding and
longitude-latitude embedding, and analyze model performance at different
scales. Experiments on two real-world datasets demonstrate that Trajectory
Transformer significantly enhances generation quality and effectively
alleviates the deviation issues observed in prior approaches.

</details>


### [38] [BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level Autoregression](https://arxiv.org/abs/2510.06293)
*Cristian Meo,Varun Sarathchandran,Avijit Majhi,Shao Hung,Carlo Saccardi,Ruben Imhoff,Roberto Deidda,Remko Uijlenhoet,Justin Dauwels*

Main category: cs.LG

TL;DR: BlockGPT是一种用于降水临近预报的生成式自回归变换器，通过批量标记化方法预测完整二维场，在准确性和推理速度方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决当前降水临近预报方法中存在的归纳偏差问题、推理速度慢以及扩散模型计算量大等局限性，需要开发既准确又高效的实时预报模型。

Method: 提出BlockGPT模型，采用批量标记化方法，在每个时间步预测完整的二维降水场。该模型将时空分解，在帧内使用自注意力机制，在帧间使用因果注意力机制。

Result: 在KNMI和SEVIR两个降水数据集上的评估显示，BlockGPT在准确性、事件定位和推理速度方面均优于现有基准模型，推理速度比可比基准模型快达31倍。

Conclusion: BlockGPT为降水临近预报提供了一种高效准确的解决方案，其模型无关的范式也可应用于其他视频预测任务。

Abstract: Predicting precipitation maps is a highly complex spatiotemporal modeling
task, critical for mitigating the impacts of extreme weather events. Short-term
precipitation forecasting, or nowcasting, requires models that are not only
accurate but also computationally efficient for real-time applications. Current
methods, such as token-based autoregressive models, often suffer from flawed
inductive biases and slow inference, while diffusion models can be
computationally intensive. To address these limitations, we introduce BlockGPT,
a generative autoregressive transformer using batched tokenization (Block)
method that predicts full two-dimensional fields (frames) at each time step.
Conceived as a model-agnostic paradigm for video prediction, BlockGPT
factorizes space-time by using self-attention within each frame and causal
attention across frames; in this work, we instantiate it for precipitation
nowcasting. We evaluate BlockGPT on two precipitation datasets, viz. KNMI
(Netherlands) and SEVIR (U.S.), comparing it to state-of-the-art baselines
including token-based (NowcastingGPT) and diffusion-based (DiffCast+Phydnet)
models. The results show that BlockGPT achieves superior accuracy, event
localization as measured by categorical metrics, and inference speeds up to 31x
faster than comparable baselines.

</details>


### [39] [SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation](https://arxiv.org/abs/2510.06303)
*Shuang Cheng,Yihan Bian,Dawei Liu,Yuhua Jiang,Yihao Liu,Linfeng Zhang,Wenhai Wang,Qipeng Guo,Kai Chen,Biqing Qi,Bowen Zhou*

Main category: cs.LG

TL;DR: SDAR提出了一种协同扩散-自回归范式，将自回归模型的训练效率与扩散模型的并行推理能力相结合，通过轻量级转换将训练好的自回归模型转化为块级扩散模型。


<details>
  <summary>Details</summary>
Motivation: 结合自回归模型的训练效率和扩散模型的并行推理能力，避免昂贵的端到端扩散训练，实现高效的自回归到扩散转换。

Method: 通过轻量级范式转换将训练好的自回归模型转化为块级扩散模型，推理时在块间自回归生成以保证全局一致性，在块内通过离散扩散过程并行解码所有token。

Result: SDAR在保持自回归级别性能的同时实现并行生成，30B MoE模型在GPQA和ChemBench等科学推理基准上超越其自回归对应模型，并在多数投票和pass@k等测试时缩放方法下获得进一步改进。

Conclusion: SDAR作为一个实用范式，成功结合了自回归和扩散的优势，实现了可扩展、高吞吐量的推理。

Abstract: We propose SDAR, a Synergistic Diffusion-Autoregression paradigm that unifies
the training efficiency of autoregressive models with the parallel inference
capability of diffusion. Instead of costly end-to-end diffusion training, SDAR
performs a lightweight paradigm conversion that transforms a well-trained
autoregressive (AR) model into a blockwise diffusion model through brief,
data-efficient adaptation. During inference, SDAR generates sequences
autoregressively across blocks for global coherence while decoding all tokens
within each block in parallel via a discrete diffusion process. Extensive
experiments show that AR models remain substantially more compute-efficient
than masked diffusion models, providing a strong foundation for adaptation.
Building on this insight, SDAR achieves efficient AR-to-diffusion conversion
with minimal cost, preserving AR-level performance while enabling parallel
generation. Scaling studies across dense and Mixture-of-Experts architectures
confirm that SDAR scales without compromise: larger models exhibit stronger
robustness to block size and decoding thresholds, yielding greater speedups
without accuracy loss. Beyond efficiency, SDAR demonstrates enhanced reasoning
and domain adaptability. Our 30B MoE model surpasses its AR counterpart on
challenging scientific reasoning benchmarks such as GPQA and ChemBench, and
gains further improvements under test-time scaling methods like majority voting
and pass@k. Together, these results establish SDAR as a practical paradigm that
combines the strengths of autoregression and diffusion for scalable,
high-throughput reasoning.

</details>


### [40] [Flexible Swarm Learning May Outpace Foundation Models in Essential Tasks](https://arxiv.org/abs/2510.06349)
*Moein E. Samadi,Andreas Schuppert*

Main category: cs.LG

TL;DR: 论文提出分散式小智能体网络架构作为替代单体基础模型的方法，认为在动态环境中群体学习能实现更好的决策，但会牺牲细节可重现性。


<details>
  <summary>Details</summary>
Motivation: 基础模型在真实世界动态环境中的决策能力有限，特别是在医疗等复杂系统中，需要开发能在数据有限和机制知识不足情况下自适应的AI方法。

Method: 提出分散式小智能体网络架构，每个智能体只覆盖系统功能的子集，通过群体学习实现自适应决策。

Result: 分散式小智能体网络在动态环境中能提供优于单体基础模型的决策能力，但代价是细节可重现性降低。

Conclusion: 在复杂动态系统中，分散式小智能体网络架构比单体基础模型更适合实现自适应决策，但需要接受可重现性方面的折衷。

Abstract: Foundation models have rapidly advanced AI, raising the question of whether
their decisions will ultimately surpass human strategies in real-world domains.
The exponential, and possibly super-exponential, pace of AI development makes
such analysis elusive. Nevertheless, many application areas that matter for
daily life and society show only modest gains so far; a prominent case is
diagnosing and treating dynamically evolving disease in intensive care.
  The common challenge is adapting complex systems to dynamic environments.
Effective strategies must optimize outcomes in systems composed of strongly
interacting functions while avoiding shared side effects; this requires
reliable, self-adaptive modeling. These tasks align with building digital twins
of highly complex systems whose mechanisms are not fully or quantitatively
understood. It is therefore essential to develop methods for self-adapting AI
models with minimal data and limited mechanistic knowledge. As this challenge
extends beyond medicine, AI should demonstrate clear superiority in these
settings before assuming broader decision-making roles.
  We identify the curse of dimensionality as a fundamental barrier to efficient
self-adaptation and argue that monolithic foundation models face conceptual
limits in overcoming it. As an alternative, we propose a decentralized
architecture of interacting small agent networks (SANs). We focus on agents
representing the specialized substructure of the system, where each agent
covers only a subset of the full system functions. Drawing on mathematical
results on the learning behavior of SANs and evidence from existing
applications, we argue that swarm-learning in diverse swarms can enable
self-adaptive SANs to deliver superior decision-making in dynamic environments
compared with monolithic foundation models, though at the cost of reduced
reproducibility in detail.

</details>


### [41] [PIKAN: Physics-Inspired Kolmogorov-Arnold Networks for Explainable UAV Channel Modelling](https://arxiv.org/abs/2510.06355)
*Kürşat Tekbıyık,Güneş Karabulut Kurt,Antoine Lesage-Landry*

Main category: cs.LG

TL;DR: 提出PIKAN模型，将物理原理嵌入学习过程，在无人机通信中实现准确且可解释的A2G信道建模，比传统DL模型轻量37倍。


<details>
  <summary>Details</summary>
Motivation: 无人机通信需要准确且可解释的A2G信道模型，但确定性模型过于刚性，深度学习模型缺乏可解释性，需要结合两者的优势。

Method: 提出物理启发的Kolmogorov-Arnold网络(PIKAN)，将自由空间路径损耗、双射线反射等物理原理作为灵活的归纳偏置嵌入学习过程。

Result: 在无人机A2G测量数据上的实验显示，PIKAN达到与DL模型相当的精度，同时提供符合传播定律的符号化可解释表达式，仅需232个参数，比多层感知机轻量37倍。

Conclusion: PIKAN为5G和6G网络中的无人机信道建模提供了高效、可解释且可扩展的解决方案。

Abstract: Unmanned aerial vehicle (UAV) communications demand accurate yet
interpretable air-to-ground (A2G) channel models that can adapt to
nonstationary propagation environments. While deterministic models offer
interpretability and deep learning (DL) models provide accuracy, both
approaches suffer from either rigidity or a lack of explainability. To bridge
this gap, we propose the Physics-Inspired Kolmogorov-Arnold Network (PIKAN)
that embeds physical principles (e.g., free-space path loss, two-ray
reflections) into the learning process. Unlike physics-informed neural networks
(PINNs), PIKAN is more flexible for applying physical information because it
introduces them as flexible inductive biases. Thus, it enables a more flexible
training process. Experiments on UAV A2G measurement data show that PIKAN
achieves comparable accuracy to DL models while providing symbolic and
explainable expressions aligned with propagation laws. Remarkably, PIKAN
achieves this performance with only 232 parameters, making it up to 37 times
lighter than multilayer perceptron (MLP) baselines with thousands of
parameters, without sacrificing correlation with measurements and also
providing symbolic expressions. These results highlight PIKAN as an efficient,
interpretable, and scalable solution for UAV channel modelling in beyond-5G and
6G networks.

</details>


### [42] [Lagrangian neural ODEs: Measuring the existence of a Lagrangian with Helmholtz metrics](https://arxiv.org/abs/2510.06367)
*Luca Wolf,Tobias Buck,Bjoern Malte Schaefer*

Main category: cs.LG

TL;DR: 该论文提出了Helmholtz指标来量化ODE与欧拉-拉格朗日方程的相似度，并结合二阶神经ODE构建拉格朗日神经ODE，能够直接学习欧拉-拉格朗日方程且无额外推理成本。


<details>
  <summary>Details</summary>
Motivation: 神经ODE是强大的机器学习技术，但并非所有解都是物理的欧拉-拉格朗日方程。需要量化ODE与物理系统的相似度，并改进神经ODE使其能直接学习拉格朗日系统。

Method: 提出Helmholtz指标来评估ODE与欧拉-拉格朗日方程的相似性，结合二阶神经ODE构建拉格朗日神经ODE，仅使用位置数据即可学习。

Result: 该方法能区分拉格朗日和非拉格朗日系统，在多个基础系统（含噪声）上验证了有效性，并改进了神经ODE的解。

Conclusion: Helmholtz指标和拉格朗日神经ODE提供了一种直接学习欧拉-拉格朗日方程的有效方法，零额外推理成本且能提升神经ODE性能。

Abstract: Neural ODEs are a widely used, powerful machine learning technique in
particular for physics. However, not every solution is physical in that it is
an Euler-Lagrange equation. We present Helmholtz metrics to quantify this
resemblance for a given ODE and demonstrate their capabilities on several
fundamental systems with noise. We combine them with a second order neural ODE
to form a Lagrangian neural ODE, which allows to learn Euler-Lagrange equations
in a direct fashion and with zero additional inference cost. We demonstrate
that, using only positional data, they can distinguish Lagrangian and
non-Lagrangian systems and improve the neural ODE solutions.

</details>


### [43] [Monte Carlo Permutation Search](https://arxiv.org/abs/2510.06381)
*Tristan Cazenave*

Main category: cs.LG

TL;DR: MCPS是一种改进GRAVE算法的蒙特卡洛树搜索方法，适用于无法使用深度强化学习或计算资源有限的情况。它通过包含从根节点到当前节点路径上所有移动的统计信息来改进探索项。


<details>
  <summary>Details</summary>
Motivation: 在无法使用深度强化学习或计算资源有限的情况下（如通用游戏博弈），需要改进现有的MCTS算法。GRAVE算法存在一些局限性，特别是对超参数的敏感性。

Method: MCPS在节点的探索项中包含从根节点到该节点路径上所有移动的统计信息。使用抽象代码代替精确代码来改进置换统计和AMAF统计，并提供了统计源权重公式的数学推导。

Result: 在多种游戏（棋盘游戏、战争游戏、投资游戏、视频游戏和多人游戏）上测试，MCPS在双人游戏中表现优于GRAVE，在多人游戏中表现相当。新公式消除了GRAVE的偏差超参数，且MCPS对ref超参数不敏感。

Conclusion: MCPS是一种有效的MCTS改进算法，在双人游戏中优于GRAVE，消除了超参数敏感性，并通过抽象代码进一步提升了性能。

Abstract: We propose Monte Carlo Permutation Search (MCPS), a general-purpose Monte
Carlo Tree Search (MCTS) algorithm that improves upon the GRAVE algorithm. MCPS
is relevant when deep reinforcement learning is not an option, or when the
computing power available before play is not substantial, such as in General
Game Playing, for example. The principle of MCPS is to include in the
exploration term of a node the statistics on all the playouts that contain all
the moves on the path from the root to the node. We extensively test MCPS on a
variety of games: board games, wargame, investment game, video game and
multi-player games. MCPS has better results than GRAVE in all the two-player
games. It has equivalent results for multi-player games because these games are
inherently balanced even when players have different strengths. We also show
that using abstract codes for moves instead of exact codes can be beneficial to
both MCPS and GRAVE, as they improve the permutation statistics and the AMAF
statistics. We also provide a mathematical derivation of the formulas used for
weighting the three sources of statistics. These formulas are an improvement on
the GRAVE formula since they no longer use the bias hyperparameter of GRAVE.
Moreover, MCPS is not sensitive to the ref hyperparameter.

</details>


### [44] [Geometry-Aware Backdoor Attacks: Leveraging Curvature in Hyperbolic Embeddings](https://arxiv.org/abs/2510.06397)
*Ali Baheri*

Main category: cs.LG

TL;DR: 本文揭示了非欧几里得基础模型在双曲几何等弯曲空间中存在的边界驱动不对称性，这使得后门触发器能够利用几何特性进行攻击。


<details>
  <summary>Details</summary>
Motivation: 随着非欧几里得基础模型在弯曲空间（如双曲几何）中表示的增加，需要理解这些几何特性如何影响模型的安全性和脆弱性。

Method: 通过理论分析形式化了边界驱动不对称性效应，提出了一种简单的几何自适应触发器，并在多个任务和架构上进行了评估。

Result: 实证研究表明，攻击成功率随着接近边界而增加，而传统检测器的效果减弱，这与理论趋势一致。

Conclusion: 这些结果揭示了非欧几里得模型中特定于几何的脆弱性，并为设计和理解防御方法的局限性提供了分析支持的指导。

Abstract: Non-Euclidean foundation models increasingly place representations in curved
spaces such as hyperbolic geometry. We show that this geometry creates a
boundary-driven asymmetry that backdoor triggers can exploit. Near the
boundary, small input changes appear subtle to standard input-space detectors
but produce disproportionately large shifts in the model's representation
space. Our analysis formalizes this effect and also reveals a limitation for
defenses: methods that act by pulling points inward along the radius can
suppress such triggers, but only by sacrificing useful model sensitivity in
that same direction. Building on these insights, we propose a simple
geometry-adaptive trigger and evaluate it across tasks and architectures.
Empirically, attack success increases toward the boundary, whereas conventional
detectors weaken, mirroring the theoretical trends. Together, these results
surface a geometry-specific vulnerability in non-Euclidean models and offer
analysis-backed guidance for designing and understanding the limits of
defenses.

</details>


### [45] [The Effect of Label Noise on the Information Content of Neural Representations](https://arxiv.org/abs/2510.06401)
*Ali Hussaini Umar,Franky Kevin Nando Tezoh,Jean Barbier,Santiago Acevedo,Alessandro Laio*

Main category: cs.LG

TL;DR: 本文研究了标签噪声对深度神经网络隐藏表示的影响，发现隐藏表示的信息内容随网络参数数量呈现双下降现象，与测试误差行为类似。在欠参数化状态下，带噪声标签学习的表示比干净标签更丰富；在过参数化状态下，两者同样丰富。


<details>
  <summary>Details</summary>
Motivation: 虽然标签噪声对深度学习模型性能的影响已被广泛研究，但其对网络隐藏表示的影响仍不清楚。本文旨在填补这一空白，系统性地分析标签噪声如何影响隐藏表示的信息内容。

Method: 使用信息不平衡（Information Imbalance）作为条件互信息的计算高效代理，系统地比较不同网络参数化状态下的隐藏表示，分析干净标签、噪声标签和随机标签训练下的表示差异。

Result: 发现隐藏表示的信息内容随网络参数数量呈现双下降现象；在欠参数化状态下，带噪声标签学习的表示比干净标签更丰富；在过参数化状态下，两者同样丰富；随机标签训练的表现比随机特征更差。

Conclusion: 过参数化网络的表示对标签噪声具有鲁棒性；末层和softmax前层之间的信息不平衡随交叉熵损失减少；这为理解分类任务中的泛化提供了新视角；随机标签训练使网络远超惰性学习，权重适应编码标签信息。

Abstract: In supervised classification tasks, models are trained to predict a label for
each data point. In real-world datasets, these labels are often noisy due to
annotation errors. While the impact of label noise on the performance of deep
learning models has been widely studied, its effects on the networks' hidden
representations remain poorly understood. We address this gap by systematically
comparing hidden representations using the Information Imbalance, a
computationally efficient proxy of conditional mutual information. Through this
analysis, we observe that the information content of the hidden representations
follows a double descent as a function of the number of network parameters,
akin to the behavior of the test error. We further demonstrate that in the
underparameterized regime, representations learned with noisy labels are more
informative than those learned with clean labels, while in the
overparameterized regime, these representations are equally informative. Our
results indicate that the representations of overparameterized networks are
robust to label noise. We also found that the information imbalance between the
penultimate and pre-softmax layers decreases with cross-entropy loss in the
overparameterized regime. This offers a new perspective on understanding
generalization in classification tasks. Extending our analysis to
representations learned from random labels, we show that these perform worse
than random features. This indicates that training on random labels drives
networks much beyond lazy learning, as weights adapt to encode labels
information.

</details>


### [46] [Test-Time Efficient Pretrained Model Portfolios for Time Series Forecasting](https://arxiv.org/abs/2510.06419)
*Mert Kayaalp,Caner Turkmen,Oleksandr Shchur,Pedro Mercado,Abdul Fatir Ansari,Michael Bohlke-Schneider,Bernie Wang*

Main category: cs.LG

TL;DR: 本文探讨了使用小型预训练模型组合替代单一大型时间序列基础模型的可行性，发现专家模型组合在参数更少的情况下能达到竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 质疑"越大越好"的时间序列基础模型范式，探索更高效的替代方案：构建小型预训练模型组合。

Method: 通过集成学习或模型选择策略，使用专家模型组合而非独立训练的通用模型，并采用后训练基础模型的方法创建多样化专家模型。

Result: 在大型基准测试中，使用更少参数的小型模型组合实现了竞争性性能，专家模型组合始终优于通用模型组合。

Conclusion: 集成学习和模型选择比测试时微调更具计算效率，后训练基础模型是创建多样化专家模型的有效方法。

Abstract: Is bigger always better for time series foundation models? With the question
in mind, we explore an alternative to training a single, large monolithic
model: building a portfolio of smaller, pretrained forecasting models. By
applying ensembling or model selection over these portfolios, we achieve
competitive performance on large-scale benchmarks using much fewer parameters.
We explore strategies for designing such portfolios and find that collections
of specialist models consistently outperform portfolios of independently
trained generalists. Remarkably, we demonstrate that post-training a base model
is a compute-effective approach for creating sufficiently diverse specialists,
and provide evidences that ensembling and model selection are more
compute-efficient than test-time fine-tuning.

</details>


### [47] [Nearly Instance-Optimal Parameter Recovery from Many Trajectories via Hellinger Localization](https://arxiv.org/abs/2510.06434)
*Eliot Shekhtman,Yichen Zhou,Ingvar Ziemann,Nikolai Matni,Stephen Tu*

Main category: cs.LG

TL;DR: 本文提出了一个基于Hellinger定位框架的新方法，用于多轨迹时序数据中的实例最优学习，显著扩展了现有方法的适用范围，并在四个不同案例中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习中，从时序相关数据中学习是一个核心问题。特别是在多轨迹设置下，现有方法要么依赖混合假设，要么只能获得次优的样本效率保证，限制了实际应用。

Method: 采用Hellinger定位框架，首先在路径测度级别通过减少到i.i.d.学习来控制平方Hellinger距离，然后在参数空间中进行以轨迹Fisher信息加权的二次形式定位。

Result: 该方法在四个不同案例中（马尔可夫链混合、非高斯噪声下的依赖线性回归、非单调激活的广义线性模型、线性注意力序列模型）都获得了接近渐近正态性的实例最优率。

Conclusion: Hellinger定位框架显著扩展了多轨迹设置中实例最优率的适用范围，在多种条件下都能实现与全数据预算成比例的样本效率，大幅优于标准简化方法。

Abstract: Learning from temporally-correlated data is a core facet of modern machine
learning. Yet our understanding of sequential learning remains incomplete,
particularly in the multi-trajectory setting where data consists of many
independent realizations of a time-indexed stochastic process. This important
regime both reflects modern training pipelines such as for large foundation
models, and offers the potential for learning without the typical mixing
assumptions made in the single-trajectory case. However, instance-optimal
bounds are known only for least-squares regression with dependent covariates;
for more general models or loss functions, the only broadly applicable
guarantees result from a reduction to either i.i.d. learning, with effective
sample size scaling only in the number of trajectories, or an existing
single-trajectory result when each individual trajectory mixes, with effective
sample size scaling as the full data budget deflated by the mixing-time.
  In this work, we significantly broaden the scope of instance-optimal rates in
multi-trajectory settings via the Hellinger localization framework, a general
approach for maximum likelihood estimation. Our method proceeds by first
controlling the squared Hellinger distance at the path-measure level via a
reduction to i.i.d. learning, followed by localization as a quadratic form in
parameter space weighted by the trajectory Fisher information. This yields
instance-optimal bounds that scale with the full data budget under a broad set
of conditions. We instantiate our framework across four diverse case studies: a
simple mixture of Markov chains, dependent linear regression under non-Gaussian
noise, generalized linear models with non-monotonic activations, and
linear-attention sequence models. In all cases, our bounds nearly match the
instance-optimal rates from asymptotic normality, substantially improving over
standard reductions.

</details>


### [48] [Bayesian Optimization under Uncertainty for Training a Scale Parameter in Stochastic Models](https://arxiv.org/abs/2510.06439)
*Akash Yadav,Ruda Zhang*

Main category: cs.LG

TL;DR: 提出了一种针对不确定性下超参数调优的贝叶斯优化框架，特别关注随机模型中尺度或精度参数的优化，相比传统方法可减少40倍计算成本。


<details>
  <summary>Details</summary>
Motivation: 超参数调优在系统存在不确定性时具有挑战性，由于噪声函数评估，不确定性下的优化计算成本高昂。

Method: 使用统计代理模型表示基础随机变量，实现期望算子的解析评估，并推导出随机采集函数优化器的闭式解。

Result: 与传统一维蒙特卡洛优化方案相比，该方法需要40倍更少的数据点，计算成本降低高达40倍。

Conclusion: 该方法在计算工程的两个数值示例中证明了其有效性，显著降低了不确定性下超参数调优的计算负担。

Abstract: Hyperparameter tuning is a challenging problem especially when the system
itself involves uncertainty. Due to noisy function evaluations, optimization
under uncertainty can be computationally expensive. In this paper, we present a
novel Bayesian optimization framework tailored for hyperparameter tuning under
uncertainty, with a focus on optimizing a scale- or precision-type parameter in
stochastic models. The proposed method employs a statistical surrogate for the
underlying random variable, enabling analytical evaluation of the expectation
operator. Moreover, we derive a closed-form expression for the optimizer of the
random acquisition function, which significantly reduces computational cost per
iteration. Compared with a conventional one-dimensional Monte Carlo-based
optimization scheme, the proposed approach requires 40 times fewer data points,
resulting in up to a 40-fold reduction in computational cost. We demonstrate
the effectiveness of the proposed method through two numerical examples in
computational engineering.

</details>


### [49] [How NOT to benchmark your SITE metric: Beyond Static Leaderboards and Towards Realistic Evaluation](https://arxiv.org/abs/2510.06448)
*Prabhant Singh,Sibylle Hess,Joaquin Vanschoren*

Main category: cs.LG

TL;DR: 本文揭示了当前可迁移性评估指标基准存在的根本缺陷，指出不现实的模型空间和静态性能层次人为夸大了现有指标的表现，甚至简单的启发式方法也能超越复杂方法。


<details>
  <summary>Details</summary>
Motivation: 尽管可迁移性评估指标的研究日益增多，但用于衡量其进展的基准测试却很少被仔细检查。本文旨在揭示这些基准测试的缺陷。

Method: 通过实证分析，展示了广泛使用的基准设置评估可迁移性评估指标的不足之处，并论证这些基准从根本上存在缺陷。

Result: 研究发现，当前基准的不现实模型空间和静态性能层次人为地夸大了现有指标的表现，简单的数据集无关启发式方法甚至能超越复杂方法。

Conclusion: 当前评估协议与现实世界模型选择的复杂性存在严重脱节，需要构建更稳健和现实的基准来指导未来研究朝着更有意义的方向发展。

Abstract: Transferability estimation metrics are used to find a high-performing
pre-trained model for a given target task without fine-tuning models and
without access to the source dataset. Despite the growing interest in
developing such metrics, the benchmarks used to measure their progress have
gone largely unexamined. In this work, we empirically show the shortcomings of
widely used benchmark setups to evaluate transferability estimation metrics. We
argue that the benchmarks on which these metrics are evaluated are
fundamentally flawed. We empirically demonstrate that their unrealistic model
spaces and static performance hierarchies artificially inflate the perceived
performance of existing metrics, to the point where simple, dataset-agnostic
heuristics can outperform sophisticated methods. Our analysis reveals a
critical disconnect between current evaluation protocols and the complexities
of real-world model selection. To address this, we provide concrete
recommendations for constructing more robust and realistic benchmarks to guide
future research in a more meaningful direction.

</details>


### [50] [Attention Sinks and Compression Valleys in LLMs are Two Sides of the Same Coin](https://arxiv.org/abs/2510.06477)
*Enrique Queipo-de-Llano,Álvaro Arroyo,Federico Barbero,Xiaowen Dong,Michael Bronstein,Yann LeCun,Ravid Shwartz-Ziv*

Main category: cs.LG

TL;DR: 本文揭示了注意力汇聚和压缩谷之间的关联，两者都源于残差流中大规模激活的形成。作者提出了Mix-Compress-Refine信息流理论，解释LLMs通过控制大规模激活来组织深度计算。


<details>
  <summary>Details</summary>
Motivation: 注意力汇聚和压缩谷作为大语言模型中的两个谜题现象，此前一直被孤立研究。本文旨在揭示两者之间的内在联系，并建立统一的理论框架来解释LLMs的计算组织方式。

Method: 通过理论证明大规模激活必然导致表征压缩并建立熵减边界，在多个模型（410M-120B参数）上进行实验验证，进行针对性消融研究验证理论预测。

Result: 实验证实当序列开始token在中间层产生极端激活范数时，压缩谷和注意力汇聚同时出现。消融研究验证了理论预测的正确性。

Conclusion: 提出了Mix-Compress-Refine信息流理论，认为基于Transformer的LLMs在三个不同阶段处理token：早期层广泛混合、中间层压缩计算、晚期层选择性精炼。该框架解释了为什么嵌入任务在中间层表现最佳，而生成任务受益于全深度处理。

Abstract: Attention sinks and compression valleys have attracted significant attention
as two puzzling phenomena in large language models, but have been studied in
isolation. In this work, we present a surprising connection between attention
sinks and compression valleys, tracing both to the formation of massive
activations in the residual stream. We prove theoretically that massive
activations necessarily produce representational compression and establish
bounds on the resulting entropy reduction. Through experiments across several
models (410M-120B parameters), we confirm that when the beginning-of-sequence
token develops extreme activation norms in the middle layers, both compression
valleys and attention sinks emerge simultaneously. Targeted ablation studies
validate our theoretical predictions. This unified view motivates us to propose
the Mix-Compress-Refine theory of information flow, as an attempt to explain
how LLMs organize their computation in depth by controlling attention and
representational compression via massive activations. Specifically, we posit
that Transformer-based LLMs process tokens in three distinct phases: (1) broad
mixing in the early layers, (2) compressed computation with limited mixing in
the middle layers, and (3) selective refinement in the late layers. Our
framework helps explain why embedding tasks perform best at intermediate
layers, whereas generation tasks benefit from full-depth processing, clarifying
differences in task-dependent representations.

</details>


### [51] [Valid Stopping for LLM Generation via Empirical Dynamic Formal Lift](https://arxiv.org/abs/2510.06478)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.LG

TL;DR: Sequential-EDFL是一种基于经验动态形式提升的序列测试方法，用于语言模型生成停止控制。该方法通过跟踪信息提升（完整模型与弱化骨架基线之间的对数似然比），使用自归一化经验-伯努利e-过程，提供形式化的delta级错误控制。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型生成过程中的效率问题，通过序列测试方法在保持生成质量的同时减少不必要的生成，降低计算开销。

Method: 使用自归一化经验-伯努利e-过程跟踪信息提升，处理未知中心化问题，通过在线均值估计，结合混合e-过程处理多参数，支持分布漂移下的自适应重置。

Result: 在六个基准测试中，相比序列基线减少22-28%的生成量，同时保持delta级控制，计算开销为12%。与轻量级正确性门控结合可提高端任务正确性。

Conclusion: EDFL作为第一级过滤器可减少83%的验证负担，但不能作为安全关键领域的独立解决方案，因为10.9%的停止序列仍然不正确。

Abstract: We introduce Sequential-EDFL (Empirical Dynamic Formal Lift), applying
anytime-valid sequential testing to language model generation stopping. Our
approach tracks information lift -- the log-likelihood ratio between full
models and deliberately weakened "skeleton" baselines -- using self-normalized
empirical-Bernstein e-processes that provide formal delta-level error control
regardless of stopping time. We handle unknown centering through online mean
estimation, combine multiple parameters via mixture e-processes, and support
adaptive resets under distributional drift. On six benchmarks, Sequential-EDFL
reduces generation by 22-28% vs. sequential baselines while maintaining
delta-level control with 12% computational overhead. We introduce automated
skeletons (distilled submodels, randomized logits) and show robustness across
skeleton families. Composing EDFL with a lightweight correctness gate (sentence
boundaries + verifier) improves end-task correctness while preserving
anytime-valid guarantees by only delaying stopping. Our certificates control
information sufficiency, not factual correctness -- 10.9% of stopped sequences
remain incorrect even with the gate (13.2-22.7% without it). EDFL serves as a
first-stage filter reducing verification burden by 83%, not as a standalone
solution for safety-critical domains.

</details>


### [52] [GUIDE: Guided Initialization and Distillation of Embeddings](https://arxiv.org/abs/2510.06502)
*Khoa Trinh,Gaurav Menghani,Erik Vee*

Main category: cs.LG

TL;DR: GUIDE是一种新的蒸馏方法，通过让学生的参数空间匹配教师模型，在大型学生模型上实现了25-26%的师生质量差距减少，且不增加训练或推理开销。


<details>
  <summary>Details</summary>
Motivation: 标准蒸馏方法仅让学生匹配教师的输出，但考虑到训练大模型的成本，应该从教师模型中提取更多有用信息。

Method: GUIDE通过引导初始化和嵌入蒸馏，强制学生在参数空间上匹配教师模型，可以与知识蒸馏结合使用。

Result: 在400M-1B参数的大型学生模型上，使用约200亿token训练，师生质量差距减少了25-26%，且与知识蒸馏结合时效果接近叠加。

Conclusion: GUIDE方法显著优于单独使用知识蒸馏，且不引入额外训练或推理成本，实现了几乎免费的质量提升。

Abstract: Algorithmic efficiency techniques such as distillation
(\cite{hinton2015distillation}) are useful in improving model quality without
increasing serving costs, provided a larger teacher model is available for a
smaller student model to learn from during training. Standard distillation
methods are limited to only forcing the student to match the teacher's outputs.
Given the costs associated with training a large model, we believe we should be
extracting more useful information from a teacher model than by just making the
student match the teacher's outputs.
  In this paper, we introduce \guide (Guided Initialization and Distillation of
Embeddings). \guide can be considered a distillation technique that forces the
student to match the teacher in the parameter space. Using \guide we show
25-26\% reduction in the teacher-student quality gap when using large student
models (400M - 1B parameters) trained on $\approx$ 20B tokens. We also present
a thorough analysis demonstrating that \guide can be combined with knowledge
distillation with near additive improvements. Furthermore, we show that
applying \guide alone leads to substantially better model quality than applying
knowledge distillation by itself.
  Most importantly, \guide introduces no training or inference overhead and
hence any model quality gains from our method are virtually free.

</details>


### [53] [ATLO-ML: Adaptive Time-Length Optimizer for Machine Learning -- Insights from Air Quality Forecasting](https://arxiv.org/abs/2510.06503)
*I-Hsi Kao,Kanji Uchino*

Main category: cs.LG

TL;DR: ATLO-ML是一个自适应时间长度优化系统，能根据用户定义的输出时间长度自动确定最佳输入时间长度和采样率，显著提升时间序列预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 机器学习中准确的时间序列预测严重依赖于选择合适的输入时间长度和采样率，传统固定参数方法效果有限。

Method: 开发了ATLO-ML系统，动态调整输入时间长度和采样率参数，提供灵活的时间序列数据预处理方法。

Result: 在空气质量数据集上的验证表明，使用优化后的时间长度和采样率相比固定参数显著提高了机器学习模型的准确性。

Conclusion: ATLO-ML在各种时间敏感应用中具有泛化潜力，为机器学习工作流中的时间参数优化提供了稳健解决方案。

Abstract: Accurate time-series predictions in machine learning are heavily influenced
by the selection of appropriate input time length and sampling rate. This paper
introduces ATLO-ML, an adaptive time-length optimization system that
automatically determines the optimal input time length and sampling rate based
on user-defined output time length. The system provides a flexible approach to
time-series data pre-processing, dynamically adjusting these parameters to
enhance predictive performance. ATLO-ML is validated using air quality
datasets, including both GAMS-dataset and proprietary data collected from a
data center, both in time series format. Results demonstrate that utilizing the
optimized time length and sampling rate significantly improves the accuracy of
machine learning models compared to fixed time lengths. ATLO-ML shows potential
for generalization across various time-sensitive applications, offering a
robust solution for optimizing temporal input parameters in machine learning
workflows.

</details>


### [54] [A Median Perspective on Unlabeled Data for Out-of-Distribution Detection](https://arxiv.org/abs/2510.06505)
*Momin Abbas,Ali Falahati,Hossein Goli,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: 提出了Medix框架，利用中位数操作从无标签数据中识别潜在异常值，然后结合标记的分布内数据训练鲁棒的OOD分类器。


<details>
  <summary>Details</summary>
Motivation: 解决在现实应用中有效利用混合了分布内和分布外样本的无标签数据来增强OOD检测能力的挑战。

Method: 使用中位数操作从无标签数据中识别潜在异常值，利用中位数对噪声和异常值的鲁棒性，然后结合标记的分布内数据训练OOD分类器。

Result: 理论分析显示Medix实现了低错误率，实证结果表明在开放世界设置中全面优于现有方法。

Conclusion: Medix框架通过中位数操作有效识别异常值，结合理论保证和实证验证，为OOD检测提供了有效的解决方案。

Abstract: Out-of-distribution (OOD) detection plays a crucial role in ensuring the
robustness and reliability of machine learning systems deployed in real-world
applications. Recent approaches have explored the use of unlabeled data,
showing potential for enhancing OOD detection capabilities. However,
effectively utilizing unlabeled in-the-wild data remains challenging due to the
mixed nature of both in-distribution (InD) and OOD samples. The lack of a
distinct set of OOD samples complicates the task of training an optimal OOD
classifier. In this work, we introduce Medix, a novel framework designed to
identify potential outliers from unlabeled data using the median operation. We
use the median because it provides a stable estimate of the central tendency,
as an OOD detection mechanism, due to its robustness against noise and
outliers. Using these identified outliers, along with labeled InD data, we
train a robust OOD classifier. From a theoretical perspective, we derive error
bounds that demonstrate Medix achieves a low error rate. Empirical results
further substantiate our claims, as Medix outperforms existing methods across
the board in open-world settings, confirming the validity of our theoretical
insights.

</details>


### [55] [Text-to-Image Models Leave Identifiable Signatures: Implications for Leaderboard Security](https://arxiv.org/abs/2510.06525)
*Ali Naseh,Anshuman Suri,Yuefeng Peng,Harsh Chaudhari,Alina Oprea,Amir Houmansadr*

Main category: cs.LG

TL;DR: 文本到图像生成模型排行榜存在严重的安全漏洞，通过简单的CLIP嵌入空间分类就能高精度识别生成模型，使得排行榜排名操纵比之前认知的更容易。


<details>
  <summary>Details</summary>
Motivation: 生成式AI排行榜是评估模型能力的重要工具，但容易受到操纵攻击。攻击者需要先识别匿名模型的身份，这在文本到图像领域可能比大型语言模型更严重。

Method: 使用150,000多张生成图像、280个提示词和19个不同模型，在CLIP嵌入空间进行实时分类，无需提示控制或历史数据即可识别生成模型。

Result: 简单分类方法就能高精度识别生成模型，某些提示词甚至能实现近乎完美的去匿名化。

Conclusion: 文本到图像排行榜的排名操纵风险被低估，需要更强的防御措施来保护排行榜的公正性。

Abstract: Generative AI leaderboards are central to evaluating model capabilities, but
remain vulnerable to manipulation. Among key adversarial objectives is rank
manipulation, where an attacker must first deanonymize the models behind
displayed outputs -- a threat previously demonstrated and explored for large
language models (LLMs). We show that this problem can be even more severe for
text-to-image leaderboards, where deanonymization is markedly easier. Using
over 150,000 generated images from 280 prompts and 19 diverse models spanning
multiple organizations, architectures, and sizes, we demonstrate that simple
real-time classification in CLIP embedding space identifies the generating
model with high accuracy, even without prompt control or historical data. We
further introduce a prompt-level separability metric and identify prompts that
enable near-perfect deanonymization. Our results indicate that rank
manipulation in text-to-image leaderboards is easier than previously
recognized, underscoring the need for stronger defenses.

</details>


### [56] [Wide Neural Networks as a Baseline for the Computational No-Coincidence Conjecture](https://arxiv.org/abs/2510.06527)
*John Dunbar,Scott Aaronson*

Main category: cs.LG

TL;DR: 随机初始化的神经网络在激活函数满足高斯测度下零均值条件时，其输出近似独立，这支持了计算无巧合猜想的验证。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络输出的独立性条件，为AI可解释性提供理论基础，验证Alignment Research Center的计算无巧合猜想。

Method: 分析随机初始化神经网络在特定超参数设置下的输出独立性，重点关注激活函数在高斯测度下的零均值性质。

Result: 发现当激活函数满足E[σ(z)]=0时（如平移后的ReLU、GeLU和tanh），神经网络输出近似独立；而普通ReLU和GeLU不满足此条件。

Conclusion: 具有零均值激活函数的神经网络是验证计算无巧合猜想的有力候选者，为AI可解释性研究提供了重要理论支持。

Abstract: We establish that randomly initialized neural networks, with large width and
a natural choice of hyperparameters, have nearly independent outputs exactly
when their activation function is nonlinear with zero mean under the Gaussian
measure: $\mathbb{E}_{z \sim \mathcal{N}(0,1)}[\sigma(z)]=0$. For example, this
includes ReLU and GeLU with an additive shift, as well as tanh, but not ReLU or
GeLU by themselves. Because of their nearly independent outputs, we propose
neural networks with zero-mean activation functions as a promising candidate
for the Alignment Research Center's computational no-coincidence conjecture --
a conjecture that aims to measure the limits of AI interpretability.

</details>


### [57] [Scalable Policy-Based RL Algorithms for POMDPs](https://arxiv.org/abs/2510.06540)
*Ameya Anjarlekar,Rasoul Etesami,R Srikant*

Main category: cs.LG

TL;DR: 该论文提出将POMDP近似为有限状态MDP（称为超状态MDP）的方法，通过理论保证和基于策略的学习方法，使用TD学习和策略优化近似求解POMDP问题。


<details>
  <summary>Details</summary>
Motivation: POMDP中信念状态的连续性给学习最优策略带来了显著的计算挑战，需要找到有效的近似方法来处理这种复杂性。

Method: 将POMDP模型近似转换为有限状态MDP（超状态MDP），使用线性函数近似的基于策略的学习方法，结合TD学习和策略优化。

Result: 证明了超状态MDP的最优值函数与原始POMDP最优值函数之间的关系，并显示近似误差随历史长度呈指数级下降。

Conclusion: POMDP可以通过将其视为MDP并使用TD学习和策略优化来近似求解，这是首个明确量化在非马尔可夫环境中应用标准TD学习引入误差的有限时间边界工作。

Abstract: The continuous nature of belief states in POMDPs presents significant
computational challenges in learning the optimal policy. In this paper, we
consider an approach that solves a Partially Observable Reinforcement Learning
(PORL) problem by approximating the corresponding POMDP model into a
finite-state Markov Decision Process (MDP) (called Superstate MDP). We first
derive theoretical guarantees that improve upon prior work that relate the
optimal value function of the transformed Superstate MDP to the optimal value
function of the original POMDP. Next, we propose a policy-based learning
approach with linear function approximation to learn the optimal policy for the
Superstate MDP. Consequently, our approach shows that a POMDP can be
approximately solved using TD-learning followed by Policy Optimization by
treating it as an MDP, where the MDP state corresponds to a finite history. We
show that the approximation error decreases exponentially with the length of
this history. To the best of our knowledge, our finite-time bounds are the
first to explicitly quantify the error introduced when applying standard TD
learning to a setting where the true dynamics are not Markovian.

</details>


### [58] [Incoherence in goal-conditioned autoregressive models](https://arxiv.org/abs/2510.06545)
*Jacek Karwowski,Raymond Douglas*

Main category: cs.LG

TL;DR: 本文从数学角度分析了强化学习中由自回归模型简单目标条件化导致的策略不连贯问题，证明了在线RL微调离线学习策略可以减少不连贯性并提高回报，建立了训练-推断权衡的三方对应关系。


<details>
  <summary>Details</summary>
Motivation: 研究强化学习策略中由自回归模型简单目标条件化引起的结构性问题——不连贯性，旨在理解通过在线RL微调离线学习策略如何改善这一问题。

Method: 通过重新构建控制即推断和软Q学习的标准概念，建立迭代再训练过程的三方对应关系：将后验折叠到奖励中，在确定性情况下降低温度参数，并通过软条件生成模型分析不连贯性与有效视野的联系。

Result: 证明了在线RL微调离线学习策略可以减少不连贯性并提高回报，建立了训练-推断权衡的计算对应关系。

Conclusion: 不连贯性是强化学习策略的重要结构问题，通过在线微调和软条件生成模型可以有效改善，并与有效视野存在理论联系。

Abstract: We investigate mathematically the notion of incoherence: a structural issue
with reinforcement learning policies derived by naive goal-conditioning of
autoregressive models. We focus on the process of re-training models on their
own actions, that is, fine-tuning offline-learned policies with online RL. We
prove that it decreases incoherence and leads to an improvement in return, and
we aim to characterize the resulting trajectory of policies. By re-framing
standard notions of control-as-inference and soft Q learning, we establish a
three-way correspondence with two other ways of understanding the iterative
re-training process: as folding the posterior into the reward and, in the
deterministic case, as decreasing the temperature parameter; the correspondence
has computational content via the training-inference trade-off. Through
soft-conditioning generative models, we discuss the link between incoherence
and the effective horizon.

</details>


### [59] [The Markovian Thinker](https://arxiv.org/abs/2510.06557)
*Milad Aghajohari,Kamran Chitsaz,Amirhossein Kazemnejad,Sarath Chandar,Alessandro Sordoni,Aaron Courville,Siva Reddy*

Main category: cs.LG

TL;DR: 提出Markovian Thinking范式，通过将推理分解为固定大小的块，实现线性计算和恒定内存，解决传统RL推理方法中状态无限增长和二次计算开销的问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习推理方法中，状态包含所有先前推理标记，导致状态无界且注意力机制的计算复杂度随思考长度呈二次增长，限制了长推理能力。

Method: 提出Delethink环境，将推理结构化为固定大小的块，在每个块边界重置上下文并用简短传递信息重新初始化提示，通过RL训练策略学习在块结束时写入足够继续推理的文本状态。

Result: 1.5B模型在8K标记块中推理但思考长度可达24K标记，性能匹配或超过使用24K预算的LongCoT-RL；在96K平均思考长度下，Delethink成本仅为LongCoT-RL的1/4。

Conclusion: 重新设计思考环境是实现高效、可扩展推理LLMs的有力杠杆，能够在没有二次开销的情况下实现非常长的推理。

Abstract: Reinforcement learning (RL) has recently become a strong recipe for training
reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard
RL "thinking environment", where the state is the prompt plus all prior
reasoning tokens, makes the state unbounded and forces attention-based policies
to pay quadratic compute as thoughts lengthen. We revisit the environment
itself. We propose Markovian Thinking, a paradigm in which the policy advances
reasoning while conditioning on a constant-size state, decoupling thinking
length from context size. As an immediate consequence this yields linear
compute with constant memory. We instantiate this idea with Delethink, an RL
environment that structures reasoning into fixed-size chunks. Within each
chunk, the model thinks as usual; at the boundary, the environment resets the
context and reinitializes the prompt with a short carryover. Through RL, the
policy learns to write a textual state near the end of each chunk sufficient
for seamless continuation of reasoning after reset. Trained in this
environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up
to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget.
With test-time scaling, Delethink continues to improve where LongCoT plateaus.
The effect of linear compute is substantial: we empirically estimate at 96K
average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink.
Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B)
often sample Markovian traces zero-shot across diverse benchmarks, providing
positive samples that make RL effective at scale. Our results show that
redesigning the thinking environment is a powerful lever: it enables very long
reasoning without quadratic overhead and opens a path toward efficient,
scalable reasoning LLMs.

</details>


### [60] [The Framework That Survives Bad Models: Human-AI Collaboration For Clinical Trials](https://arxiv.org/abs/2510.06567)
*Yao Chen,David Ohlssen,Aimee Readie,Gregory Ligozio,Ruvie Martin,Thibaud Coroller*

Main category: cs.LG

TL;DR: 比较了两种AI框架与纯人工评估在医学影像疾病评估中的表现，发现AI作为辅助阅读器（AI-SR）是最适合临床试验的方法，即使在模型性能较差时也能保持可靠。


<details>
  <summary>Details</summary>
Motivation: AI在临床试验中具有巨大潜力，但无保障措施地部署AI存在重大风险，特别是评估直接影响试验结论的患者终点时。

Method: 比较两种AI框架与纯人工评估，测量成本、准确性、鲁棒性和泛化能力，通过注入从随机猜测到简单预测的劣质模型来压力测试框架。

Result: AI作为辅助阅读器（AI-SR）在所有标准上都表现最佳，能够提供可靠的疾病估计，保持临床试验治疗效果估计和结论的有效性，并在不同人群中保持这些优势。

Conclusion: AI-SR是临床试验中最合适的方法，即使在模型性能严重下降的情况下也能确保观察到的治疗效果保持有效。

Abstract: Artificial intelligence (AI) holds great promise for supporting clinical
trials, from patient recruitment and endpoint assessment to treatment response
prediction. However, deploying AI without safeguards poses significant risks,
particularly when evaluating patient endpoints that directly impact trial
conclusions. We compared two AI frameworks against human-only assessment for
medical image-based disease evaluation, measuring cost, accuracy, robustness,
and generalization ability. To stress-test these frameworks, we injected bad
models, ranging from random guesses to naive predictions, to ensure that
observed treatment effects remain valid even under severe model degradation. We
evaluated the frameworks using two randomized controlled trials with endpoints
derived from spinal X-ray images. Our findings indicate that using AI as a
supporting reader (AI-SR) is the most suitable approach for clinical trials, as
it meets all criteria across various model types, even with bad models. This
method consistently provides reliable disease estimation, preserves clinical
trial treatment effect estimates and conclusions, and retains these advantages
when applied to different populations.

</details>


### [61] [DPA-Net: A Dual-Path Attention Neural Network for Inferring Glycemic Control Metrics from Self-Monitored Blood Glucose Data](https://arxiv.org/abs/2510.06623)
*Canyu Lei,Benjamin Lobo,Jianxin Xie*

Main category: cs.LG

TL;DR: 提出DPA-Net双路径注意力神经网络，从稀疏的SMBG数据直接估计AGP血糖指标，解决CGM设备昂贵且不易获取的问题。


<details>
  <summary>Details</summary>
Motivation: CGM设备成本高、可及性差，特别是在中低收入地区难以普及；而SMBG虽然便宜且广泛可用，但数据稀疏不规则，难以转化为临床有意义的血糖指标。

Method: 使用双路径注意力神经网络：1）空间通道注意力路径重建CGM样轨迹；2）多尺度ResNet路径直接预测AGP指标；引入路径对齐机制减少偏差和过拟合；开发主动点选择器识别真实且有信息的SMBG采样点。

Result: 在大型真实世界数据集上的实验结果表明，DPA-Net实现了稳健的准确性，误差低，同时减少了系统性偏差。

Conclusion: 这是首个从SMBG数据估计AGP指标的监督机器学习框架，在无法获取CGM的情况下提供了实用且临床相关的决策支持工具。

Abstract: Continuous glucose monitoring (CGM) provides dense and dynamic glucose
profiles that enable reliable estimation of Ambulatory Glucose Profile (AGP)
metrics, such as Time in Range (TIR), Time Below Range (TBR), and Time Above
Range (TAR). However, the high cost and limited accessibility of CGM restrict
its widespread adoption, particularly in low- and middle-income regions. In
contrast, self-monitoring of blood glucose (SMBG) is inexpensive and widely
available but yields sparse and irregular data that are challenging to
translate into clinically meaningful glycemic metrics.
  In this work, we propose a Dual-Path Attention Neural Network (DPA-Net) to
estimate AGP metrics directly from SMBG data. DPA-Net integrates two
complementary paths: (1) a spatial-channel attention path that reconstructs a
CGM-like trajectory from sparse SMBG observations, and (2) a multi-scale ResNet
path that directly predicts AGP metrics. An alignment mechanism between the two
paths is introduced to reduce bias and mitigate overfitting. In addition, we
develop an active point selector to identify realistic and informative SMBG
sampling points that reflect patient behavioral patterns.
  Experimental results on a large, real-world dataset demonstrate that DPA-Net
achieves robust accuracy with low errors while reducing systematic bias. To the
best of our knowledge, this is the first supervised machine learning framework
for estimating AGP metrics from SMBG data, offering a practical and clinically
relevant decision-support tool in settings where CGM is not accessible.

</details>


### [62] [POME: Post Optimization Model Edit via Muon-style Projection](https://arxiv.org/abs/2510.06627)
*Yong Liu,Di Fu,Yang Luo,Zirui Zhu,Minhao Cheng,Cho-Jui Hsieh,Yang You*

Main category: cs.LG

TL;DR: POME是一种无需额外数据或优化的后处理算法，通过对微调权重与预训练权重之差进行截断SVD投影，提升微调后大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法往往在性能提升的同时引入噪声，需要一种简单、零成本的后处理方法来优化微调模型的性能。

Method: 使用截断奇异值分解对权重差异ΔW进行投影，均衡主导更新方向的影响并修剪代表噪声的小奇异值。

Result: 在GSM8K上平均性能提升+2.5%，在代码生成任务上提升+1.0%，适用于从7B到72B的各种规模模型。

Conclusion: POME是一种实用、零成本的通用增强方法，可无缝集成到任何微调流程中。

Abstract: We introduce Post-Optimization Model Edit (POME), a new algorithm that
enhances the performance of fine-tuned large language models using only their
pretrained and fine-tuned checkpoints, without requiring extra data or further
optimization. The core idea is to apply a muon-style projection to $\Delta W$,
the difference between the fine-tuned and pretrained weights. This projection
uses truncated singular value decomposition (SVD) to equalize the influence of
dominant update directions and prune small singular values, which often
represent noise. As a simple post-processing step, POME is completely decoupled
from the training pipeline. It requires zero modifications and imposes no
overhead, making it universally compatible with any optimizer or distributed
framework. POME delivers consistent gains, boosting average performance by
+2.5\% on GSM8K and +1.0\% on code generation. Its broad applicability -- from
7B foundation models to 72B RLHF-instructed models -- establishes it as a
practical, zero-cost enhancement for any fine-tuning pipeline. Code is
available at https://github.com/NUS-HPC-AI-Lab/POME.

</details>


### [63] [AI-Driven Forecasting and Monitoring of Urban Water System](https://arxiv.org/abs/2510.06631)
*Qiming Guo,Bishal Khatri,Hua Zhang,Wenlu Wang*

Main category: cs.LG

TL;DR: 提出了一种结合AI和远程传感器的地下水管泄漏检测框架，使用稀疏传感器部署和HydroNet模型，通过管道属性进行高精度建模，在真实校园污水网络数据集上表现优于先进基线方法。


<details>
  <summary>Details</summary>
Motivation: 地下水和污水管道存在泄漏和渗透等异常问题，导致严重的水资源损失、环境破坏和高昂维修成本。传统人工检测效率低下，而密集传感器部署成本过高。

Method: 部署稀疏远程传感器采集实时流量和深度数据，结合HydroNet模型，利用管道属性（材料、直径、坡度等）构建有向图进行高精度建模，集成边缘感知消息传递与水力模拟。

Result: 在真实校园污水网络数据集上的评估表明，系统能有效收集时空水力数据，HydroNet模型性能优于先进基线方法，能够从有限传感器部署实现准确的网络范围预测。

Conclusion: 该方法可有效扩展到广泛的地下水管网络，为城市基础设施管理提供高效解决方案。

Abstract: Underground water and wastewater pipelines are vital for city operations but
plagued by anomalies like leaks and infiltrations, causing substantial water
loss, environmental damage, and high repair costs. Conventional manual
inspections lack efficiency, while dense sensor deployments are prohibitively
expensive. In recent years, artificial intelligence has advanced rapidly and is
increasingly applied to urban infrastructure. In this research, we propose an
integrated AI and remote-sensor framework to address the challenge of leak
detection in underground water pipelines, through deploying a sparse set of
remote sensors to capture real-time flow and depth data, paired with HydroNet -
a dedicated model utilizing pipeline attributes (e.g., material, diameter,
slope) in a directed graph for higher-precision modeling. Evaluations on a
real-world campus wastewater network dataset demonstrate that our system
collects effective spatio-temporal hydraulic data, enabling HydroNet to
outperform advanced baselines. This integration of edge-aware message passing
with hydraulic simulations enables accurate network-wide predictions from
limited sensor deployments. We envision that this approach can be effectively
extended to a wide range of underground water pipeline networks.

</details>


### [64] [Chem-NMF: Multi-layer $α$-divergence Non-Negative Matrix Factorization for Cardiorespiratory Disease Clustering, with Improved Convergence Inspired by Chemical Catalysts and Rigorous Asymptotic Analysis](https://arxiv.org/abs/2510.06632)
*Yasaman Torabi,Shahram Shirani,James P. Reilly*

Main category: cs.LG

TL;DR: 提出了一种基于物理化学视角的Chem-NMF方法，通过引入边界因子来稳定多层NMF的收敛，在生物医学信号和面部图像上分别提升了5.6%±2.7%和11.1%±7.2%的聚类准确率。


<details>
  <summary>Details</summary>
Motivation: 虽然α-散度增强了NMF的优化灵活性，但将其扩展到多层架构时难以确保收敛性，需要新的收敛分析方法。

Method: 受化学反应中能量势垒的玻尔兹曼概率启发，提出Chem-NMF方法，引入边界因子来稳定收敛，首次从物理化学角度严格分析NMF算法的收敛行为。

Result: 实验结果表明，在生物医学信号上聚类准确率提升5.6%±2.7%，在面部图像上提升11.1%±7.2%。

Conclusion: 这是首个从物理化学视角严格分析NMF算法收敛行为的研究，提出的Chem-NMF方法有效提升了多层NMF的收敛稳定性和聚类性能。

Abstract: Non-Negative Matrix Factorization (NMF) is an unsupervised learning method
offering low-rank representations across various domains such as audio
processing, biomedical signal analysis, and image recognition. The
incorporation of $\alpha$-divergence in NMF formulations enhances flexibility
in optimization, yet extending these methods to multi-layer architectures
presents challenges in ensuring convergence. To address this, we introduce a
novel approach inspired by the Boltzmann probability of the energy barriers in
chemical reactions to theoretically perform convergence analysis. We introduce
a novel method, called Chem-NMF, with a bounding factor which stabilizes
convergence. To our knowledge, this is the first study to apply a physical
chemistry perspective to rigorously analyze the convergence behaviour of the
NMF algorithm. We start from mathematically proven asymptotic convergence
results and then show how they apply to real data. Experimental results
demonstrate that the proposed algorithm improves clustering accuracy by 5.6%
$\pm$ 2.7% on biomedical signals and 11.1% $\pm$ 7.2% on face images (mean
$\pm$ std).

</details>


### [65] [Three Forms of Stochastic Injection for Improved Distribution-to-Distribution Generative Modeling](https://arxiv.org/abs/2510.06634)
*Shiye Su,Yuhui Zhang,Linqi Zhou,Rajesh Ranganath,Serena Yeung-Levy*

Main category: cs.LG

TL;DR: 提出了一种改进的流匹配方法，通过向源样本和流插值注入随机性来解决分布到分布转换中的稀疏监督问题，在多个科学成像任务中显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 流匹配在噪声到数据转换中表现良好，但在分布到分布转换任务中，由于源分布也是从有限样本中学习的数据分布，标准流匹配方法因稀疏监督而失效。

Method: 提出简单且计算高效的方法，通过扰动源样本和流插值来向训练过程注入随机性，解决稀疏监督问题。

Result: 在五个涵盖生物学、放射学和天文学的多样化成像任务中，该方法显著提高了生成质量，平均比现有基线高出9个FID点，并降低了输入与生成样本之间的传输成本。

Conclusion: 该方法使流匹配成为模拟科学中出现的多样化分布转换的更实用工具，能更好地突出转换的真实效果。

Abstract: Modeling transformations between arbitrary data distributions is a
fundamental scientific challenge, arising in applications like drug discovery
and evolutionary simulation. While flow matching offers a natural framework for
this task, its use has thus far primarily focused on the noise-to-data setting,
while its application in the general distribution-to-distribution setting is
underexplored. We find that in the latter case, where the source is also a data
distribution to be learned from limited samples, standard flow matching fails
due to sparse supervision. To address this, we propose a simple and
computationally efficient method that injects stochasticity into the training
process by perturbing source samples and flow interpolants. On five diverse
imaging tasks spanning biology, radiology, and astronomy, our method
significantly improves generation quality, outperforming existing baselines by
an average of 9 FID points. Our approach also reduces the transport cost
between input and generated samples to better highlight the true effect of the
transformation, making flow matching a more practical tool for simulating the
diverse distribution transformations that arise in science.

</details>


### [66] [StruSR: Structure-Aware Symbolic Regression with Physics-Informed Taylor Guidance](https://arxiv.org/abs/2510.06635)
*Yunpeng Gong,Sihan Lan,Can Yang,Kunpeng Xu,Min Jiang*

Main category: cs.LG

TL;DR: 提出StruSR框架，利用训练好的PINN从时间序列数据中提取局部结构化物理先验，通过局部泰勒展开获取导数结构信息来指导符号表达式演化，提高符号回归的收敛速度和结构保真度。


<details>
  <summary>Details</summary>
Motivation: 传统符号回归方法缺乏从时间序列观测中提取结构化物理先验的机制，难以捕捉反映系统全局行为的符号表达式。

Method: 使用训练好的PINN进行局部泰勒展开获取导数结构信息；引入基于掩码的属性机制量化子树贡献；使用混合适应度函数联合最小化物理残差和泰勒系数失配；通过遗传编程进行表达式演化。

Result: 在基准PDE系统上的实验表明，StruSR相比传统基线方法提高了收敛速度、结构保真度和表达式可解释性。

Conclusion: StruSR为基于物理的符号发现提供了一个有原则的范式，能够更好地捕捉系统的全局行为。

Abstract: Symbolic regression aims to find interpretable analytical expressions by
searching over mathematical formula spaces to capture underlying system
behavior, particularly in scientific modeling governed by physical laws.
However, traditional methods lack mechanisms for extracting structured physical
priors from time series observations, making it difficult to capture symbolic
expressions that reflect the system's global behavior. In this work, we propose
a structure-aware symbolic regression framework, called StruSR, that leverages
trained Physics-Informed Neural Networks (PINNs) to extract locally structured
physical priors from time series data. By performing local Taylor expansions on
the outputs of the trained PINN, we obtain derivative-based structural
information to guide symbolic expression evolution. To assess the importance of
expression components, we introduce a masking-based attribution mechanism that
quantifies each subtree's contribution to structural alignment and physical
residual reduction. These sensitivity scores steer mutation and crossover
operations within genetic programming, preserving substructures with high
physical or structural significance while selectively modifying less
informative components. A hybrid fitness function jointly minimizes physics
residuals and Taylor coefficient mismatch, ensuring consistency with both the
governing equations and the local analytical behavior encoded by the PINN.
Experiments on benchmark PDE systems demonstrate that StruSR improves
convergence speed, structural fidelity, and expression interpretability
compared to conventional baselines, offering a principled paradigm for
physics-grounded symbolic discovery.

</details>


### [67] [Control-Augmented Autoregressive Diffusion for Data Assimilation](https://arxiv.org/abs/2510.06637)
*Prakhar Srivastava,Farrin Marouf Sofian,Francesco Immorlano,Kushagra Pandey,Stephan Mandt*

Main category: cs.LG

TL;DR: 提出了一个摊销框架，通过轻量级控制器网络增强预训练的自动回归扩散模型，在数据同化任务中实现单次前向推理，避免昂贵的伴随计算和优化。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在测试时缩放和微调方面取得进展，但自动回归扩散模型中的引导机制研究不足。特别是在混沌时空偏微分方程的数据同化中，现有方法计算成本高且容易在稀疏观测下产生预测漂移。

Method: 使用摊销框架，在预训练ARDMs基础上添加轻量级控制器网络，通过预览未来ARDMs展开来离线训练，学习在终端成本目标下预测即将观测的逐步控制。

Result: 在两个典型偏微分方程和六种观测机制下，该方法在稳定性、准确性和物理保真度方面持续优于四种最先进的基线方法。

Conclusion: 该方法将数据同化推理简化为单次前向展开，避免了推理过程中的昂贵伴随计算和优化，在多个指标上表现优异，代码和检查点将公开。

Abstract: Despite recent advances in test-time scaling and finetuning of diffusion
models, guidance in Auto-Regressive Diffusion Models (ARDMs) remains
underexplored. We introduce an amortized framework that augments pretrained
ARDMs with a lightweight controller network, trained offline by previewing
future ARDM rollouts and learning stepwise controls that anticipate upcoming
observations under a terminal cost objective. We evaluate this framework in the
context of data assimilation (DA) for chaotic spatiotemporal partial
differential equations (PDEs), a setting where existing methods are often
computationally prohibitive and prone to forecast drift under sparse
observations. Our approach reduces DA inference to a single forward rollout
with on-the-fly corrections, avoiding expensive adjoint computations and/or
optimizations during inference. We demonstrate that our method consistently
outperforms four state-of-the-art baselines in stability, accuracy, and
physical fidelity across two canonical PDEs and six observation regimes. We
will release code and checkpoints publicly.

</details>


### [68] [The False Promise of Zero-Shot Super-Resolution in Machine-Learned Operators](https://arxiv.org/abs/2510.06646)
*Mansi Sakarvadia,Kareem Hegazy,Amin Totounferoush,Kyle Chard,Yaoqing Yang,Ian Foster,Michael W. Mahoney*

Main category: cs.LG

TL;DR: 评估机器学习算子(MLOs)的零样本超分辨率能力，发现其无法在未经训练的分辨率下准确推理，并提出多分辨率训练方案解决此问题。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习中需要建模连续现象，MLOs被设计为可在任意分辨率进行推理，但需要验证其是否具备零样本超分辨率能力。

Method: 将多分辨率推理分解为频率信息外推和分辨率插值两个关键行为，通过实证评估MLOs的零样本能力，并提出多分辨率训练协议。

Result: 实证表明MLOs无法零样本执行多分辨率推理，对训练分辨率外的推理表现脆弱且易受混叠影响。

Conclusion: MLOs本身不具备零样本多分辨率推理能力，但通过提出的多分辨率训练方案可以克服混叠问题并实现稳健的多分辨率泛化。

Abstract: A core challenge in scientific machine learning, and scientific computing
more generally, is modeling continuous phenomena which (in practice) are
represented discretely. Machine-learned operators (MLOs) have been introduced
as a means to achieve this modeling goal, as this class of architecture can
perform inference at arbitrary resolution. In this work, we evaluate whether
this architectural innovation is sufficient to perform "zero-shot
super-resolution," namely to enable a model to serve inference on
higher-resolution data than that on which it was originally trained. We
comprehensively evaluate both zero-shot sub-resolution and super-resolution
(i.e., multi-resolution) inference in MLOs. We decouple multi-resolution
inference into two key behaviors: 1) extrapolation to varying frequency
information; and 2) interpolating across varying resolutions. We empirically
demonstrate that MLOs fail to do both of these tasks in a zero-shot manner.
Consequently, we find MLOs are not able to perform accurate inference at
resolutions different from those on which they were trained, and instead they
are brittle and susceptible to aliasing. To address these failure modes, we
propose a simple, computationally-efficient, and data-driven multi-resolution
training protocol that overcomes aliasing and that provides robust
multi-resolution generalization.

</details>


### [69] [Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions](https://arxiv.org/abs/2510.06649)
*Frank Wu,Mengye Ren*

Main category: cs.LG

TL;DR: 提出了ARQ方法，将前向-前向算法的goodness函数与动作条件结合，用于局部强化学习，在多个基准测试中优于最先进的无反向传播RL方法。


<details>
  <summary>Details</summary>
Motivation: 前向-前向算法目前主要局限于监督学习，而强化学习领域自然产生学习信号，因此需要将其扩展到RL领域。

Method: 使用层活动统计的goodness函数和动作条件，结合时间差分学习进行局部值函数估计。

Result: 在MinAtar和DeepMind Control Suite基准测试中，ARQ方法优于最先进的无反向传播RL方法，并在大多数任务中超过使用反向传播的算法。

Conclusion: ARQ方法简单且具有生物学基础，在强化学习中取得了优越性能，展示了前向-前向算法在RL领域的潜力。

Abstract: The Forward-Forward (FF) Algorithm is a recently proposed learning procedure
for neural networks that employs two forward passes instead of the traditional
forward and backward passes used in backpropagation. However, FF remains
largely confined to supervised settings, leaving a gap at domains where
learning signals can be yielded more naturally such as RL. In this work,
inspired by FF's goodness function using layer activity statistics, we
introduce Action-conditioned Root mean squared Q-Functions (ARQ), a novel value
estimation method that applies a goodness function and action conditioning for
local RL using temporal difference learning. Despite its simplicity and
biological grounding, our approach achieves superior performance compared to
state-of-the-art local backprop-free RL methods in the MinAtar and the DeepMind
Control Suite benchmarks, while also outperforming algorithms trained with
backpropagation on most tasks. Code can be found at
https://github.com/agentic-learning-ai-lab/arq.

</details>


### [70] [Rethinking Nonlinearity: Trainable Gaussian Mixture Modules for Modern Neural Architectures](https://arxiv.org/abs/2510.06660)
*Weiguo Lu,Gangnan Yuan,Hong-kun Zhang,Shangyang Li*

Main category: cs.LG

TL;DR: 提出高斯混合非线性模块(GMNM)，基于高斯混合模型和高斯核的距离特性，增强神经网络非线性能力，可集成到多种架构中端到端训练。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络受限于激活函数引入的非线性能力，需要更强大的非线性模块来提升模型性能。

Method: 放松概率约束，采用灵活的高斯投影参数化，将高斯混合模型思想转化为可微分模块，可集成到MLP、CNN、注意力机制和LSTM等架构中。

Result: 在多种神经网络架构中集成GMNM均能持续提升性能，优于标准基线模型。

Conclusion: GMNM作为强大灵活的非线性模块，有潜力在广泛机器学习应用中提升效率和准确性。

Abstract: Neural networks in general, from MLPs and CNNs to attention-based
Transformers, are constructed from layers of linear combinations followed by
nonlinear operations such as ReLU, Sigmoid, or Softmax. Despite their strength,
these conventional designs are often limited in introducing non-linearity by
the choice of activation functions. In this work, we introduce Gaussian
Mixture-Inspired Nonlinear Modules (GMNM), a new class of differentiable
modules that draw on the universal density approximation Gaussian mixture
models (GMMs) and distance properties (metric space) of Gaussian kernal. By
relaxing probabilistic constraints and adopting a flexible parameterization of
Gaussian projections, GMNM can be seamlessly integrated into diverse neural
architectures and trained end-to-end with gradient-based methods. Our
experiments demonstrate that incorporating GMNM into architectures such as
MLPs, CNNs, attention mechanisms, and LSTMs consistently improves performance
over standard baselines. These results highlight GMNM's potential as a powerful
and flexible module for enhancing efficiency and accuracy across a wide range
of machine learning applications.

</details>


### [71] [The Effect of Attention Head Count on Transformer Approximation](https://arxiv.org/abs/2510.06662)
*Penghao Yu,Haotian Jiang,Zeyu Bao,Ruoxi Yu,Qianxiao Li*

Main category: cs.LG

TL;DR: 本文研究了Transformer架构中注意力头数量对表达能力的理论影响，建立了参数复杂度的上下界，证明了多头注意力对高效近似的重要性。


<details>
  <summary>Details</summary>
Motivation: 虽然Transformer已成为序列建模的主流架构，但其结构参数如何影响表达能力仍缺乏深入理解，特别是注意力头数量的作用。

Method: 引入广义D-检索任务作为理论基础，建立参数复杂度的上下界分析，并通过合成数据和真实任务实验验证理论结果。

Result: 证明了多头Transformer可实现高效近似，而头数不足时参数复杂度至少为O(1/ε^{cT})；单头情况下需要O(T)维嵌入才能完全记忆输入。

Conclusion: 注意力头数量是Transformer表达能力的关键因素，多头设计对高效近似至关重要，理论结果在实际任务中具有相关性。

Abstract: Transformer has become the dominant architecture for sequence modeling, yet a
detailed understanding of how its structural parameters influence expressive
power remains limited. In this work, we study the approximation properties of
transformers, with particular emphasis on the role of the number of attention
heads. Our analysis begins with the introduction of a generalized $D$-retrieval
task, which we prove to be dense in the space of continuous functions, thereby
providing the basis for our theoretical framework. We then establish both upper
and lower bounds on the parameter complexity required for
$\epsilon$-approximation. Specifically, we show that transformers with
sufficiently many heads admit efficient approximation, whereas with too few
heads, the number of parameters must scale at least as $O(1/\epsilon^{cT})$,
for some constant $c$ and sequence length $T$. To the best of our knowledge,
this constitutes the first rigorous lower bound of this type in a nonlinear and
practically relevant setting. We further examine the single-head case and
demonstrate that an embedding dimension of order $O(T)$ allows complete
memorization of the input, where approximation is entirely achieved by the
feed-forward block. Finally, we validate our theoretical findings with
experiments on both synthetic data and real-world tasks, illustrating the
practical relevance of our results.

</details>


### [72] [XRPO: Pushing the limits of GRPO with Targeted Exploration and Exploitation](https://arxiv.org/abs/2510.06672)
*Udbhav Bamba,Minghao Fang,Yifan Yu,Haizhong Zheng,Fan Lai*

Main category: cs.LG

TL;DR: XRPO是一个基于探索-利用原则的强化学习框架，通过自适应rollout分配、上下文种子注入和优势函数锐化机制，提升大语言模型在推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO等强化学习方法在复杂提示上探索不足，且未充分利用信息反馈信号，主要原因是跨提示的上下文无关rollout分配和过度依赖稀疏奖励。

Method: 1. 引入数学基础的rollout分配器，自适应优先处理不确定性降低潜力高的提示；2. 通过上下文种子策略注入精选示例，引导模型进入更困难的推理轨迹；3. 开发基于组相对、新颖性感知的优势函数锐化机制，利用序列似然放大低概率但正确的响应。

Result: 在数学和编程基准测试中，XRPO相比现有方法（如GRPO和GSPO）在pass@1上提升达4%，在cons@32上提升达6%，同时训练收敛速度加快达2.7倍。

Conclusion: XRPO通过统一的探索-利用框架有效解决了现有强化学习方法在LLM推理中的局限性，显著提升了模型性能和训练效率。

Abstract: Reinforcement learning algorithms such as GRPO have driven recent advances in
large language model (LLM) reasoning. While scaling the number of rollouts
stabilizes training, existing approaches suffer from limited exploration on
challenging prompts and leave informative feedback signals underexploited, due
to context-independent rollout allocation across prompts (e.g., generating 16
rollouts per prompt) and relying heavily on sparse rewards. This paper presents
XRPO(eXplore - eXploit GRPO), a unified framework that recasts policy
optimization through the principled lens of rollout exploration-exploitation.
To enhance exploration, XRPO introduces a mathematically grounded rollout
allocator that adaptively prioritizes prompts with higher potential for
uncertainty reduction. It further addresses stagnation on zero-reward prompts
through an in-context seeding strategy that injects curated exemplars, steering
the model into more difficult reasoning trajectories. To strengthen
exploitation, XRPO develops a group-relative, novelty-aware advantage
sharpening mechanism that leverages sequence likelihoods to amplify
low-probability yet correct responses, thereby extending the policy's reach
beyond sparse rewards. Experiments across diverse math and coding benchmarks on
both reasoning and non-reasoning models demonstrate that XRPO outperforms
existing advances (e.g., GRPO and GSPO) up to 4% pass@1 and 6% cons@32, while
accelerating training convergence by up to 2.7X.

</details>


### [73] [TimeFormer: Transformer with Attention Modulation Empowered by Temporal Characteristics for Time Series Forecasting](https://arxiv.org/abs/2510.06680)
*Zhipeng Liu,Peibo Duan,Xuan Tang,Baixin Li,Yongsheng Huang,Mingyang Geng,Changsheng Zhang,Bin Zhang,Binwu Wang*

Main category: cs.LG

TL;DR: 提出TimeFormer，一种专为时间序列设计的Transformer架构，通过引入考虑时间序列特性的自注意力机制和多重尺度分析，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer在时间序列预测中表现不佳，主要因为未能充分考虑文本和时间序列模态的差异，特别是时间序列的单向性和影响力衰减特性。

Method: 提出TimeFormer，核心创新是包含两个调制项的自注意力机制（MoSA），基于Hawkes过程和因果掩码捕捉时间先验，并结合多尺度和子序列分析框架。

Result: 在多个真实数据集上的实验表明，TimeFormer显著优于现有最优方法，MSE最多降低7.45%，在94.04%的评估指标上创下新基准。

Conclusion: TimeFormer成功地将时间序列特性融入Transformer架构，MoSA机制可广泛用于增强其他基于Transformer的模型性能。

Abstract: Although Transformers excel in natural language processing, their extension
to time series forecasting remains challenging due to insufficient
consideration of the differences between textual and temporal modalities. In
this paper, we develop a novel Transformer architecture designed for time
series data, aiming to maximize its representational capacity. We identify two
key but often overlooked characteristics of time series: (1) unidirectional
influence from the past to the future, and (2) the phenomenon of decaying
influence over time. These characteristics are introduced to enhance the
attention mechanism of Transformers. We propose TimeFormer, whose core
innovation is a self-attention mechanism with two modulation terms (MoSA),
designed to capture these temporal priors of time series under the constraints
of the Hawkes process and causal masking. Additionally, TimeFormer introduces a
framework based on multi-scale and subsequence analysis to capture semantic
dependencies at different temporal scales, enriching the temporal dependencies.
Extensive experiments conducted on multiple real-world datasets show that
TimeFormer significantly outperforms state-of-the-art methods, achieving up to
a 7.45% reduction in MSE compared to the best baseline and setting new
benchmarks on 94.04\% of evaluation metrics. Moreover, we demonstrate that the
MoSA mechanism can be broadly applied to enhance the performance of other
Transformer-based models.

</details>


### [74] [Distributed Algorithms for Multi-Agent Multi-Armed Bandits with Collision](https://arxiv.org/abs/2510.06683)
*Daoyuan Zhou,Xuchuang Wang,Lin Yang,Yang Gao*

Main category: cs.LG

TL;DR: 提出了一种分布式多玩家多臂老虎机算法，通过自适应通信协议实现近最优的群体和个人遗憾，通信成本仅为O(log log T)，并在异步设置下扩展了该方法。


<details>
  <summary>Details</summary>
Motivation: 解决多玩家多臂老虎机问题中的分布式协作挑战，在无中心协调的情况下，玩家只能观察到自身动作和碰撞反馈，需要设计高效的通信机制来最小化碰撞并最大化累积奖励。

Method: 设计分布式算法，采用自适应高效的通信协议，玩家通过有限通信协调臂选择来避免碰撞，并在异步设置下扩展该方法。

Result: 算法实现了近最优的群体和个人遗憾，通信成本仅为O(log log T)，实验显示相比现有方法显著降低了个人遗憾，在异步设置下实现了对数遗憾。

Conclusion: 提出的分布式算法通过自适应通信协议有效解决了MMAB问题，在通信效率和性能方面都优于现有方法，并成功扩展到异步设置。

Abstract: We study the stochastic Multiplayer Multi-Armed Bandit (MMAB) problem, where
multiple players select arms to maximize their cumulative rewards. Collisions
occur when two or more players select the same arm, resulting in no reward, and
are observed by the players involved. We consider a distributed setting without
central coordination, where each player can only observe their own actions and
collision feedback. We propose a distributed algorithm with an adaptive,
efficient communication protocol. The algorithm achieves near-optimal group and
individual regret, with a communication cost of only $\mathcal{O}(\log\log T)$.
Our experiments demonstrate significant performance improvements over existing
baselines. Compared to state-of-the-art (SOTA) methods, our approach achieves a
notable reduction in individual regret. Finally, we extend our approach to a
periodic asynchronous setting, proving the lower bound for this problem and
presenting an algorithm that achieves logarithmic regret.

</details>


### [75] [AutoBalance: An Automatic Balancing Framework for Training Physics-Informed Neural Networks](https://arxiv.org/abs/2510.06684)
*Kang An,Chenhao Si,Ming Yan,Shiqian Ma*

Main category: cs.LG

TL;DR: 提出AutoBalance训练范式，通过为每个损失分量分配独立的自适应优化器，解决PINNs训练中多损失项平衡困难的问题。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs训练方法在处理具有冲突目标和不同曲率的多重损失项时存在困难，现有方法在优化前操纵梯度的策略存在根本限制。

Method: 采用"后组合"训练范式，为每个损失分量分配独立的自适应优化器，然后聚合预处理后的更新结果。

Result: 在具有挑战性的PDE基准测试中，AutoBalance始终优于现有框架，显著降低了MSE和L∞范数度量的解误差。

Conclusion: AutoBalance是一种正交且互补的PINN方法，能够增强其他流行PINN方法在要求苛刻基准测试中的有效性。

Abstract: Physics-Informed Neural Networks (PINNs) provide a powerful and general
framework for solving Partial Differential Equations (PDEs) by embedding
physical laws into loss functions. However, training PINNs is notoriously
difficult due to the need to balance multiple loss terms, such as PDE residuals
and boundary conditions, which often have conflicting objectives and vastly
different curvatures. Existing methods address this issue by manipulating
gradients before optimization (a "pre-combine" strategy). We argue that this
approach is fundamentally limited, as forcing a single optimizer to process
gradients from spectrally heterogeneous loss landscapes disrupts its internal
preconditioning. In this work, we introduce AutoBalance, a novel "post-combine"
training paradigm. AutoBalance assigns an independent adaptive optimizer to
each loss component and aggregates the resulting preconditioned updates
afterwards. Extensive experiments on challenging PDE benchmarks show that
AutoBalance consistently outperforms existing frameworks, achieving significant
reductions in solution error, as measured by both the MSE and $L^{\infty}$
norms. Moreover, AutoBalance is orthogonal to and complementary with other
popular PINN methodologies, amplifying their effectiveness on demanding
benchmarks.

</details>


### [76] [Is the Hard-Label Cryptanalytic Model Extraction Really Polynomial?](https://arxiv.org/abs/2510.06692)
*Akira Ito,Takayuki Miura,Yosuke Todo*

Main category: cs.LG

TL;DR: 本文指出现有深度神经网络模型提取攻击在深层网络中需要指数级查询次数，并提出跨层提取方法显著降低查询复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有硬标签设置下的模型提取攻击假设在深层网络中变得不现实，需要指数级查询次数，限制了实际可行性。

Method: 提出跨层提取攻击方法，利用跨层神经元交互来提取深层网络参数，避免直接提取特定神经元参数的高昂成本。

Result: 新方法显著降低了查询复杂度，缓解了现有模型提取方法的局限性。

Conclusion: 跨层提取技术为深层神经网络的安全分析提供了更实用的攻击方法，突破了现有方法的深度限制。

Abstract: Deep Neural Networks (DNNs) have attracted significant attention, and their
internal models are now considered valuable intellectual assets. Extracting
these internal models through access to a DNN is conceptually similar to
extracting a secret key via oracle access to a block cipher. Consequently,
cryptanalytic techniques, particularly differential-like attacks, have been
actively explored recently. ReLU-based DNNs are the most commonly and widely
deployed architectures. While early works (e.g., Crypto 2020, Eurocrypt 2024)
assume access to exact output logits, which are usually invisible, more recent
works (e.g., Asiacrypt 2024, Eurocrypt 2025) focus on the hard-label setting,
where only the final classification result (e.g., "dog" or "car") is available
to the attacker. Notably, Carlini et al. (Eurocrypt 2025) demonstrated that
model extraction is feasible in polynomial time even under this restricted
setting.
  In this paper, we first show that the assumptions underlying their attack
become increasingly unrealistic as the attack-target depth grows. In practice,
satisfying these assumptions requires an exponential number of queries with
respect to the attack depth, implying that the attack does not always run in
polynomial time. To address this critical limitation, we propose a novel attack
method called CrossLayer Extraction. Instead of directly extracting the secret
parameters (e.g., weights and biases) of a specific neuron, which incurs
exponential cost, we exploit neuron interactions across layers to extract this
information from deeper layers. This technique significantly reduces query
complexity and mitigates the limitations of existing model extraction
approaches.

</details>


### [77] [A Diffusion Model for Regular Time Series Generation from Irregular Data with Completion and Masking](https://arxiv.org/abs/2510.06699)
*Gal Fadlon,Idan Arbiv,Nimrod Berman,Omri Azencot*

Main category: cs.LG

TL;DR: 提出了一种新的两阶段框架，用于生成具有不规则采样和缺失值的真实时间序列数据，通过结合时间序列补全和视觉扩散模型，在性能和计算成本上取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理不规则采样和缺失值时效果不佳且计算成本高，而将时间序列转换为图像表示的扩散模型虽然表现出色，但简单掩码处理会破坏自然邻域结构。

Method: 两阶段框架：1）使用时间序列变换器补全不规则序列，创建自然邻域；2）使用带掩码的视觉扩散模型，减少对补全值的依赖。

Result: 实现了最先进的性能，判别分数相对提升70%，计算成本降低85%。

Conclusion: 该方法有效结合了补全和掩码的优势，能够稳健高效地生成真实时间序列数据。

Abstract: Generating realistic time series data is critical for applications in
healthcare, finance, and science. However, irregular sampling and missing
values present significant challenges. While prior methods address these
irregularities, they often yield suboptimal results and incur high
computational costs. Recent advances in regular time series generation, such as
the diffusion-based ImagenTime model, demonstrate strong, fast, and scalable
generative capabilities by transforming time series into image representations,
making them a promising solution. However, extending ImagenTime to irregular
sequences using simple masking introduces "unnatural" neighborhoods, where
missing values replaced by zeros disrupt the learning process. To overcome
this, we propose a novel two-step framework: first, a Time Series Transformer
completes irregular sequences, creating natural neighborhoods; second, a
vision-based diffusion model with masking minimizes dependence on the completed
values. This approach leverages the strengths of both completion and masking,
enabling robust and efficient generation of realistic time series. Our method
achieves state-of-the-art performance, achieving a relative improvement in
discriminative score by $70\%$ and in computational cost by $85\%$. Code is at
https://github.com/azencot-group/ImagenI2R.

</details>


### [78] [Dual Goal Representations](https://arxiv.org/abs/2510.06714)
*Seohong Park,Deepinder Mann,Sergey Levine*

Main category: cs.LG

TL;DR: 提出了用于目标条件强化学习的双重目标表示方法，通过状态间的时序距离关系来编码状态，具有环境动态不变性和噪声过滤能力，能显著提升离线目标达成性能。


<details>
  <summary>Details</summary>
Motivation: 传统目标条件强化学习中的状态表示可能包含外生噪声且依赖于原始表示，需要一种更鲁棒且仅依赖于环境内在动态的目标表示方法。

Method: 开发双重目标表示，通过状态间的时序距离关系来表征状态，并设计了可结合现有GCRL算法的实用表示学习方法。

Result: 在OGBench任务套件的20个状态和像素任务中，双重目标表示持续提升了离线目标达成性能。

Conclusion: 双重目标表示提供了一种理论上优越且实践有效的目标表示方法，能显著改善目标条件强化学习的性能。

Abstract: In this work, we introduce dual goal representations for goal-conditioned
reinforcement learning (GCRL). A dual goal representation characterizes a state
by "the set of temporal distances from all other states"; in other words, it
encodes a state through its relations to every other state, measured by
temporal distance. This representation provides several appealing theoretical
properties. First, it depends only on the intrinsic dynamics of the environment
and is invariant to the original state representation. Second, it contains
provably sufficient information to recover an optimal goal-reaching policy,
while being able to filter out exogenous noise. Based on this concept, we
develop a practical goal representation learning method that can be combined
with any existing GCRL algorithm. Through diverse experiments on the OGBench
task suite, we empirically show that dual goal representations consistently
improve offline goal-reaching performance across 20 state- and pixel-based
tasks.

</details>


### [79] [Incorporating Expert Knowledge into Bayesian Causal Discovery of Mixtures of Directed Acyclic Graphs](https://arxiv.org/abs/2510.06735)
*Zachris Björkman,Jorge Loría,Sophie Wharrie,Samuel Kaski*

Main category: cs.LG

TL;DR: 提出了一种用于异构领域的因果发现方法，结合专家先验知识和变分混合结构学习，通过贝叶斯实验设计原则来推断因果贝叶斯网络混合物。


<details>
  <summary>Details</summary>
Motivation: 在异构领域中，现有的先验知识获取方法假设单一因果图，不适用于异构设置，需要开发能够处理异构性的因果发现方法。

Method: 基于贝叶斯实验设计原则的因果获取策略，以及扩展DiBS方法的变分混合结构学习方法VaMSL，通过迭代推断因果贝叶斯网络混合物。

Result: 该方法成功生成了一组替代因果模型，在异构合成数据上实现了改进的结构学习性能，并在乳腺癌数据库中展示了捕捉复杂分布的能力。

Conclusion: 所提出的方法能够有效处理异构领域的因果发现问题，结合专家反馈信息，在合成数据和真实医疗数据上都表现出良好性能。

Abstract: Bayesian causal discovery benefits from prior information elicited from
domain experts, and in heterogeneous domains any prior knowledge would be badly
needed. However, so far prior elicitation approaches have assumed a single
causal graph and hence are not suited to heterogeneous domains. We propose a
causal elicitation strategy for heterogeneous settings, based on Bayesian
experimental design (BED) principles, and a variational mixture structure
learning (VaMSL) method -- extending the earlier differentiable Bayesian
structure learning (DiBS) method -- to iteratively infer mixtures of causal
Bayesian networks (CBNs). We construct an informative graph prior incorporating
elicited expert feedback in the inference of mixtures of CBNs. Our proposed
method successfully produces a set of alternative causal models (mixture
components or clusters), and achieves an improved structure learning
performance on heterogeneous synthetic data when informed by a simulated
expert. Finally, we demonstrate that our approach is capable of capturing
complex distributions in a breast cancer database.

</details>


### [80] [Function regression using the forward forward training and inferring paradigm](https://arxiv.org/abs/2510.06762)
*Shivam Padmani,Akshay Joshi*

Main category: cs.LG

TL;DR: 本文提出了一种使用前向-前向算法进行函数回归的新方法，并评估了该方法在单变量和多变量函数上的表现，还初步研究了将其扩展到Kolmogorov Arnold网络和深度物理神经网络。


<details>
  <summary>Details</summary>
Motivation: 函数回归是机器学习的基本应用，前向-前向学习算法是一种无需反向传播的训练方法，但目前仅限于分类任务。本文旨在将前向-前向算法扩展到函数回归领域。

Method: 开发了一种基于前向-前向算法的函数回归方法，该方法避免了传统的反向传播，适合在神经形态计算和物理模拟神经网络中实现。

Result: 该方法在单变量和多变量函数回归任务上进行了评估，并初步探索了在Kolmogorov Arnold网络和深度物理神经网络中的应用。

Conclusion: 前向-前向算法可以成功应用于函数回归任务，为神经形态计算和物理神经网络提供了新的训练范式。

Abstract: Function regression/approximation is a fundamental application of machine
learning. Neural networks (NNs) can be easily trained for function regression
using a sufficient number of neurons and epochs. The forward-forward learning
algorithm is a novel approach for training neural networks without
backpropagation, and is well suited for implementation in neuromorphic
computing and physical analogs for neural networks. To the best of the authors'
knowledge, the Forward Forward paradigm of training and inferencing NNs is
currently only restricted to classification tasks. This paper introduces a new
methodology for approximating functions (function regression) using the
Forward-Forward algorithm. Furthermore, the paper evaluates the developed
methodology on univariate and multivariate functions, and provides preliminary
studies of extending the proposed Forward-Forward regression to Kolmogorov
Arnold Networks, and Deep Physical Neural Networks.

</details>


### [81] [Modeling COVID-19 Dynamics in German States Using Physics-Informed Neural Networks](https://arxiv.org/abs/2510.06776)
*Phillip Rothenbeck,Sai Karthikeya Vemuri,Niklas Penzel,Joachim Denzler*

Main category: cs.LG

TL;DR: 使用物理信息神经网络（PINNs）求解SIR模型逆问题，对德国各联邦州进行细粒度时空分析，估计传播和恢复参数以及时变再生数，揭示区域间传播行为的显著差异。


<details>
  <summary>Details</summary>
Motivation: COVID-19大流行凸显了定量建模分析的重要性，但传统隔间模型（如SIR）难以直接处理噪声观测数据，需要开发能够有效结合观测数据的建模方法。

Method: 采用物理信息神经网络（PINNs）求解SIR模型的逆问题，使用罗伯特·科赫研究所（RKI）的感染数据，对德国所有联邦州进行为期三年的细粒度时空分析。

Result: 估计了各州特定的传播和恢复参数以及时变再生数（R_t），结果显示各地区传播行为存在显著差异，与疫苗接种率和主要大流行阶段的时间模式相关。

Conclusion: 研究证明了PINNs在局部化、长期流行病学建模中的实用性，能够有效跟踪大流行进展并揭示区域特异性动态。

Abstract: The COVID-19 pandemic has highlighted the need for quantitative modeling and
analysis to understand real-world disease dynamics. In particular, post hoc
analyses using compartmental models offer valuable insights into the
effectiveness of public health interventions, such as vaccination strategies
and containment policies. However, such compartmental models like SIR
(Susceptible-Infectious-Recovered) often face limitations in directly
incorporating noisy observational data. In this work, we employ
Physics-Informed Neural Networks (PINNs) to solve the inverse problem of the
SIR model using infection data from the Robert Koch Institute (RKI). Our main
contribution is a fine-grained, spatio-temporal analysis of COVID-19 dynamics
across all German federal states over a three-year period. We estimate
state-specific transmission and recovery parameters and time-varying
reproduction number (R_t) to track the pandemic progression. The results
highlight strong variations in transmission behavior across regions, revealing
correlations with vaccination uptake and temporal patterns associated with
major pandemic phases. Our findings demonstrate the utility of PINNs in
localized, long-term epidemiological modeling.

</details>


### [82] [Get RICH or Die Scaling: Profitably Trading Inference Compute for Robustness](https://arxiv.org/abs/2510.06790)
*Tavish McDonald,Bo Lei,Stanislav Fort,Bhavya Kailkhura,Brian Bartoldson*

Main category: cs.LG

TL;DR: 该论文提出了推理计算鲁棒性假设(RICH)，认为当模型训练数据更好地反映被攻击数据的组件时，推理计算防御能带来鲁棒性收益。通过组合泛化，模型能够理解分布外数据的分布内组件，从而遵循防御规范。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明LLM推理能提高对防御规范的遵循，从而增强对越狱攻击的鲁棒性，但这种测试计算的好处会在攻击者获得梯度或多模态输入时减弱。本文旨在填补这一空白，证明推理计算即使在这些情况下也能提供好处。

Method: 提出RICH假设，通过组合泛化使模型能够理解分布外数据的分布内组件。在视觉语言模型和多种攻击类型上进行实证验证，通过提示增强防御规范的强调来测试推理计算的效果。

Result: 研究发现，如果通过组合泛化解锁了对分布外数据的规范遵循，测试时计算能带来鲁棒性收益。例如，通过提示增强防御规范能降低对经过对抗预训练的VLM的梯度多模态攻击成功率，但对未鲁棒化模型无此效果。

Conclusion: 推理计算的鲁棒性收益与基础模型鲁棒性相关，形成富者愈富的RICH动态。建议将训练时和测试时防御层叠使用以获得协同效益。

Abstract: Models are susceptible to adversarially out-of-distribution (OOD) data
despite large training-compute investments into their robustification. Zaremba
et al. (2025) make progress on this problem at test time, showing LLM reasoning
improves satisfaction of model specifications designed to thwart attacks,
resulting in a correlation between reasoning effort and robustness to
jailbreaks. However, this benefit of test compute fades when attackers are
given access to gradients or multimodal inputs. We address this gap, clarifying
that inference-compute offers benefits even in such cases. Our approach argues
that compositional generalization, through which OOD data is understandable via
its in-distribution (ID) components, enables adherence to defensive
specifications on adversarially OOD inputs. Namely, we posit the Robustness
from Inference Compute Hypothesis (RICH): inference-compute defenses profit as
the model's training data better reflects the attacked data's components. We
empirically support this hypothesis across vision language model and attack
types, finding robustness gains from test-time compute if specification
following on OOD data is unlocked by compositional generalization, while RL
finetuning and protracted reasoning are not critical. For example, increasing
emphasis on defensive specifications via prompting lowers the success rate of
gradient-based multimodal attacks on VLMs robustified by adversarial
pretraining, but this same intervention provides no such benefit to
not-robustified models. This correlation of inference-compute's robustness
benefit with base model robustness is the rich-get-richer dynamic of the RICH:
attacked data components are more ID for robustified models, aiding
compositional generalization to OOD data. Accordingly, we advise layering
train-time and test-time defenses to obtain their synergistic benefit.

</details>


### [83] [The Unreasonable Effectiveness of Randomized Representations in Online Continual Graph Learning](https://arxiv.org/abs/2510.06819)
*Giovanni Donghi,Daniele Zambon,Luca Pasa,Cesare Alippi,Nicolò Navarin*

Main category: cs.LG

TL;DR: 提出一种简单有效的在线持续图学习方法：使用固定的随机初始化编码器生成节点嵌入，仅在线训练轻量级分类器，无需复杂回放或正则化机制


<details>
  <summary>Details</summary>
Motivation: 解决在线持续图学习中的灾难性遗忘问题，传统方法面临节点逐个到达、分布漂移随时发生、无法离线训练任务特定子图的挑战

Method: 冻结随机初始化的图编码器，仅在线训练轻量级分类器，通过固定表示参数来消除遗忘的关键来源

Result: 在多个OCGL基准测试中，该方法相比最先进方法获得一致提升，最高提升30%，性能接近联合离线训练的上界

Conclusion: 在在线持续图学习中，通过架构简单性和稳定性可以最小化灾难性遗忘，无需复杂的回放或正则化机制

Abstract: Catastrophic forgetting is one of the main obstacles for Online Continual
Graph Learning (OCGL), where nodes arrive one by one, distribution drifts may
occur at any time and offline training on task-specific subgraphs is not
feasible. In this work, we explore a surprisingly simple yet highly effective
approach for OCGL: we use a fixed, randomly initialized encoder to generate
robust and expressive node embeddings by aggregating neighborhood information,
training online only a lightweight classifier. By freezing the encoder, we
eliminate drifts of the representation parameters, a key source of forgetting,
obtaining embeddings that are both expressive and stable. When evaluated across
several OCGL benchmarks, despite its simplicity and lack of memory buffer, this
approach yields consistent gains over state-of-the-art methods, with surprising
improvements of up to 30% and performance often approaching that of the joint
offline-training upper bound. These results suggest that in OCGL, catastrophic
forgetting can be minimized without complex replay or regularization by
embracing architectural simplicity and stability.

</details>


### [84] [Efficient numeracy in language models through single-token number embeddings](https://arxiv.org/abs/2510.06824)
*Linus Kreitner,Paul Hager,Jonathan Mengedoht,Georgios Kaissis,Daniel Rueckert,Martin J. Menten*

Main category: cs.LG

TL;DR: 提出BitTokens方法，将数字编码为单个token，解决LLM处理数值计算时token效率低下的问题，使小型语言模型也能近乎完美地执行基本算术运算。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型处理数值数据需要大量推理token，且数字被分割成多个token，限制了模型处理复杂数值问题的能力。

Method: 提出BitTokens方法，使用IEEE 754二进制浮点数表示将任何数字嵌入到单个token中。

Result: 实验表明BitTokens使小型语言模型能够近乎完美地学习基本算术运算算法。

Conclusion: BitTokens提高了语言模型处理数值计算的效率，扩展了模型可解决问题的长度和复杂度。

Abstract: To drive progress in science and engineering, large language models (LLMs)
must be able to process large amounts of numerical data and solve long
calculations efficiently. This is currently only possible through the use of
external tools or extensive reasoning chains, either limiting the numerical
intuition of LLMs or limiting the length of problems they can solve. We show
that frontier LLMs require excessive amounts of reasoning tokens to solve even
basic calculations, which is exacerbated by their tokenization strategies that
split single numbers into multiple tokens. This motivates the need for
efficient and effective single-token number encodings. We introduce a set of
desiderata for such encodings and show that existing approaches fail to fulfill
them. To address these shortcomings, we propose BitTokens, a novel tokenization
strategy that embeds any number into a single token using its IEEE 754 binary
floating-point representation. Through extensive experiments we show that our
BitTokens allow even small language models to learn algorithms that solve basic
arithmetic operations nearly perfectly. This newly gained efficiency could
expand the length and complexity of problems language models can solve.

</details>


### [85] [Recurrence-Complete Frame-based Action Models](https://arxiv.org/abs/2510.06828)
*Michael Keiblinger*

Main category: cs.LG

TL;DR: 该论文挑战了"注意力机制就是全部"的观点，提出在长序列任务中需要结合循环机制，并设计了一种循环完备架构来解决非循环完备模型在长时间任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 挑战当前大语言模型中仅依赖注意力机制的趋势，指出完全并行化的前向或反向传播架构无法表示某些对长时间智能体任务特别重要的问题类别。

Method: 引入一种循环完备架构，并在GitHub行为序列上进行训练，通过固定参数数量来研究训练序列长度与损失的关系。

Result: 损失随训练序列长度呈幂律分布，参数数量保持固定；长序列训练的时间成本线性增加但总能得到补偿，在时间函数上产生更低的损失。

Conclusion: 循环机制对于长时间智能体任务至关重要，循环完备架构能够有效解决非循环完备模型在长时间输入聚合上的失败问题。

Abstract: In recent years, attention-like mechanisms have been used to great success in
the space of large language models, unlocking scaling potential to a previously
unthinkable extent. "Attention Is All You Need" famously claims RNN cells are
not needed in conjunction with attention. We challenge this view. In this
paper, we point to existing proofs that architectures with fully parallelizable
forward or backward passes cannot represent classes of problems specifically
interesting for long-running agentic tasks. We further conjecture a critical
time t beyond which non-recurrence-complete models fail to aggregate inputs
correctly, with concrete implications for agentic systems (e.g., software
engineering agents). To address this, we introduce a recurrence-complete
architecture and train it on GitHub-derived action sequences. Loss follows a
power law in the trained sequence length while the parameter count remains
fixed. Moreover, longer-sequence training always amortizes its linearly
increasing wall-time cost, yielding lower loss as a function of wall time.

</details>


### [86] [Early wind turbine alarm prediction based on machine learning: AlarmForecasting](https://arxiv.org/abs/2510.06831)
*Syed Shazaib Shah,Daoliang Tan*

Main category: cs.LG

TL;DR: 提出了一种基于LSTM的警报预测和分类框架，能够提前10-30分钟预测风力涡轮机警报，从而预防故障发生。


<details>
  <summary>Details</summary>
Motivation: 传统研究仅将警报数据用作诊断工具，本研究旨在实现警报预防，在警报触发前进行干预，避免故障发生。

Method: 采用两阶段框架：基于LSTM的回归模块进行时间序列警报预测，然后分类模块对预测警报进行标签分类。

Result: 在14台Senvion MM82风力涡轮机5年运行数据上测试，10、20、30分钟警报预测准确率分别为82%、52%、41%。

Conclusion: 该方法能够可靠预测整个警报分类体系，显著降低警报频率，通过主动干预提高运行效率。

Abstract: Alarm data is pivotal in curbing fault behavior in Wind Turbines (WTs) and
forms the backbone for advancedpredictive monitoring systems. Traditionally,
research cohorts have been confined to utilizing alarm data solelyas a
diagnostic tool, merely indicative of unhealthy status. However, this study
aims to offer a transformativeleap towards preempting alarms, preventing alarms
from triggering altogether, and consequently avertingimpending failures. Our
proposed Alarm Forecasting and Classification (AFC) framework is designed on
twosuccessive modules: first, the regression module based on long short-term
memory (LSTM) for time-series alarmforecasting, and thereafter, the
classification module to implement alarm tagging on the forecasted alarm.
Thisway, the entire alarm taxonomy can be forecasted reliably rather than a few
specific alarms. 14 Senvion MM82turbines with an operational period of 5 years
are used as a case study; the results demonstrated 82%, 52%,and 41% accurate
forecasts for 10, 20, and 30 min alarm forecasts, respectively. The results
substantiateanticipating and averting alarms, which is significant in curbing
alarm frequency and enhancing operationalefficiency through proactive
intervention.

</details>


### [87] [CNN-TFT explained by SHAP with multi-head attention weights for time series forecasting](https://arxiv.org/abs/2510.06840)
*Stefano F. Stefenon,João P. Matos-Carvalho,Valderi R. Q. Leithardt,Kin-Choong Yow*

Main category: cs.LG

TL;DR: 提出CNN-TFT-SHAP-MHAW混合架构，结合CNN的局部特征提取和Transformer的长程依赖建模能力，用于多元时间序列预测，在水电自然流量数据集上取得2.2%的平均绝对百分比误差。


<details>
  <summary>Details</summary>
Motivation: CNN擅长捕捉局部模式和平移不变性，而Transformer通过自注意力有效建模长程依赖，结合两者优势以提升多元时间序列预测性能。

Method: 使用一维卷积层层次结构从原始输入序列中提取局部特征，然后输入到TFT中应用多头注意力捕捉短长期依赖并自适应加权相关协变量。

Result: 在自然流量时间序列数据集上，CNN-TFT优于现有深度学习模型，平均绝对百分比误差达2.2%。通过SHAP-MHAW方法获得模型可解释性。

Conclusion: CNN-TFT-SHAP-MHAW架构在需要高保真多元时间序列预测的应用中具有前景，已在GitHub上开源。

Abstract: Convolutional neural networks (CNNs) and transformer architectures offer
strengths for modeling temporal data: CNNs excel at capturing local patterns
and translational invariances, while transformers effectively model long-range
dependencies via self-attention. This paper proposes a hybrid architecture
integrating convolutional feature extraction with a temporal fusion transformer
(TFT) backbone to enhance multivariate time series forecasting. The CNN module
first applies a hierarchy of one-dimensional convolutional layers to distill
salient local patterns from raw input sequences, reducing noise and
dimensionality. The resulting feature maps are then fed into the TFT, which
applies multi-head attention to capture both short- and long-term dependencies
and to weigh relevant covariates adaptively. We evaluate the CNN-TFT on a
hydroelectric natural flow time series dataset. Experimental results
demonstrate that CNN-TFT outperforms well-established deep learning models,
with a mean absolute percentage error of up to 2.2%. The explainability of the
model is obtained by a proposed Shapley additive explanations with multi-head
attention weights (SHAP-MHAW). Our novel architecture, named CNN-TFT-SHAP-MHAW,
is promising for applications requiring high-fidelity, multivariate time series
forecasts, being available for future analysis at
https://github.com/SFStefenon/CNN-TFT-SHAP-MHAW .

</details>


### [88] [Enhancing Bankruptcy Prediction of Banks through Advanced Machine Learning Techniques: An Innovative Approach and Analysis](https://arxiv.org/abs/2510.06852)
*Zuherman Rustam,Sri Hartini,Sardar M. N. Islam,Fevi Novkaniza,Fiftitah R. Aszhari,Muhammad Rifqi*

Main category: cs.LG

TL;DR: 本研究使用机器学习方法（逻辑回归、随机森林、支持向量机）预测银行破产概率，在土耳其商业银行和印尼农村银行数据上验证了方法的有效性，其中随机森林模型达到90%准确率。


<details>
  <summary>Details</summary>
Motivation: 传统统计方法如Altman's Z-Score依赖刚性假设，预测精度低。需要更准确有效的银行破产预测方法来维护金融系统稳定。

Method: 使用逻辑回归、随机森林和支持向量机三种机器学习方法，分别基于土耳其44家活跃银行和21家破产银行的年度财务数据（1994-2004），以及印尼43家活跃和43家破产农村银行的季度财务数据（2013-2019）。

Result: 随机森林模型在商业银行数据上达到90%的预测准确率，三种机器学习方法都能准确预测农村银行的破产可能性。

Conclusion: 提出的创新机器学习方法有助于实施降低破产成本的政策，为银行风险管理提供更有效的工具。

Abstract: Context: Financial system stability is determined by the condition of the
banking system. A bank failure can destroy the stability of the financial
system, as banks are subject to systemic risk, affecting not only individual
banks but also segments or the entire financial system. Calculating the
probability of a bank going bankrupt is one way to ensure the banking system is
safe and sound. Existing literature and limitations: Statistical models, such
as Altman's Z-Score, are one of the common techniques for developing a
bankruptcy prediction model. However, statistical methods rely on rigid and
sometimes irrelevant assumptions, which can result in low forecast accuracy.
New approaches are necessary. Objective of the research: Bankruptcy models are
developed using machine learning techniques, such as logistic regression (LR),
random forest (RF), and support vector machines (SVM). According to several
studies, machine learning is also more accurate and effective than statistical
methods for categorising and forecasting banking risk management. Present
Research: The commercial bank data are derived from the annual financial
statements of 44 active banks and 21 bankrupt banks in Turkey from 1994 to
2004, and the rural bank data are derived from the quarterly financial reports
of 43 active and 43 bankrupt rural banks in Indonesia between 2013 and 2019.
Five rural banks in Indonesia have also been selected to demonstrate the
feasibility of analysing bank bankruptcy trends. Findings and implications: The
results of the research experiments show that RF can forecast data from
commercial banks with a 90% accuracy rate. Furthermore, the three machine
learning methods proposed accurately predict the likelihood of rural bank
bankruptcy. Contribution and Conclusion: The proposed innovative machine
learning approach help to implement policies that reduce the costs of
bankruptcy.

</details>


### [89] [Towards Generalization of Graph Neural Networks for AC Optimal Power Flow](https://arxiv.org/abs/2510.06860)
*Olayiwola Arowolo,Jochen L. Cremer*

Main category: cs.LG

TL;DR: 提出混合异构消息传递神经网络(HH-MPNN)解决AC最优潮流计算问题，实现跨电网规模和拓扑变化的可扩展性与适应性，相比传统求解器获得1000-10000倍计算加速。


<details>
  <summary>Details</summary>
Motivation: 传统AC最优潮流求解器计算成本高，机器学习方法难以适应电网规模和拓扑变化且需要昂贵的重新训练。

Method: 将电网组件建模为不同节点和边类型，结合可扩展的Transformer模型处理长程依赖关系，仅需在默认拓扑上训练即可适应未见拓扑。

Result: 在14-2000节点电网上，默认拓扑下最优性差距小于1%，未见拓扑下小于3%；小电网预训练提升大电网性能；计算加速1000-10000倍。

Conclusion: HH-MPNN推进了实用、可泛化的机器学习在实时电力系统运行中的应用。

Abstract: AC Optimal Power Flow (ACOPF) is computationally expensive for large-scale
power systems, with conventional solvers requiring prohibitive solution times.
Machine learning approaches offer computational speedups but struggle with
scalability and topology adaptability without expensive retraining. To enable
scalability across grid sizes and adaptability to topology changes, we propose
a Hybrid Heterogeneous Message Passing Neural Network (HH-MPNN). HH-MPNN models
buses, generators, loads, shunts, transmission lines and transformers as
distinct node or edge types, combined with a scalable transformer model for
handling long-range dependencies. On grids from 14 to 2,000 buses, HH-MPNN
achieves less than 1% optimality gap on default topologies. Applied zero-shot
to thousands of unseen topologies, HH-MPNN achieves less than 3% optimality gap
despite training only on default topologies. Pre-training on smaller grids also
improves results on a larger grid. Computational speedups reach 1,000x to
10,000x compared to interior point solvers. These results advance practical,
generalizable machine learning for real-time power system operations.

</details>


### [90] [SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models](https://arxiv.org/abs/2510.06871)
*Huahui Yi,Kun Wang,Qiankun Li,Miao Yu,Liang Lin,Gongli Xi,Hao Wu,Xuming Hu,Kang Li,Yang Liu*

Main category: cs.LG

TL;DR: 提出SaFeR-VLM框架，通过安全对齐的强化学习将安全性直接嵌入多模态推理过程，解决了多模态大推理模型在对抗性或不安全提示下放大安全风险的问题。


<details>
  <summary>Details</summary>
Motivation: 现有防御主要在输出层面，不约束推理过程，导致模型面临隐性风险。多模态大推理模型在推理时往往会放大安全风险，这种现象被称为"推理税"。

Method: 包含四个组件：(I) QI-Safe-10K数据集，强调安全关键和推理敏感案例；(II) 安全感知的rollout，不安全生成会经过反思和修正；(III) 结构化奖励建模，具有多维加权标准和明确惩罚；(IV) GRPO优化，强化安全和修正轨迹。

Result: SaFeR-VLM-3B在六个基准测试中安全性和有用性平均得分分别为70.13和78.97，超越相同规模和>10倍更大模型。SaFeR-VLM-7B在安全指标上超越GPT-5-mini和Gemini-2.5-Flash，且不降低有用性性能。

Conclusion: 该框架将安全性从被动保护转变为推理的主动驱动因素，支持超越表面过滤的动态和可解释安全决策，实现了可扩展和可泛化的安全感知推理。

Abstract: Multimodal Large Reasoning Models (MLRMs) demonstrate impressive cross-modal
reasoning but often amplify safety risks under adversarial or unsafe prompts, a
phenomenon we call the \textit{Reasoning Tax}. Existing defenses mainly act at
the output level and do not constrain the reasoning process, leaving models
exposed to implicit risks. In this paper, we propose SaFeR-VLM, a
safety-aligned reinforcement learning framework that embeds safety directly
into multimodal reasoning. The framework integrates four components: (I)
QI-Safe-10K, a curated dataset emphasizing safety-critical and
reasoning-sensitive cases; (II) safety-aware rollout, where unsafe generations
undergo reflection and correction instead of being discarded; (III) structured
reward modeling with multi-dimensional weighted criteria and explicit penalties
for hallucinations and contradictions; and (IV) GRPO optimization, which
reinforces both safe and corrected trajectories. This unified design shifts
safety from a passive safeguard to an active driver of reasoning, enabling
scalable and generalizable safety-aware reasoning. SaFeR-VLM further
demonstrates robustness against both explicit and implicit risks, supporting
dynamic and interpretable safety decisions beyond surface-level filtering.
SaFeR-VLM-3B achieves average performance $70.13$ and $78.97$ on safety and
helpfulness across six benchmarks, surpassing both same-scale and $>10\times$
larger models such as Skywork-R1V3-38B, Qwen2.5VL-72B, and GLM4.5V-106B.
Remarkably, SaFeR-VLM-7B benefits from its increased scale to surpass
GPT-5-mini and Gemini-2.5-Flash by \num{6.47} and \num{16.76} points
respectively on safety metrics, achieving this improvement without any
degradation in helpfulness performance. Our codes are available at
https://github.com/HarveyYi/SaFeR-VLM.

</details>


### [91] [MoRE-GNN: Multi-omics Data Integration with a Heterogeneous Graph Autoencoder](https://arxiv.org/abs/2510.06880)
*Zhiyu Wang,Sonia Koszut,Pietro Liò,Francesco Ceccarelli*

Main category: cs.LG

TL;DR: 提出了MoRE-GNN（多组学关系边图神经网络），一种异构图自编码器，用于整合高维多组学单细胞数据，通过动态构建关系图来捕捉跨模态相关性。


<details>
  <summary>Details</summary>
Motivation: 多组学单细胞数据整合面临高维度和复杂跨模态关系的挑战，需要开发能够自适应构建数据关系的新方法。

Method: 使用异构图自编码器，结合图卷积和注意力机制，直接从数据中动态构建关系图。

Result: 在六个公开数据集上的评估显示，MoRE-GNN能捕捉生物学意义的关系，在强跨模态相关性设置下优于现有方法，学习到的表示支持准确的下游跨模态预测。

Conclusion: MoRE-GNN提供了一个自适应、可扩展且可解释的多组学整合框架，尽管性能可能随数据集复杂度变化。

Abstract: The integration of multi-omics single-cell data remains challenging due to
high-dimensionality and complex inter-modality relationships. To address this,
we introduce MoRE-GNN (Multi-omics Relational Edge Graph Neural Network), a
heterogeneous graph autoencoder that combines graph convolution and attention
mechanisms to dynamically construct relational graphs directly from data.
Evaluations on six publicly available datasets demonstrate that MoRE-GNN
captures biologically meaningful relationships and outperforms existing
methods, particularly in settings with strong inter-modality correlations.
Furthermore, the learned representations allow for accurate downstream
cross-modal predictions. While performance may vary with dataset complexity,
MoRE-GNN offers an adaptive, scalable and interpretable framework for advancing
multi-omics integration.

</details>


### [92] [Angular Constraint Embedding via SpherePair Loss for Constrained Clustering](https://arxiv.org/abs/2510.06907)
*Shaojie Zhang,Ke Chen*

Main category: cs.LG

TL;DR: 提出SpherePair方法，通过角度约束嵌入解决深度约束聚类问题，避免现有方法的局限性，实现更好的可扩展性和实际应用效果。


<details>
  <summary>Details</summary>
Motivation: 现有深度约束聚类方法要么受限于端到端建模中的锚点问题，要么难以学习判别性欧几里得嵌入，限制了其可扩展性和实际应用。

Method: 使用SpherePair损失函数和几何公式，在角度空间中编码成对约束，实现表示学习与聚类的有效分离。

Result: 在多样化基准测试中与最先进的深度约束聚类方法进行比较，证实了其优越性能、可扩展性和实际有效性。

Conclusion: SpherePair方法能够无冲突地保持成对关系，无需指定确切聚类数量，泛化到未见数据，快速推断聚类数量，并具有严格的理论保证。

Abstract: Constrained clustering integrates domain knowledge through pairwise
constraints. However, existing deep constrained clustering (DCC) methods are
either limited by anchors inherent in end-to-end modeling or struggle with
learning discriminative Euclidean embedding, restricting their scalability and
real-world applicability. To avoid their respective pitfalls, we propose a
novel angular constraint embedding approach for DCC, termed SpherePair. Using
the SpherePair loss with a geometric formulation, our method faithfully encodes
pairwise constraints and leads to embeddings that are clustering-friendly in
angular space, effectively separating representation learning from clustering.
SpherePair preserves pairwise relations without conflict, removes the need to
specify the exact number of clusters, generalizes to unseen data, enables rapid
inference of the number of clusters, and is supported by rigorous theoretical
guarantees. Comparative evaluations with state-of-the-art DCC methods on
diverse benchmarks, along with empirical validation of theoretical insights,
confirm its superior performance, scalability, and overall real-world
effectiveness. Code is available at
\href{https://github.com/spherepaircc/SpherePairCC/tree/main}{our repository}.

</details>


### [93] [Vacuum Spiker: A Spiking Neural Network-Based Model for Efficient Anomaly Detection in Time Series](https://arxiv.org/abs/2510.06910)
*Iago Xabier Vázquez,Javier Sedano,Muhammad Afzal,Ángel Miguel García-Vico*

Main category: cs.LG

TL;DR: 提出Vacuum Spiker算法，一种基于脉冲神经网络的时序异常检测方法，通过新的检测标准和编码方案显著降低能耗，在保持竞争力的同时实现高效异常检测。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在资源受限环境（如IoT设备、边缘计算平台）中能耗过高的问题，为时序异常检测提供更可持续的解决方案。

Method: 采用脉冲神经网络，引入基于全局神经活动变化的检测标准，使用脉冲时间依赖可塑性进行训练，并提出高效编码方案将输入空间离散化为非重叠区间。

Result: 在公开数据集上实现竞争性性能，同时显著降低能耗；在真实案例中成功识别太阳能逆变器的功率削减事件。

Conclusion: Vacuum Spiker算法展示了在保持检测性能的同时实现高能效异常检测的潜力，适用于资源受限环境。

Abstract: Anomaly detection is a key task across domains such as industry, healthcare,
and cybersecurity. Many real-world anomaly detection problems involve analyzing
multiple features over time, making time series analysis a natural approach for
such problems. While deep learning models have achieved strong performance in
this field, their trend to exhibit high energy consumption limits their
deployment in resource-constrained environments such as IoT devices, edge
computing platforms, and wearables. To address this challenge, this paper
introduces the \textit{Vacuum Spiker algorithm}, a novel Spiking Neural
Network-based method for anomaly detection in time series. It incorporates a
new detection criterion that relies on global changes in neural activity rather
than reconstruction or prediction error. It is trained using Spike
Time-Dependent Plasticity in a novel way, intended to induce changes in neural
activity when anomalies occur. A new efficient encoding scheme is also
proposed, which discretizes the input space into non-overlapping intervals,
assigning each to a single neuron. This strategy encodes information with a
single spike per time step, improving energy efficiency compared to
conventional encoding methods. Experimental results on publicly available
datasets show that the proposed algorithm achieves competitive performance
while significantly reducing energy consumption, compared to a wide set of deep
learning and machine learning baselines. Furthermore, its practical utility is
validated in a real-world case study, where the model successfully identifies
power curtailment events in a solar inverter. These results highlight its
potential for sustainable and efficient anomaly detection.

</details>


### [94] [Utilizing Large Language Models for Machine Learning Explainability](https://arxiv.org/abs/2510.06912)
*Alexandros Vassiliades,Nikolaos Polatidis,Stamatios Samaras,Sotiris Diplaris,Ignacio Cabrera Martin,Yannis Manolopoulos,Stefanos Vrochidis,Ioannis Kompatsiaris*

Main category: cs.LG

TL;DR: 本研究探索大语言模型在自主生成机器学习解决方案时的可解释性能力，通过两个分类任务评估GPT、Claude和DeepSeek生成的模型性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在自动生成机器学习解决方案时的可解释性能力，评估其作为自动化可解释ML流水线生成工具的潜力。

Method: 使用三种先进LLM为两个分类任务生成四种分类器的训练流水线，通过SHAP评估模型的可解释性（保真度和稀疏性）。

Result: LLM能够生成有效且可解释的模型，实现高保真度和一致的稀疏性，与人工设计的基线模型表现相当。

Conclusion: 大语言模型具有作为自动化可解释机器学习流水线生成工具的潜力，能够产生高性能且可解释的模型。

Abstract: This study explores the explainability capabilities of large language models
(LLMs), when employed to autonomously generate machine learning (ML) solutions.
We examine two classification tasks: (i) a binary classification problem
focused on predicting driver alertness states, and (ii) a multilabel
classification problem based on the yeast dataset. Three state-of-the-art LLMs
(i.e. OpenAI GPT, Anthropic Claude, and DeepSeek) are prompted to design
training pipelines for four common classifiers: Random Forest, XGBoost,
Multilayer Perceptron, and Long Short-Term Memory networks. The generated
models are evaluated in terms of predictive performance (recall, precision, and
F1-score) and explainability using SHAP (SHapley Additive exPlanations).
Specifically, we measure Average SHAP Fidelity (Mean Squared Error between SHAP
approximations and model outputs) and Average SHAP Sparsity (number of features
deemed influential). The results reveal that LLMs are capable of producing
effective and interpretable models, achieving high fidelity and consistent
sparsity, highlighting their potential as automated tools for interpretable ML
pipeline generation. The results show that LLMs can produce effective,
interpretable pipelines with high fidelity and consistent sparsity, closely
matching manually engineered baselines.

</details>


### [95] [DecompGAIL: Learning Realistic Traffic Behaviors with Decomposed Multi-Agent Generative Adversarial Imitation Learning](https://arxiv.org/abs/2510.06913)
*Ke Guo,Haochen Liu,Xiaojun Wu,Chen Lv*

Main category: cs.LG

TL;DR: 提出DecompGAIL方法解决多智能体交通仿真中的不稳定问题，通过分解真实度为ego-map和ego-neighbor组件，过滤误导性交互，并在SMART骨干网络上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法无法建模真实交通行为：行为克隆存在协变量偏移问题，而GAIL在多智能体设置中极不稳定，主要原因是无关交互误导。

Method: DecompGAIL方法：1) 将真实度分解为ego-map和ego-neighbor组件；2) 过滤neighbor:neighbor和neighbor:map误导交互；3) 引入社会PPO目标，用距离加权的邻域奖励增强自我奖励。

Result: 在WOMD Sim Agents 2025基准测试中实现了最先进的性能。

Conclusion: DecompGAIL通过分解真实度组件和过滤误导交互，有效解决了多智能体GAIL的不稳定性问题，为自动驾驶和城市交通规划提供了更真实的仿真工具。

Abstract: Realistic traffic simulation is critical for the development of autonomous
driving systems and urban mobility planning, yet existing imitation learning
approaches often fail to model realistic traffic behaviors. Behavior cloning
suffers from covariate shift, while Generative Adversarial Imitation Learning
(GAIL) is notoriously unstable in multi-agent settings. We identify a key
source of this instability: irrelevant interaction misguidance, where a
discriminator penalizes an ego vehicle's realistic behavior due to unrealistic
interactions among its neighbors. To address this, we propose Decomposed
Multi-agent GAIL (DecompGAIL), which explicitly decomposes realism into ego-map
and ego-neighbor components, filtering out misleading neighbor: neighbor and
neighbor: map interactions. We further introduce a social PPO objective that
augments ego rewards with distance-weighted neighborhood rewards, encouraging
overall realism across agents. Integrated into a lightweight SMART-based
backbone, DecompGAIL achieves state-of-the-art performance on the WOMD Sim
Agents 2025 benchmark.

</details>


### [96] [Revisiting Node Affinity Prediction in Temporal Graphs](https://arxiv.org/abs/2510.06940)
*Krishna Sri Ipsit Mantri,Or Feldman,Moshe Eliasof,Chaim Baskin*

Main category: cs.LG

TL;DR: 提出了NAViS模型，通过利用启发式方法与状态空间模型的等价性来解决节点亲和性预测问题，在TGB基准上超越了现有最佳方法包括启发式方法


<details>
  <summary>Details</summary>
Motivation: 当前最先进的动态链接属性预测模型在节点亲和性预测任务上表现不佳，甚至被简单的启发式方法（如持久预测或移动平均）超越，需要分析训练挑战并提出解决方案

Method: 开发NAViS模型，利用启发式方法与状态空间模型的等价性，使用虚拟状态进行节点亲和性预测，并引入新的损失函数来解决训练困难

Result: 在TGB基准测试中，NAViS模型超越了现有最先进方法，包括启发式方法

Conclusion: NAViS模型通过结合状态空间模型和新的损失函数，成功解决了节点亲和性预测中的训练挑战，取得了优于现有方法的表现

Abstract: Node affinity prediction is a common task that is widely used in temporal
graph learning with applications in social and financial networks, recommender
systems, and more. Recent works have addressed this task by adapting
state-of-the-art dynamic link property prediction models to node affinity
prediction. However, simple heuristics, such as Persistent Forecast or Moving
Average, outperform these models. In this work, we analyze the challenges in
training current Temporal Graph Neural Networks for node affinity prediction
and suggest appropriate solutions. Combining the solutions, we develop NAViS -
Node Affinity prediction model using Virtual State, by exploiting the
equivalence between heuristics and state space models. While promising,
training NAViS is non-trivial. Therefore, we further introduce a novel loss
function for node affinity prediction. We evaluate NAViS on TGB and show that
it outperforms the state-of-the-art, including heuristics. Our source code is
available at https://github.com/orfeld415/NAVIS

</details>


### [97] [Fisher Information, Training and Bias in Fourier Regression Models](https://arxiv.org/abs/2510.06945)
*Lorenzo Pastori,Veronika Eyring,Mierk Schwabe*

Main category: cs.LG

TL;DR: 本文研究了量子神经网络中基于Fisher信息矩阵的评估指标如何有效预测训练和预测性能，揭示了模型有效维度与任务偏差之间的相互作用。


<details>
  <summary>Details</summary>
Motivation: 随着量子机器学习特别是量子神经网络兴趣的增长，需要有效的评估指标来预测其训练和预测性能。

Method: 利用QNN与傅里叶模型的等价性，推导傅里叶模型的FIM解析表达式，构建具有可调有效维度和偏差的模型进行比较。

Result: 发现对于无偏模型，更高的有效维度可能带来更好的可训练性和性能；而对于有偏模型，较低的有效维度在训练中可能更有益。

Conclusion: 这些发现提供了几何特性、模型-任务对齐和训练之间相互作用的具体示例，对更广泛的机器学习社区具有相关性。

Abstract: Motivated by the growing interest in quantum machine learning, in particular
quantum neural networks (QNNs), we study how recently introduced evaluation
metrics based on the Fisher information matrix (FIM) are effective for
predicting their training and prediction performance. We exploit the
equivalence between a broad class of QNNs and Fourier models, and study the
interplay between the \emph{effective dimension} and the \emph{bias} of a model
towards a given task, investigating how these affect the model's training and
performance. We show that for a model that is completely agnostic, or unbiased,
towards the function to be learned, a higher effective dimension likely results
in a better trainability and performance. On the other hand, for models that
are biased towards the function to be learned a lower effective dimension is
likely beneficial during training. To obtain these results, we derive an
analytical expression of the FIM for Fourier models and identify the features
controlling a model's effective dimension. This allows us to construct models
with tunable effective dimension and bias, and to compare their training. We
furthermore introduce a tensor network representation of the considered Fourier
models, which could be a tool of independent interest for the analysis of QNN
models. Overall, these findings provide an explicit example of the interplay
between geometrical properties, model-task alignment and training, which are
relevant for the broader machine learning community.

</details>


### [98] [Grouped Differential Attention](https://arxiv.org/abs/2510.06949)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Wai Ting Cheung,Beomgyu Kim,Taehwan Kim,Haesol Lee,Junhyeok Lee,Dongpin Oh,Eunhwan Park*

Main category: cs.LG

TL;DR: 提出Grouped Differential Attention (GDA)，通过不平衡的注意力头分配策略，将更多头分配给信号提取组，较少头分配给噪声控制组，从而提高信号保真度和模型效率。


<details>
  <summary>Details</summary>
Motivation: 传统自注意力机制存在效率问题，经常将大量注意力分配给冗余或噪声上下文。Differential Attention虽然解决了部分问题，但其平衡的头分配限制了表示灵活性和可扩展性。

Method: GDA采用不平衡的头分配策略，将注意力头分为信号保留组和噪声控制组，前者分配更多头，后者通过受控重复来稳定。还提出了组差异化增长策略，仅复制信号聚焦的头来实现高效容量扩展。

Result: 大规模预训练和持续训练实验表明，GDA中的适度不平衡比率相比对称基线在泛化性和稳定性方面带来显著改进。

Conclusion: 比率感知的头分配和选择性扩展为设计可扩展、计算高效的Transformer架构提供了有效实用的路径。

Abstract: The self-attention mechanism, while foundational to modern Transformer
architectures, suffers from a critical inefficiency: it frequently allocates
substantial attention to redundant or noisy context. Differential Attention
addressed this by using subtractive attention maps for signal and noise, but
its required balanced head allocation imposes rigid constraints on
representational flexibility and scalability.
  To overcome this, we propose Grouped Differential Attention (GDA), a novel
approach that introduces unbalanced head allocation between signal-preserving
and noise-control groups. GDA significantly enhances signal focus by
strategically assigning more heads to signal extraction and fewer to
noise-control, stabilizing the latter through controlled repetition (akin to
GQA). This design achieves stronger signal fidelity with minimal computational
overhead. We further extend this principle to group-differentiated growth, a
scalable strategy that selectively replicates only the signal-focused heads,
thereby ensuring efficient capacity expansion.
  Through large-scale pretraining and continual training experiments, we
demonstrate that moderate imbalance ratios in GDA yield substantial
improvements in generalization and stability compared to symmetric baselines.
Our results collectively establish that ratio-aware head allocation and
selective expansion offer an effective and practical path toward designing
scalable, computation-efficient Transformer architectures.

</details>


### [99] [From Condensation to Rank Collapse: A Two-Stage Analysis of Transformer Training Dynamics](https://arxiv.org/abs/2510.06954)
*Zheng-An Chen,Tao Luo*

Main category: cs.LG

TL;DR: 该论文分析了线性化Transformer的训练动态，揭示了注意力模块训练的两个阶段：第一阶段权重扰动帮助逃离小初始化，第二阶段键查询矩阵参与训练导致秩崩溃。


<details>
  <summary>Details</summary>
Motivation: 虽然Transformer模型表现出优异的实证性能，但其训练动态的基本原理在配置特定研究之外缺乏充分表征。受语言模型在小初始化尺度下推理能力提升的实证证据启发，本研究旨在系统分析Transformer训练动态。

Method: 使用[Zhou et al. NeurIPS 2022]建立的梯度流分析框架，系统研究线性化Transformer的训练动态，理论分析将注意力模块动态分解为两个不同阶段。

Result: 第一阶段：随机初始化的非对称权重扰动维持参数矩阵中的非退化梯度动态，促进系统性地逃离小初始化状态；第二阶段：先前静态的键查询矩阵积极参与训练，驱动归一化矩阵向渐近秩崩溃发展。

Conclusion: 这个两阶段框架推广了经典的方向收敛结果，为理解Transformer训练动态提供了理论洞察。

Abstract: Although transformer-based models have shown exceptional empirical
performance, the fundamental principles governing their training dynamics are
inadequately characterized beyond configuration-specific studies. Inspired by
empirical evidence showing improved reasoning capabilities under small
initialization scales in language models, we employ the gradient flow
analytical framework established in [Zhou et al. NeurIPS 2022] to
systematically investigate linearized Transformer training dynamics. Our
theoretical analysis dissects the dynamics of attention modules into two
distinct stages. In the first stage, asymmetric weight perturbations from
random initialization sustain non-degenerate gradient dynamics in parameter
matrices, facilitating systematic escape from small initialization regimes.
Subsequently, these matrices undergo condensation, progressively aligning
toward the target orientation. In the second stage, the previously static
key-query matrices actively participate in training, driving the normalized
matrices toward asymptotic rank collapse. This two-stage framework generalizes
classical directional convergence results.

</details>


### [100] [High-Rate Mixout: Revisiting Mixout for Robust Domain Generalization](https://arxiv.org/abs/2510.06955)
*Masih Aminbeidokhti,Heitor Rapela Medeiros,Eric Granger,Marco Pedersoli*

Main category: cs.LG

TL;DR: Mixout是一种替代Dropout的随机正则化技术，通过在训练过程中概率性地将微调权重与预训练权重交换来平衡适应性和先验知识保留，在领域泛化基准测试中达到与集成方法相当的精度，同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 集成微调模型虽然能提高分布偏移下的鲁棒性，但计算和存储成本高昂；Dropout虽然轻量，但在预训练模型中容易过度正则化并破坏关键表示。需要一种既能保持泛化能力又计算高效的方法。

Method: 使用Mixout技术，以高掩码概率（ViT为0.9，ResNet为0.8）在训练过程中随机将微调权重替换为预训练权重，惩罚偏离预训练参数的行为，同时减少梯度计算和内存使用。

Result: 在PACS、VLCS、OfficeHome、TerraIncognita和DomainNet五个领域泛化基准测试中，高掩码率Mixout达到了与集成方法相当的域外精度，同时梯度计算减少45%，梯度内存使用减少90%。

Conclusion: 高掩码率Mixout是一种有效的领域泛化方法，能够在保持性能的同时显著降低计算开销，为预训练模型的微调提供了更高效的替代方案。

Abstract: Ensembling fine-tuned models initialized from powerful pre-trained weights is
a common strategy to improve robustness under distribution shifts, but it comes
with substantial computational costs due to the need to train and store
multiple models. Dropout offers a lightweight alternative by simulating
ensembles through random neuron deactivation; however, when applied to
pre-trained models, it tends to over-regularize and disrupt critical
representations necessary for generalization. In this work, we investigate
Mixout, a stochastic regularization technique that provides an alternative to
Dropout for domain generalization. Rather than deactivating neurons, Mixout
mitigates overfitting by probabilistically swapping a subset of fine-tuned
weights with their pre-trained counterparts during training, thereby
maintaining a balance between adaptation and retention of prior knowledge. Our
study reveals that achieving strong performance with Mixout on domain
generalization benchmarks requires a notably high masking probability of 0.9
for ViTs and 0.8 for ResNets. While this may seem like a simple adjustment, it
yields two key advantages for domain generalization: (1) higher masking rates
more strongly penalize deviations from the pre-trained parameters, promoting
better generalization to unseen domains; and (2) high-rate masking
substantially reduces computational overhead, cutting gradient computation by
up to 45% and gradient memory usage by up to 90%. Experiments across five
domain generalization benchmarks, PACS, VLCS, OfficeHome, TerraIncognita, and
DomainNet, using ResNet and ViT architectures, show that our approach,
High-rate Mixout, achieves out-of-domain accuracy comparable to ensemble-based
methods while significantly reducing training costs.

</details>


### [101] [Revisiting Mixout: An Overlooked Path to Robust Finetuning](https://arxiv.org/abs/2510.06982)
*Masih Aminbeidokhti,Heitor Rapela Medeiros,Eric Granger,Marco Pedersoli*

Main category: cs.LG

TL;DR: GMixout是一种改进的随机正则化方法，通过动态锚点和显式重采样频率控制，在微调视觉基础模型时同时提升域内精度和分布偏移下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法在提高域内准确率的同时会损害模型在分布偏移下的鲁棒性。Mixout正则化器虽然能缓解这一问题，但其固定锚点和隐式重采样机制存在局限性。

Method: 提出GMixout方法：1) 使用指数移动平均快照作为动态锚点替代固定锚点；2) 引入显式重采样频率超参数控制掩码周期；3) 采用稀疏核实现，仅更新少量参数且无推理开销。

Result: 在ImageNet、DomainNet、iWildCam和CIFAR100-C等多个基准测试中，GMixout在保持域内精度提升的同时，在分布偏移下的表现优于Model Soups和参数高效微调基线方法。

Conclusion: GMixout通过动态锚点和可控重采样机制，有效平衡了微调过程中的域内精度和分布偏移鲁棒性，为视觉基础模型的微调提供了更优的解决方案。

Abstract: Finetuning vision foundation models often improves in-domain accuracy but
comes at the cost of robustness under distribution shift. We revisit Mixout, a
stochastic regularizer that intermittently replaces finetuned weights with
their pretrained reference, through the lens of a single-run, weight-sharing
implicit ensemble. This perspective reveals three key levers that govern
robustness: the \emph{masking anchor}, \emph{resampling frequency}, and
\emph{mask sparsity}. Guided by this analysis, we introduce GMixout, which (i)
replaces the fixed anchor with an exponential moving-average snapshot that
adapts during training, and (ii) regulates masking period via an explicit
resampling-frequency hyperparameter. Our sparse-kernel implementation updates
only a small fraction of parameters with no inference-time overhead, enabling
training on consumer-grade GPUs. Experiments on benchmarks covering covariate
shift, corruption, and class imbalance, ImageNet / ImageNet-LT, DomainNet,
iWildCam, and CIFAR100-C, GMixout consistently improves in-domain accuracy
beyond zero-shot performance while surpassing both Model Soups and strong
parameter-efficient finetuning baselines under distribution shift.

</details>


### [102] [Sharpness-Aware Data Generation for Zero-shot Quantization](https://arxiv.org/abs/2510.07018)
*Dung Hoang-Anh,Cuong Pham Trung Le,Jianfei Cai,Thanh-Toan Do*

Main category: cs.LG

TL;DR: 提出了一种考虑量化模型锐度的零样本量化方法，通过最大化合成数据与真实验证数据之间的梯度匹配来最小化锐度，从而提高量化模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有零样本量化方法在生成合成数据时未考虑量化模型的锐度，而低锐度的深度神经网络具有更好的泛化能力。

Method: 通过最大化合成数据与真实验证数据之间的梯度匹配来实现锐度最小化，并在没有真实验证集的情况下，用生成样本与其邻域之间的梯度匹配来近似。

Result: 在CIFAR-100和ImageNet数据集上的实验表明，该方法在低比特量化设置下优于现有最先进技术。

Conclusion: 提出的考虑量化模型锐度的零样本量化方法能有效提升量化模型的泛化性能。

Abstract: Zero-shot quantization aims to learn a quantized model from a pre-trained
full-precision model with no access to original real training data. The common
idea in zero-shot quantization approaches is to generate synthetic data for
quantizing the full-precision model. While it is well-known that deep neural
networks with low sharpness have better generalization ability, none of the
previous zero-shot quantization works considers the sharpness of the quantized
model as a criterion for generating training data. This paper introduces a
novel methodology that takes into account quantized model sharpness in
synthetic data generation to enhance generalization. Specifically, we first
demonstrate that sharpness minimization can be attained by maximizing gradient
matching between the reconstruction loss gradients computed on synthetic and
real validation data, under certain assumptions. We then circumvent the problem
of the gradient matching without real validation set by approximating it with
the gradient matching between each generated sample and its neighbors.
Experimental evaluations on CIFAR-100 and ImageNet datasets demonstrate the
superiority of the proposed method over the state-of-the-art techniques in
low-bit quantization settings.

</details>


### [103] [Federated Unlearning in the Wild: Rethinking Fairness and Data Discrepancy](https://arxiv.org/abs/2510.07022)
*ZiHeng Huang,Di Wu,Jun Bai,Jiale Zhang,Sicong Cao,Ji Zhang,Yingjie Hu*

Main category: cs.LG

TL;DR: 本文提出了一种公平感知的联邦遗忘方法FedCCCU，解决了联邦学习中数据遗忘的两个关键挑战：公平性问题和现实数据异质性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的遗忘技术面临公平性挑战（所有客户端被迫重新训练）和现实数据异质性评估不足的问题，现有方法在真实场景下表现不佳。

Method: 提出了FedCCCU方法，通过联邦跨客户端约束遗忘来明确解决公平性和现实数据异质性挑战。

Result: 实验结果表明，现有方法在真实设置下表现较差，而FedCCCU方法始终优于现有方法。

Conclusion: FedCCCU为现实世界的联邦遗忘提供了一个实用且可扩展的解决方案，有效解决了公平性和数据异质性挑战。

Abstract: Machine unlearning is critical for enforcing data deletion rights like the
"right to be forgotten." As a decentralized paradigm, Federated Learning (FL)
also requires unlearning, but realistic implementations face two major
challenges. First, fairness in Federated Unlearning (FU) is often overlooked.
Exact unlearning methods typically force all clients into costly retraining,
even those uninvolved. Approximate approaches, using gradient ascent or
distillation, make coarse interventions that can unfairly degrade performance
for clients with only retained data. Second, most FU evaluations rely on
synthetic data assumptions (IID/non-IID) that ignore real-world heterogeneity.
These unrealistic benchmarks obscure the true impact of unlearning and limit
the applicability of current methods. We first conduct a comprehensive
benchmark of existing FU methods under realistic data heterogeneity and
fairness conditions. We then propose a novel, fairness-aware FU approach,
Federated Cross-Client-Constrains Unlearning (FedCCCU), to explicitly address
both challenges. FedCCCU offers a practical and scalable solution for
real-world FU. Experimental results show that existing methods perform poorly
in realistic settings, while our approach consistently outperforms them.

</details>


### [104] [Unified Molecule Pre-training with Flexible 2D and 3D Modalities: Single and Paired Modality Integration](https://arxiv.org/abs/2510.07035)
*Tengwei Song,Min Wu,Yuan Fang*

Main category: cs.LG

TL;DR: FlexMol是一个灵活的分子预训练框架，能够学习统一的分子表示并支持单模态输入，解决了现有方法需要配对2D和3D数据的问题。


<details>
  <summary>Details</summary>
Motivation: 现有分子表示学习方法需要配对的2D和3D分子数据来训练，这在某些模态不可用或计算成本高时存在局限性。

Method: 采用分离的2D和3D模型，通过参数共享提高计算效率，并使用解码器生成缺失模态的特征，支持多阶段连续学习。

Result: 在广泛的分子性质预测任务中表现出优越性能，并在不完整数据情况下也验证了有效性。

Conclusion: FlexMol框架能够灵活处理单模态输入，在分子表示学习方面取得了显著进展。

Abstract: Molecular representation learning plays a crucial role in advancing
applications such as drug discovery and material design. Existing work
leverages 2D and 3D modalities of molecular information for pre-training,
aiming to capture comprehensive structural and geometric insights. However,
these methods require paired 2D and 3D molecular data to train the model
effectively and prevent it from collapsing into a single modality, posing
limitations in scenarios where a certain modality is unavailable or
computationally expensive to generate. To overcome this limitation, we propose
FlexMol, a flexible molecule pre-training framework that learns unified
molecular representations while supporting single-modality input. Specifically,
inspired by the unified structure in vision-language models, our approach
employs separate models for 2D and 3D molecular data, leverages parameter
sharing to improve computational efficiency, and utilizes a decoder to generate
features for the missing modality. This enables a multistage continuous
learning process where both modalities contribute collaboratively during
training, while ensuring robustness when only one modality is available during
inference. Extensive experiments demonstrate that FlexMol achieves superior
performance across a wide range of molecular property prediction tasks, and we
also empirically demonstrate its effectiveness with incomplete data. Our code
and data are available at https://github.com/tewiSong/FlexMol.

</details>


### [105] [COMPASS: A Multi-Turn Benchmark for Tool-Mediated Planning & Preference Optimization](https://arxiv.org/abs/2510.07043)
*Tian Qin,Felix Bai,Ting-Yao Hu,Raviteja Vemulapalli,Hema Swetha Koppula,Zhiyang Xu,Bowen Jin,Mert Cemri,Jiarui Lu,Zirui Wang,Meng Cao*

Main category: cs.LG

TL;DR: COMPASS是一个评估LLM智能体在真实旅行规划场景中表现的新基准，将旅行规划建模为约束偏好优化问题，要求智能体在满足硬约束的同时优化用户软偏好。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的LLM智能体需要通过多轮交互掌握策略性工具使用和用户偏好优化，以协助用户完成复杂规划任务。现有模型在真实场景中的表现存在明显差距。

Method: 构建了覆盖20个美国国家公园的交通、住宿和票务的真实旅行数据库，以及模拟商业预订平台的综合工具生态系统，将旅行规划建模为约束偏好优化问题。

Result: 评估发现两个关键差距：(1)可接受-最优差距：智能体可靠满足约束但无法优化偏好；(2)计划协调差距：在多服务协调任务上性能崩溃，特别是开源模型。

Conclusion: COMPASS通过在实际用户面向领域中建立推理和规划基础，提供了一个直接衡量智能体在真实任务中优化用户偏好能力的基准，连接理论进展与现实影响。

Abstract: Real-world large language model (LLM) agents must master strategic tool use
and user preference optimization through multi-turn interactions to assist
users with complex planning tasks. We introduce COMPASS (Constrained
Optimization through Multi-turn Planning and Strategic Solutions), a benchmark
that evaluates agents on realistic travel-planning scenarios. We cast travel
planning as a constrained preference optimization problem, where agents must
satisfy hard constraints while simultaneously optimizing soft user preferences.
To support this, we build a realistic travel database covering transportation,
accommodation, and ticketing for 20 U.S. National Parks, along with a
comprehensive tool ecosystem that mirrors commercial booking platforms.
Evaluating state-of-the-art models, we uncover two critical gaps: (i) an
acceptable-optimal gap, where agents reliably meet constraints but fail to
optimize preferences, and (ii) a plan-coordination gap, where performance
collapses on multi-service (flight and hotel) coordination tasks, especially
for open-source models. By grounding reasoning and planning in a practical,
user-facing domain, COMPASS provides a benchmark that directly measures an
agent's ability to optimize user preferences in realistic tasks, bridging
theoretical advances with real-world impact.

</details>


### [106] [Enhancing Speech Emotion Recognition via Fine-Tuning Pre-Trained Models and Hyper-Parameter Optimisation](https://arxiv.org/abs/2510.07052)
*Aryan Golbaghi,Shuo Zhou*

Main category: cs.LG

TL;DR: 提出结合预训练表示和自动超参数优化的语音情感识别工作流，在CPU上实现高效且竞争性的性能


<details>
  <summary>Details</summary>
Motivation: 解决语音情感识别中传统方法需要大量计算资源和时间的问题，探索在普通CPU上使用预训练模型和高效超参数优化的可行性

Method: 使用SpeechBrain wav2vec2-base模型作为编码器，在IEMOCAP上微调，比较高斯过程贝叶斯优化和树结构Parzen估计器两种HPO策略

Result: GP-BO在11分钟内达到0.96平衡类准确率，TPE在15分钟内达到0.97，显著优于网格搜索和AutoSpeech 2020基线

Conclusion: 高效的超参数优化与预训练编码器结合可以在普通CPU上实现竞争性的语音情感识别性能，并具有良好的跨语言泛化能力

Abstract: We propose a workflow for speech emotion recognition (SER) that combines
pre-trained representations with automated hyperparameter optimisation (HPO).
Using SpeechBrain wav2vec2-base model fine-tuned on IEMOCAP as the encoder, we
compare two HPO strategies, Gaussian Process Bayesian Optimisation (GP-BO) and
Tree-structured Parzen Estimators (TPE), under an identical four-dimensional
search space and 15-trial budget, with balanced class accuracy (BCA) on the
German EmoDB corpus as the objective. All experiments run on 8 CPU cores with
32 GB RAM. GP-BO achieves 0.96 BCA in 11 minutes, and TPE (Hyperopt
implementation) attains 0.97 in 15 minutes. In contrast, grid search requires
143 trials and 1,680 minutes to exceed 0.9 BCA, and the best AutoSpeech 2020
baseline reports only 0.85 in 30 minutes on GPU. For cross-lingual
generalisation, an EmoDB-trained HPO-tuned model improves zero-shot accuracy by
0.25 on CREMA-D and 0.26 on RAVDESS. Results show that efficient HPO with
pre-trained encoders delivers competitive SER on commodity CPUs. Source code to
this work is available at:
https://github.com/youngaryan/speechbrain-emotion-hpo.

</details>


### [107] [Introspection in Learned Semantic Scene Graph Localisation](https://arxiv.org/abs/2510.07053)
*Manshika Charvi Bissessur,Efimia Panagiotaki,Daniele De Martini*

Main category: cs.LG

TL;DR: 该研究探讨了语义信息如何影响自监督对比语义定位框架中的定位性能和鲁棒性，通过可解释性分析发现模型能够过滤环境噪声并优先关注显著地标。


<details>
  <summary>Details</summary>
Motivation: 研究语义在自学习定位系统中的作用，探究模型是否能够区分环境噪声与显著地标，提高定位的鲁棒性和可解释性。

Method: 训练定位网络于原始和扰动地图上，进行事后内省分析，验证多种可解释性方法，并进行语义类别消融实验。

Result: 积分梯度和注意力权重被证明是最可靠的学习行为探针，频繁出现的对象往往被降权处理，模型学习到了噪声鲁棒的语义显著关系。

Conclusion: 模型能够学习噪声鲁棒、语义显著的场所定义关系，从而在具有挑战性的视觉和结构变化下实现可解释的定位注册。

Abstract: This work investigates how semantics influence localisation performance and
robustness in a learned self-supervised, contrastive semantic localisation
framework. After training a localisation network on both original and perturbed
maps, we conduct a thorough post-hoc introspection analysis to probe whether
the model filters environmental noise and prioritises distinctive landmarks
over routine clutter. We validate various interpretability methods and present
a comparative reliability analysis. Integrated gradients and Attention Weights
consistently emerge as the most reliable probes of learned behaviour. A
semantic class ablation further reveals an implicit weighting in which frequent
objects are often down-weighted. Overall, the results indicate that the model
learns noise-robust, semantically salient relations about place definition,
thereby enabling explainable registration under challenging visual and
structural variations.

</details>


### [108] [Blind Construction of Angular Power Maps in Massive MIMO Networks](https://arxiv.org/abs/2510.07071)
*Zheng Xing,Junting Chen*

Main category: cs.LG

TL;DR: 提出基于隐马尔可夫模型的无监督角度功率地图构建方法，利用大规模MIMO网络中的CSI数据无需位置标签即可估计移动设备位置并构建角度功率地图。


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO网络中CSI获取具有挑战性，传统无线电地图构建方法需要位置标记的CSI数据，这在实践中难以实现。

Method: 构建隐马尔可夫模型连接移动设备隐藏轨迹与大规模MIMO信道CSI演化，从而估计移动设备位置并构建角度功率地图。

Result: 在均匀直线移动和泊松分布基站场景下，定位误差的克拉美罗下界可在任何信噪比下消失；在基站受限区域，即使有无限独立测量误差仍非零。实际多小区大规模MIMO网络中实现平均18米定位误差。

Conclusion: 所提出的无监督方法能够有效构建角度功率地图，在真实网络环境中实现精确的移动设备定位。

Abstract: Channel state information (CSI) acquisition is a challenging problem in
massive multiple-input multiple-output (MIMO) networks. Radio maps provide a
promising solution for radio resource management by reducing online CSI
acquisition. However, conventional approaches for radio map construction
require location-labeled CSI data, which is challenging in practice. This paper
investigates unsupervised angular power map construction based on large
timescale CSI data collected in a massive MIMO network without location labels.
A hidden Markov model (HMM) is built to connect the hidden trajectory of a
mobile with the CSI evolution of a massive MIMO channel. As a result, the
mobile location can be estimated, enabling the construction of an angular power
map. We show that under uniform rectilinear mobility with Poisson-distributed
base stations (BSs), the Cramer-Rao Lower Bound (CRLB) for localization error
can vanish at any signal-to-noise ratios (SNRs), whereas when BSs are confined
to a limited region, the error remains nonzero even with infinite independent
measurements. Based on reference signal received power (RSRP) data collected in
a real multi-cell massive MIMO network, an average localization error of 18
meters can be achieved although measurements are mainly obtained from a single
serving cell.

</details>


### [109] [HTMformer: Hybrid Time and Multivariate Transformer for Time Series Forecasting](https://arxiv.org/abs/2510.07084)
*Tan Wang,Yun Wei Dong,Tao Zhang,Qi Wang*

Main category: cs.LG

TL;DR: 提出HTMformer模型，通过混合时序和多变量嵌入(HTME)增强Transformer在时间序列预测中的性能，在保持轻量化的同时提升准确率


<details>
  <summary>Details</summary>
Motivation: 现有Transformer方法在时间序列预测中过度强调时序依赖关系，导致计算开销增加但性能提升有限，性能高度依赖于嵌入方法的质量

Method: 设计HTME提取器，整合轻量级时序特征提取模块和精心设计的多变量特征提取模块，提供互补特征，平衡模型复杂度和性能

Result: 在8个真实世界数据集上的实验表明，该方法在准确性和效率方面均优于现有基线方法

Conclusion: HTME能够提取更丰富的序列表示，使Transformer预测器更好地理解时间序列，实现轻量化高性能的时间序列预测

Abstract: Transformer-based methods have achieved impressive results in time series
forecasting. However, existing Transformers still exhibit limitations in
sequence modeling as they tend to overemphasize temporal dependencies. This
incurs additional computational overhead without yielding corresponding
performance gains. We find that the performance of Transformers is highly
dependent on the embedding method used to learn effective representations. To
address this issue, we extract multivariate features to augment the effective
information captured in the embedding layer, yielding multidimensional
embeddings that convey richer and more meaningful sequence representations.
These representations enable Transformer-based forecasters to better understand
the series. Specifically, we introduce Hybrid Temporal and Multivariate
Embeddings (HTME). The HTME extractor integrates a lightweight temporal feature
extraction module with a carefully designed multivariate feature extraction
module to provide complementary features, thereby achieving a balance between
model complexity and performance. By combining HTME with the Transformer
architecture, we present HTMformer, leveraging the enhanced feature extraction
capability of the HTME extractor to build a lightweight forecaster. Experiments
conducted on eight real-world datasets demonstrate that our approach
outperforms existing baselines in both accuracy and efficiency.

</details>


### [110] [Non-Stationary Online Structured Prediction with Surrogate Losses](https://arxiv.org/abs/2510.07086)
*Shinsaku Sakaue,Han Bao,Yuzhou Cao*

Main category: cs.LG

TL;DR: 本文针对非平稳环境中的在线结构化预测问题，提出了一个新型的累积目标损失上界：F_T + C(1 + P_T)，其中F_T是比较器序列的累积代理损失，P_T是其路径长度，C为常数。该界限仅通过F_T和P_T依赖于时间T，在非平稳环境中提供更强的保证。


<details>
  <summary>Details</summary>
Motivation: 传统在线结构化预测中的代理遗憾分析在非平稳环境中失效，因为每个固定估计器都可能产生与时间T线性增长的代理损失。需要开发适用于非平稳环境的理论保证。

Method: 将在线梯度下降(OGD)的动态遗憾界限与利用代理差距的技术相结合，并引入新的Polyak风格学习率。进一步通过卷积Fenchel-Young损失扩展到更广泛的问题类别。

Result: 证明了累积目标损失的上界为F_T + C(1 + P_T)，该界限仅通过比较器序列的累积代理损失F_T和路径长度P_T依赖于时间T。同时证明了这种对F_T和P_T的依赖是紧的。

Conclusion: 所提出的方法在非平稳环境中提供了更强的理论保证，通过新的Polyak风格学习率实现了系统性的目标损失保证，并在实证中表现出良好性能。

Abstract: Online structured prediction, including online classification as a special
case, is the task of sequentially predicting labels from input features.
Therein the surrogate regret -- the cumulative excess of the target loss (e.g.,
0-1 loss) over the surrogate loss (e.g., logistic loss) of the fixed best
estimator -- has gained attention, particularly because it often admits a
finite bound independent of the time horizon $T$. However, such guarantees
break down in non-stationary environments, where every fixed estimator may
incur the surrogate loss growing linearly with $T$. We address this by proving
a bound of the form $F_T + C(1 + P_T)$ on the cumulative target loss, where
$F_T$ is the cumulative surrogate loss of any comparator sequence, $P_T$ is its
path length, and $C > 0$ is some constant. This bound depends on $T$ only
through $F_T$ and $P_T$, often yielding much stronger guarantees in
non-stationary environments. Our core idea is to synthesize the dynamic regret
bound of the online gradient descent (OGD) with the technique of exploiting the
surrogate gap. Our analysis also sheds light on a new Polyak-style learning
rate for OGD, which systematically offers target-loss guarantees and exhibits
promising empirical performance. We further extend our approach to a broader
class of problems via the convolutional Fenchel--Young loss. Finally, we prove
a lower bound showing that the dependence on $F_T$ and $P_T$ is tight.

</details>


### [111] [Generative World Modelling for Humanoids: 1X World Model Challenge Technical Report](https://arxiv.org/abs/2510.07092)
*Riccardo Mereu,Aidan Scannell,Yuxin Hou,Yi Zhao,Aditya Jitta,Antonio Dominguez,Luigi Acerbi,Amos Storkey,Paul Chang*

Main category: cs.LG

TL;DR: 该论文介绍了在1X世界模型挑战赛中，针对真实世界人形机器人交互的两种方法：采样跟踪使用Wan-2.2 TI2V-5B视频生成模型进行未来帧预测，压缩跟踪使用时空Transformer模型预测离散潜在代码，在两个挑战中都获得了第一名。


<details>
  <summary>Details</summary>
Motivation: 世界模型是AI和机器人技术中的强大范式，能够通过预测视觉观察或紧凑潜在状态来推理未来。1X世界模型挑战赛提供了一个真实世界人形机器人交互的开源基准，包含采样和压缩两个互补的跟踪任务。

Method: 对于采样跟踪，采用Wan-2.2 TI2V-5B视频生成基础模型，通过AdaLN-Zero条件化机器人状态，并使用LoRA进行后训练。对于压缩跟踪，从头训练一个时空Transformer模型来预测未来离散潜在代码。

Result: 采样任务中达到23.0 dB PSNR，压缩任务中Top-500 CE为6.6386，在两个挑战中都获得了第一名。

Conclusion: 通过适应视频生成基础模型和从头训练专用模型，成功解决了真实世界人形机器人交互的未来预测问题，在两个不同的世界模型任务中都取得了最佳性能。

Abstract: World models are a powerful paradigm in AI and robotics, enabling agents to
reason about the future by predicting visual observations or compact latent
states. The 1X World Model Challenge introduces an open-source benchmark of
real-world humanoid interaction, with two complementary tracks: sampling,
focused on forecasting future image frames, and compression, focused on
predicting future discrete latent codes. For the sampling track, we adapt the
video generation foundation model Wan-2.2 TI2V-5B to video-state-conditioned
future frame prediction. We condition the video generation on robot states
using AdaLN-Zero, and further post-train the model using LoRA. For the
compression track, we train a Spatio-Temporal Transformer model from scratch.
Our models achieve 23.0 dB PSNR in the sampling task and a Top-500 CE of 6.6386
in the compression task, securing 1st place in both challenges.

</details>


### [112] [Non-Asymptotic Analysis of Efficiency in Conformalized Regression](https://arxiv.org/abs/2510.07093)
*Yunzhen Yao,Lie He,Michael Gastpar*

Main category: cs.LG

TL;DR: 该论文建立了保形预测中预测集长度与理想区间长度偏差的非渐近界，揭示了效率与训练集大小、校准集大小和错误覆盖水平之间的联合依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现有关于保形回归效率的研究通常将错误覆盖水平α视为固定常数，而本文旨在分析α变化时预测集效率的理论保证。

Method: 使用通过随机梯度下降训练的保形化分位数和中位数回归，在数据分布温和假设下建立非渐近界。

Result: 推导出阶数为O(1/√n + 1/(α²n) + 1/√m + exp(-α²m))的边界，识别了α不同区间内收敛速率的相变。

Conclusion: 理论结果为数据分配以控制预测集长度提供了指导，实证结果与理论发现一致。

Abstract: Conformal prediction provides prediction sets with coverage guarantees. The
informativeness of conformal prediction depends on its efficiency, typically
quantified by the expected size of the prediction set. Prior work on the
efficiency of conformalized regression commonly treats the miscoverage level
$\alpha$ as a fixed constant. In this work, we establish non-asymptotic bounds
on the deviation of the prediction set length from the oracle interval length
for conformalized quantile and median regression trained via SGD, under mild
assumptions on the data distribution. Our bounds of order
$\mathcal{O}(1/\sqrt{n} + 1/(\alpha^2 n) + 1/\sqrt{m} + \exp(-\alpha^2 m))$
capture the joint dependence of efficiency on the proper training set size $n$,
the calibration set size $m$, and the miscoverage level $\alpha$. The results
identify phase transitions in convergence rates across different regimes of
$\alpha$, offering guidance for allocating data to control excess prediction
set length. Empirical results are consistent with our theoretical findings.

</details>


### [113] [ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL](https://arxiv.org/abs/2510.07151)
*Egor Cherepanov,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: ELMUR是一种具有结构化外部记忆的Transformer架构，通过双向交叉注意力和LRU记忆模块处理长序列依赖，在部分可观测环境中显著提升决策性能


<details>
  <summary>Details</summary>
Motivation: 现实机器人需要在部分可观测和长视野环境下决策，但现有方法主要依赖瞬时信息，难以处理长期依赖关系

Method: 提出ELMUR架构，每层维护记忆嵌入，通过双向交叉注意力交互，使用LRU记忆模块进行替换或凸混合更新

Result: 在T-Maze任务中达到100%成功率（最长100万步走廊），在POPGym中超过半数任务优于基线，在MIKASA-Robo稀疏奖励任务中性能翻倍

Conclusion: 结构化、层局部外部记忆为部分可观测环境下的决策提供了一种简单且可扩展的解决方案

Abstract: Real-world robotic agents must act under partial observability and long
horizons, where key cues may appear long before they affect decision making.
However, most modern approaches rely solely on instantaneous information,
without incorporating insights from the past. Standard recurrent or transformer
models struggle with retaining and leveraging long-term dependencies: context
windows truncate history, while naive memory extensions fail under scale and
sparsity. We propose ELMUR (External Layer Memory with Update/Rewrite), a
transformer architecture with structured external memory. Each layer maintains
memory embeddings, interacts with them via bidirectional cross-attention, and
updates them through an Least Recently Used (LRU) memory module using
replacement or convex blending. ELMUR extends effective horizons up to 100,000
times beyond the attention window and achieves a 100% success rate on a
synthetic T-Maze task with corridors up to one million steps. In POPGym, it
outperforms baselines on more than half of the tasks. On MIKASA-Robo
sparse-reward manipulation tasks with visual observations, it nearly doubles
the performance of strong baselines. These results demonstrate that structured,
layer-local external memory offers a simple and scalable approach to decision
making under partial observability.

</details>


### [114] [Bridged Clustering for Representation Learning: Semi-Supervised Sparse Bridging](https://arxiv.org/abs/2510.07182)
*Patrick Peixuan Ye,Chen Shani,Ellen Vitercik*

Main category: cs.LG

TL;DR: 提出了Bridged Clustering框架，使用少量配对样本在未配对的输入X和输出Y数据集之间学习稀疏、可解释的桥梁，通过聚类和桥接实现半监督预测。


<details>
  <summary>Details</summary>
Motivation: 传统半监督学习未充分利用输出数据，而密集传输方法缺乏可解释性，需要一种既能利用输出数据又保持稀疏可解释性的方法。

Method: 首先独立聚类X和Y，然后用少量配对样本学习稀疏的桥接关系，推理时通过最近输入簇找到对应的输出簇中心作为预测。

Result: 理论分析表明在有限误聚类和误桥接率下算法有效，实证结果在低监督设置下与SOTA方法竞争，同时保持简单、模型无关和高标签效率。

Conclusion: Bridged Clustering提供了一种简单有效的半监督学习框架，特别适合标签稀缺场景，兼具可解释性和效率优势。

Abstract: We introduce Bridged Clustering, a semi-supervised framework to learn
predictors from any unpaired input $X$ and output $Y$ dataset. Our method first
clusters $X$ and $Y$ independently, then learns a sparse, interpretable bridge
between clusters using only a few paired examples. At inference, a new input
$x$ is assigned to its nearest input cluster, and the centroid of the linked
output cluster is returned as the prediction $\hat{y}$. Unlike traditional SSL,
Bridged Clustering explicitly leverages output-only data, and unlike dense
transport-based methods, it maintains a sparse and interpretable alignment.
Through theoretical analysis, we show that with bounded mis-clustering and
mis-bridging rates, our algorithm becomes an effective and efficient predictor.
Empirically, our method is competitive with SOTA methods while remaining
simple, model-agnostic, and highly label-efficient in low-supervision settings.

</details>


### [115] [Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples](https://arxiv.org/abs/2510.07192)
*Alexandra Souly,Javier Rando,Ed Chapman,Xander Davies,Burak Hasircioglu,Ezzeldin Shereen,Carlos Mougan,Vasilios Mavroudis,Erik Jones,Chris Hicks,Nicholas Carlini,Yarin Gal,Robert Kirk*

Main category: cs.LG

TL;DR: 研究表明，对大语言模型进行数据投毒攻击所需的恶意文档数量与模型大小无关，仅需约250个文档即可成功攻击从6亿到130亿参数的模型，这比之前认为的要容易得多。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设攻击者需要控制训练数据的一定比例，但对于大型模型来说，即使很小的比例也意味着不切实际的大量数据。本文旨在验证投毒攻击是否真的需要与数据集大小成比例的恶意数据。

Method: 进行了迄今为止最大规模的预训练投毒实验，训练了从600M到13B参数的模型，使用了6B到260B tokens的数据集。同时进行了小规模实验来研究影响攻击成功的因素，包括投毒数据与干净数据的比例以及投毒样本的非随机分布。

Result: 发现仅需250个投毒文档就能在所有模型和数据集大小上成功攻击，即使最大的模型训练了超过20倍的干净数据。同样的动态也适用于微调阶段的投毒。

Conclusion: 通过数据投毒注入后门对大型模型来说可能比之前认为的更容易，因为所需的投毒数量不会随模型大小而增加，这凸显了需要更多研究来减轻未来模型中的这种风险。

Abstract: Poisoning attacks can compromise the safety of large language models (LLMs)
by injecting malicious documents into their training data. Existing work has
studied pretraining poisoning assuming adversaries control a percentage of the
training corpus. However, for large models, even small percentages translate to
impractically large amounts of data. This work demonstrates for the first time
that poisoning attacks instead require a near-constant number of documents
regardless of dataset size. We conduct the largest pretraining poisoning
experiments to date, pretraining models from 600M to 13B parameters on
chinchilla-optimal datasets (6B to 260B tokens). We find that 250 poisoned
documents similarly compromise models across all model and dataset sizes,
despite the largest models training on more than 20 times more clean data. We
also run smaller-scale experiments to ablate factors that could influence
attack success, including broader ratios of poisoned to clean data and
non-random distributions of poisoned samples. Finally, we demonstrate the same
dynamics for poisoning during fine-tuning. Altogether, our results suggest that
injecting backdoors through data poisoning may be easier for large models than
previously believed as the number of poisons required does not scale up with
model size, highlighting the need for more research on defences to mitigate
this risk in future models.

</details>


### [116] [An in-depth look at approximation via deep and narrow neural networks](https://arxiv.org/abs/2510.07202)
*Joris Dommel,Sven A. Wegner*

Main category: cs.LG

TL;DR: 该论文研究了在宽度w=n和w=n+1情况下，神经网络对Hanin和Sellke(2017)提出的反例函数的逼近能力，分析了深度变化对逼近质量的影响以及神经元死亡现象的作用。


<details>
  <summary>Details</summary>
Motivation: Hanin和Sellke在2017年证明了当且仅当w>n时，任意深度的ReLU网络才能在紧集上一致逼近连续函数。本文旨在研究在临界宽度w=n和w=n+1时，神经网络对原反例函数的实际逼近能力。

Method: 通过构建神经网络来逼近Hanin和Sellke的反例函数，分析在不同深度下逼近质量的变化，并研究神经元死亡现象对逼近行为的影响。

Result: 在宽度w=n和w=n+1的临界情况下，神经网络确实能够逼近反例函数，但逼近质量受到深度变化和神经元死亡现象的影响。

Conclusion: 即使在临界宽度w=n和w=n+1时，神经网络仍具有一定的逼近能力，但深度和神经元死亡现象对逼近效果有显著影响，这为理解神经网络逼近能力的边界提供了新的见解。

Abstract: In 2017, Hanin and Sellke showed that the class of arbitrarily deep,
real-valued, feed-forward and ReLU-activated networks of width w forms a dense
subset of the space of continuous functions on R^n, with respect to the
topology of uniform convergence on compact sets, if and only if w>n holds. To
show the necessity, a concrete counterexample function f:R^n->R was used. In
this note we actually approximate this very f by neural networks in the two
cases w=n and w=n+1 around the aforementioned threshold. We study how the
approximation quality behaves if we vary the depth and what effect (spoiler
alert: dying neurons) cause that behavior.

</details>


### [117] [Guided by the Experts: Provable Feature Learning Dynamic of Soft-Routed Mixture-of-Experts](https://arxiv.org/abs/2510.07205)
*Fangshuo Liao,Anastasios Kyrillidis*

Main category: cs.LG

TL;DR: 该论文为软路由MoE模型提供了收敛保证，证明了在适度过参数化下，学生网络能够通过特征学习阶段恢复教师网络参数，并展示了后训练剪枝和微调的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管MoE架构在现代AI系统中广泛应用，但其训练动态的理论理解仍局限于分离的专家-路由器优化或特定场景。本文旨在推进MoE理论，为联合训练提供收敛保证。

Method: 采用学生-教师框架，分析软路由MoE模型与非线性的路由器和专家的联合训练过程，证明在适度过参数化下的特征学习阶段，并研究后训练剪枝和微调。

Result: 证明了学生网络能够通过路由器被专家"引导"的学习过程恢复教师网络参数，后训练剪枝可有效消除冗余神经元，微调过程可收敛到全局最优。

Conclusion: 该分析首次为理解MoE架构的优化景观提供了新的理论见解，填补了MoE联合训练理论研究的空白。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a cornerstone of
modern AI systems. In particular, MoEs route inputs dynamically to specialized
experts whose outputs are aggregated through weighted summation. Despite their
widespread application, theoretical understanding of MoE training dynamics
remains limited to either separate expert-router optimization or only top-1
routing scenarios with carefully constructed datasets. This paper advances MoE
theory by providing convergence guarantees for joint training of soft-routed
MoE models with non-linear routers and experts in a student-teacher framework.
We prove that, with moderate over-parameterization, the student network
undergoes a feature learning phase, where the router's learning process is
``guided'' by the experts, that recovers the teacher's parameters. Moreover, we
show that a post-training pruning can effectively eliminate redundant neurons,
followed by a provably convergent fine-tuning process that reaches global
optimality. To our knowledge, our analysis is the first to bring novel insights
in understanding the optimization landscape of the MoE architecture.

</details>


### [118] [A Broader View of Thompson Sampling](https://arxiv.org/abs/2510.07208)
*Yanlin Qu,Hongseok Namkoong,Assaf Zeevi*

Main category: cs.LG

TL;DR: 本文揭示了Thompson Sampling作为在线优化算法的本质，通过引入"忠实"平稳化方法，将有限时域动态优化问题转换为平稳对应问题，从而解释了Thompson Sampling如何平衡探索与利用。


<details>
  <summary>Details</summary>
Motivation: Thompson Sampling作为最广泛使用的多臂赌博机算法之一，其通过后验采样来平衡探索与利用的确切机制一直是个谜。本文旨在揭示这一核心机制。

Method: 引入"忠实"平稳化方法，将有限时域动态优化问题转换为平稳对应问题，利用Bellman原理研究时间不变的最优策略，将Thompson Sampling重新表述为在线优化算法形式。

Result: 发现Thompson Sampling具有简单的在线优化形式，模仿Bellman最优策略的结构，其中贪婪性通过基于点二列相关性的剩余不确定性度量进行正则化。

Conclusion: 这回答了Thompson Sampling如何平衡探索与利用的问题，并为研究和改进Thompson原始思想提供了原则性框架。

Abstract: Thompson Sampling is one of the most widely used and studied bandit
algorithms, known for its simple structure, low regret performance, and solid
theoretical guarantees. Yet, in stark contrast to most other families of bandit
algorithms, the exact mechanism through which posterior sampling (as introduced
by Thompson) is able to "properly" balance exploration and exploitation,
remains a mystery. In this paper we show that the core insight to address this
question stems from recasting Thompson Sampling as an online optimization
algorithm. To distill this, a key conceptual tool is introduced, which we refer
to as "faithful" stationarization of the regret formulation. Essentially, the
finite horizon dynamic optimization problem is converted into a stationary
counterpart which "closely resembles" the original objective (in contrast, the
classical infinite horizon discounted formulation, that leads to the Gittins
index, alters the problem and objective in too significant a manner). The newly
crafted time invariant objective can be studied using Bellman's principle which
leads to a time invariant optimal policy. When viewed through this lens,
Thompson Sampling admits a simple online optimization form that mimics the
structure of the Bellman-optimal policy, and where greediness is regularized by
a measure of residual uncertainty based on point-biserial correlation. This
answers the question of how Thompson Sampling balances
exploration-exploitation, and moreover, provides a principled framework to
study and further improve Thompson's original idea.

</details>


### [119] [Discriminative Feature Feedback with General Teacher Classes](https://arxiv.org/abs/2510.07245)
*Omri Bar Oz,Tosca Lechner,Sivan Sabato*

Main category: cs.LG

TL;DR: 本文系统研究了判别性特征反馈（DFF）学习协议的理论性质，分析了其在可实现和不可实现设置中的最优错误界限，并揭示了DFF与在线学习等经典协议的关键差异。


<details>
  <summary>Details</summary>
Motivation: DFF学习协议使用判别性特征解释作为反馈形式，但缺乏与经典学习协议（如监督学习和在线学习）可比的一般框架研究。本文旨在填补这一空白，系统分析DFF的理论性质。

Method: 在可实现设置中，使用新的维度概念来刻画错误界限；在不可实现设置中，提供了错误上界并证明其一般不可改进。通过比较DFF与在线学习，揭示了两者在理论性质上的差异。

Result: 在可实现设置中，使用新维度概念成功刻画了错误界限；在不可实现设置中，获得了错误上界并证明其最优性。研究结果表明，与在线学习不同，DFF中可实现维度不足以刻画最优不可实现错误界限或无遗憾算法的存在性。

Conclusion: DFF学习协议具有与经典在线学习不同的理论性质，特别是在可实现维度不足以表征不可实现设置性能方面。这为理解具有丰富反馈的学习协议提供了新的理论洞见。

Abstract: We study the theoretical properties of the interactive learning protocol
Discriminative Feature Feedback (DFF) (Dasgupta et al., 2018). The DFF learning
protocol uses feedback in the form of discriminative feature explanations. We
provide the first systematic study of DFF in a general framework that is
comparable to that of classical protocols such as supervised learning and
online learning. We study the optimal mistake bound of DFF in the realizable
and the non-realizable settings, and obtain novel structural results, as well
as insights into the differences between Online Learning and settings with
richer feedback such as DFF. We characterize the mistake bound in the
realizable setting using a new notion of dimension. In the non-realizable
setting, we provide a mistake upper bound and show that it cannot be improved
in general. Our results show that unlike Online Learning, in DFF the realizable
dimension is insufficient to characterize the optimal non-realizable mistake
bound or the existence of no-regret algorithms.

</details>


### [120] [Test-Time Graph Search for Goal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2510.07257)
*Evgenii Opryshko,Junwei Quan,Claas Voelcker,Yilun Du,Igor Gilitschenski*

Main category: cs.LG

TL;DR: TTGS是一种轻量级规划方法，通过构建数据集状态图并执行快速搜索来组装子目标序列，以解决离线目标条件强化学习中的长时程决策问题。


<details>
  <summary>Details</summary>
Motivation: 离线目标条件强化学习在长时程决策中面临时间信用分配和误差累积的挑战，离线设置会放大这些效应。

Method: TTGS接受任何状态空间距离或成本信号，构建数据集状态的加权图，执行快速搜索来组装子目标序列，由冻结策略执行。对于基于价值的学习器，距离直接从学习的目标条件价值函数导出。

Result: 在OGBench基准测试中，TTGS提高了多个基础学习器在挑战性运动任务上的成功率。

Conclusion: 简单的度量引导测试时规划对离线GCRL有益，TTGS无需改变训练、额外监督、在线交互或特权信息，完全在推理时运行。

Abstract: Offline goal-conditioned reinforcement learning (GCRL) trains policies that
reach user-specified goals at test time, providing a simple, unsupervised,
domain-agnostic way to extract diverse behaviors from unlabeled, reward-free
datasets. Nonetheless, long-horizon decision making remains difficult for GCRL
agents due to temporal credit assignment and error accumulation, and the
offline setting amplifies these effects. To alleviate this issue, we introduce
Test-Time Graph Search (TTGS), a lightweight planning approach to solve the
GCRL task. TTGS accepts any state-space distance or cost signal, builds a
weighted graph over dataset states, and performs fast search to assemble a
sequence of subgoals that a frozen policy executes. When the base learner is
value-based, the distance is derived directly from the learned goal-conditioned
value function, so no handcrafted metric is needed. TTGS requires no changes to
training, no additional supervision, no online interaction, and no privileged
information, and it runs entirely at inference. On the OGBench benchmark, TTGS
improves success rates of multiple base learners on challenging locomotion
tasks, demonstrating the benefit of simple metric-guided test-time planning for
offline GCRL.

</details>


### [121] [Dynamic Regret Bounds for Online Omniprediction with Long Term Constraints](https://arxiv.org/abs/2510.07266)
*Yahav Bechavod,Jiuyao Lu,Aaron Roth*

Main category: cs.LG

TL;DR: 提出了首个保证在线全预测中所有智能体同时获得动态遗憾界限的算法，同时确保每个智能体的约束违反趋近于零


<details>
  <summary>Details</summary>
Motivation: 解决在线全预测问题中，学习器需要为下游决策者生成预测序列，使这些决策者基于预测选择行动时能获得最坏情况效用保证并最小化约束违反

Method: 设计了一种算法，使下游决策者无需维护状态，只需在每轮解决由预测定义的单轮约束优化问题

Result: 获得了所有智能体同时的动态遗憾保证，其中每个智能体的遗憾是相对于交互轮次中可能变化的行动序列来衡量的

Conclusion: 该算法在在线全预测框架下首次实现了所有智能体的动态遗憾保证和约束违反趋近于零

Abstract: We present an algorithm guaranteeing dynamic regret bounds for online
omniprediction with long term constraints. The goal in this recently introduced
problem is for a learner to generate a sequence of predictions which are
broadcast to a collection of downstream decision makers. Each decision maker
has their own utility function, as well as a vector of constraint functions,
each mapping their actions and an adversarially selected state to reward or
constraint violation terms. The downstream decision makers select actions "as
if" the state predictions are correct, and the goal of the learner is to
produce predictions such that all downstream decision makers choose actions
that give them worst-case utility guarantees while minimizing worst-case
constraint violation. Within this framework, we give the first algorithm that
obtains simultaneous \emph{dynamic regret} guarantees for all of the agents --
where regret for each agent is measured against a potentially changing sequence
of actions across rounds of interaction, while also ensuring vanishing
constraint violation for each agent. Our results do not require the agents
themselves to maintain any state -- they only solve one-round constrained
optimization problems defined by the prediction made at that round.

</details>


### [122] [GTCN-G: A Residual Graph-Temporal Fusion Network for Imbalanced Intrusion Detection (Preprint)](https://arxiv.org/abs/2510.07285)
*Tianxiang Xu,Zhichao Wen,Xinyu Zhao,Qi Hu,Yan Li,Chang Liu*

Main category: cs.LG

TL;DR: 提出GTCN-G框架，结合时序卷积网络和图神经网络，通过残差学习和图注意力机制解决网络入侵检测中的数据不平衡问题，在多个数据集上达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 网络威胁日益复杂，流量数据存在类别不平衡问题，现有方法难以同时处理拓扑结构和时序依赖关系，且缺乏对数据不平衡的专门处理。

Method: 融合门控时序卷积网络(G-TCN)提取时序特征和图卷积网络(GCN)学习图结构，通过图注意力网络(GAT)实现残差学习机制，保留原始特征信息以缓解类别不平衡。

Result: 在UNSW-NB15和ToN-IoT数据集上的实验表明，GTCN-G在二元和多类分类任务中均显著优于现有基线模型，达到最先进的性能。

Conclusion: GTCN-G框架有效整合了时序和图结构信息，通过残差学习机制成功解决了数据不平衡问题，为网络入侵检测提供了更优的解决方案。

Abstract: The escalating complexity of network threats and the inherent class imbalance
in traffic data present formidable challenges for modern Intrusion Detection
Systems (IDS). While Graph Neural Networks (GNNs) excel in modeling topological
structures and Temporal Convolutional Networks (TCNs) are proficient in
capturing time-series dependencies, a framework that synergistically integrates
both while explicitly addressing data imbalance remains an open challenge. This
paper introduces a novel deep learning framework, named Gated Temporal
Convolutional Network and Graph (GTCN-G), engineered to overcome these
limitations. Our model uniquely fuses a Gated TCN (G-TCN) for extracting
hierarchical temporal features from network flows with a Graph Convolutional
Network (GCN) designed to learn from the underlying graph structure. The core
innovation lies in the integration of a residual learning mechanism,
implemented via a Graph Attention Network (GAT). This mechanism preserves
original feature information through residual connections, which is critical
for mitigating the class imbalance problem and enhancing detection sensitivity
for rare malicious activities (minority classes). We conducted extensive
experiments on two public benchmark datasets, UNSW-NB15 and ToN-IoT, to
validate our approach. The empirical results demonstrate that the proposed
GTCN-G model achieves state-of-the-art performance, significantly outperforming
existing baseline models in both binary and multi-class classification tasks.

</details>


### [123] [Evolutionary Profiles for Protein Fitness Prediction](https://arxiv.org/abs/2510.07286)
*Jigang Fan,Xiaoran Jiao,Shengdong Lin,Zhanming Liang,Weian Mao,Chenchen Jing,Hao Chen,Chunhua Shen*

Main category: cs.LG

TL;DR: 提出了EvoIF模型，通过整合家族内同源序列信息和跨家族结构进化约束，实现了高效的蛋白质适应性预测，在ProteinGym基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 蛋白质适应性预测对蛋白质工程至关重要，但受限于有限的实验数据与巨大序列空间之间的矛盾。现有蛋白质语言模型在零样本适应性预测方面表现良好，需要从进化角度提供统一的理论解释。

Method: 将自然进化视为隐式奖励最大化，将掩码语言建模视为逆强化学习。提出EvoIF模型，整合家族内同源序列谱和跨家族结构进化约束，通过紧凑的转换块融合序列-结构表示。

Result: 在ProteinGym基准测试（217个突变实验，>250万突变体）中，EvoIF及其MSA增强变体达到最先进或竞争性性能，仅使用0.15%的训练数据和更少的参数。消融实验证实家族内和跨家族谱是互补的。

Conclusion: EvoIF提供了一个轻量级但高效的解决方案，通过整合互补的进化信号显著提升了蛋白质适应性预测的鲁棒性和准确性。

Abstract: Predicting the fitness impact of mutations is central to protein engineering
but constrained by limited assays relative to the size of sequence space.
Protein language models (pLMs) trained with masked language modeling (MLM)
exhibit strong zero-shot fitness prediction; we provide a unifying view by
interpreting natural evolution as implicit reward maximization and MLM as
inverse reinforcement learning (IRL), in which extant sequences act as expert
demonstrations and pLM log-odds serve as fitness estimates. Building on this
perspective, we introduce EvoIF, a lightweight model that integrates two
complementary sources of evolutionary signal: (i) within-family profiles from
retrieved homologs and (ii) cross-family structural-evolutionary constraints
distilled from inverse folding logits. EvoIF fuses sequence-structure
representations with these profiles via a compact transition block, yielding
calibrated probabilities for log-odds scoring. On ProteinGym (217 mutational
assays; >2.5M mutants), EvoIF and its MSA-enabled variant achieve
state-of-the-art or competitive performance while using only 0.15% of the
training data and fewer parameters than recent large models. Ablations confirm
that within-family and cross-family profiles are complementary, improving
robustness across function types, MSA depths, taxa, and mutation depths. The
codes will be made publicly available at https://github.com/aim-uofa/EvoIF.

</details>


### [124] [MolGA: Molecular Graph Adaptation with Pre-trained 2D Graph Encoder](https://arxiv.org/abs/2510.07289)
*Xingtong Yu,Chang Zhou,Xinming Zhang,Yuan Fang*

Main category: cs.LG

TL;DR: MolGA是一种通过灵活整合分子领域知识来适应预训练2D图编码器到下游分子应用的方法，包括分子对齐策略和条件适应机制。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练2D图编码器忽略了与亚分子实例相关的丰富分子领域知识，而专门的分子预训练方法缺乏整合多样化知识的灵活性。

Method: 提出分子对齐策略弥合预训练拓扑表示与领域知识表示之间的差距，并引入条件适应机制生成实例特定令牌以实现细粒度知识整合。

Result: 在11个公共数据集上的广泛实验证明了MolGA的有效性。

Conclusion: MolGA提供了一种实用的替代方案，可以在下游适应过程中重用预训练的2D编码器并整合分子领域知识。

Abstract: Molecular graph representation learning is widely used in chemical and
biomedical research. While pre-trained 2D graph encoders have demonstrated
strong performance, they overlook the rich molecular domain knowledge
associated with submolecular instances (atoms and bonds). While molecular
pre-training approaches incorporate such knowledge into their pre-training
objectives, they typically employ designs tailored to a specific type of
knowledge, lacking the flexibility to integrate diverse knowledge present in
molecules. Hence, reusing widely available and well-validated pre-trained 2D
encoders, while incorporating molecular domain knowledge during downstream
adaptation, offers a more practical alternative. In this work, we propose
MolGA, which adapts pre-trained 2D graph encoders to downstream molecular
applications by flexibly incorporating diverse molecular domain knowledge.
First, we propose a molecular alignment strategy that bridge the gap between
pre-trained topological representations with domain-knowledge representations.
Second, we introduce a conditional adaptation mechanism that generates
instance-specific tokens to enable fine-grained integration of molecular domain
knowledge for downstream tasks. Finally, we conduct extensive experiments on
eleven public datasets, demonstrating the effectiveness of MolGA.

</details>


### [125] [MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline](https://arxiv.org/abs/2510.07307)
*Rushi Qiang,Yuchen Zhuang,Anikait Singh,Percy Liang,Chao Zhang,Sherry Yang,Bo Dai*

Main category: cs.LG

TL;DR: MLE-Smith是一个全自动多智能体流水线，通过生成-验证-执行范式将原始数据集转化为竞赛式机器学习工程挑战，解决了现有MLE基准测试可扩展性差和数据获取困难的问题。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习工程训练数据获取困难，现有基准测试依赖静态手动策划任务，可扩展性差且适用性有限，需要大量时间和人工投入。

Method: 采用多智能体流水线驱动结构化任务设计和标准化重构，结合混合验证机制强制执行严格的结构规则和高级语义合理性，通过交互式执行验证经验可解性和真实世界保真度。

Result: 在224个真实世界数据集上生成606个任务，涵盖多类别、目标和模态，主流LLM在MLE-Smith任务上的表现与人工设计任务高度相关。

Conclusion: MLE-Smith能够有效扩展MLE任务规模，同时保持任务质量，为机器学习工程提供了可扩展的高质量基准测试生成方案。

Abstract: While Language Models (LMs) have made significant progress in automating
machine learning engineering (MLE), the acquisition of high-quality MLE
training data is significantly constrained. Current MLE benchmarks suffer from
low scalability and limited applicability because they rely on static, manually
curated tasks, demanding extensive time and manual effort to produce. We
introduce MLE-Smith, a fully automated multi-agent pipeline, to transform raw
datasets into competition-style MLE challenges through an efficient
generate-verify-execute paradigm for scaling MLE tasks with verifiable quality,
real-world usability, and rich diversity. The proposed multi-agent pipeline in
MLE-Smith drives structured task design and standardized refactoring, coupled
with a hybrid verification mechanism that enforces strict structural rules and
high-level semantic soundness. It further validates empirical solvability and
real-world fidelity through interactive execution. We apply MLE-Smith to 224 of
real-world datasets and generate 606 tasks spanning multiple categories,
objectives, and modalities, demonstrating that MLE-Smith can work effectively
across a wide range of real-world datasets. Evaluation on the generated tasks
shows that the performance of eight mainstream and cutting-edge LLMs on
MLE-Smith tasks is strongly correlated with their performance on carefully
human-designed tasks, highlighting the effectiveness of the MLE-Smith to
scaling up MLE tasks, while maintaining task quality.

</details>


### [126] [h1: Bootstrapping LLMs to Reason over Longer Horizons via Reinforcement Learning](https://arxiv.org/abs/2510.07312)
*Sumeet Ramesh Motwani,Alesia Ivanova,Ziyang Cai,Philip Torr,Riashat Islam,Shital Shah,Christian Schroeder de Witt,Charles London*

Main category: cs.LG

TL;DR: 提出一种可扩展的方法，通过合成简单问题为复杂多步依赖链来提升大语言模型的长程推理能力，仅使用现有的短程数据，无需推理时支架或昂贵的步骤级监督。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在短程推理任务上表现出色，但随着推理长度增加性能下降。现有方法依赖推理时支架或昂贵的步骤级监督，难以扩展。

Method: 通过合成方法将简单问题组合成任意长度的复杂多步依赖链，使用仅结果奖励在自动增加复杂度的课程下进行强化学习训练。

Result: 在6年级数学问题上进行课程训练，可将竞赛级基准（GSM-Symbolic、MATH-500、AIME）的准确率提升高达2.06倍，且在高pass@k下长程改进显著高于基线。

Conclusion: 该方法为仅使用现有数据扩展强化学习以解决长程问题提供了高效路径，理论上证明课程强化学习相比全长度训练在样本复杂度上实现指数级改进。

Abstract: Large language models excel at short-horizon reasoning tasks, but performance
drops as reasoning horizon lengths increase. Existing approaches to combat this
rely on inference-time scaffolding or costly step-level supervision, neither of
which scales easily. In this work, we introduce a scalable method to bootstrap
long-horizon reasoning capabilities using only existing, abundant short-horizon
data. Our approach synthetically composes simple problems into complex,
multi-step dependency chains of arbitrary length. We train models on this data
using outcome-only rewards under a curriculum that automatically increases in
complexity, allowing RL training to be scaled much further without saturating.
Empirically, our method generalizes remarkably well: curriculum training on
composed 6th-grade level math problems (GSM8K) boosts accuracy on longer,
competition-level benchmarks (GSM-Symbolic, MATH-500, AIME) by up to 2.06x.
Importantly, our long-horizon improvements are significantly higher than
baselines even at high pass@k, showing that models can learn new reasoning
paths under RL. Theoretically, we show that curriculum RL with outcome rewards
achieves an exponential improvement in sample complexity over full-horizon
training, providing training signal comparable to dense supervision. h1
therefore introduces an efficient path towards scaling RL for long-horizon
problems using only existing data.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [127] [A Mixed-Methods Analysis of Repression and Mobilization in Bangladesh's July Revolution Using Machine Learning and Statistical Modeling](https://arxiv.org/abs/2510.06264)
*Md. Saiful Bari Siddiqui,Anupam Debashis Roy*

Main category: stat.AP

TL;DR: 该研究分析了2024年孟加拉国七月革命中，国家暴力如何反而推动了学生领导的平民起义成功，揭示了镇压与动员之间的复杂非线性关系。


<details>
  <summary>Details</summary>
Motivation: 研究七月革命这一公民抵抗的里程碑事件，探讨国家暴力本意是镇压异议，却最终推动运动胜利的核心悖论。

Method: 采用混合方法：首先建立冲突时间线的定性叙述以生成可检验假设，然后使用事件级数据集进行多方法定量分析，包括双向固定效应面板模型、向量自回归分析和机器学习方法。

Result: 研究发现存在直接的局部镇压反效果，致命暴力增加会立即引发全国性动员，这种效应是非线性的，由7月16日第一波致命暴力及其视觉传播的道德冲击触发。机器学习分析确认"对抗议者过度使用武力"是全国性升级的最主要预测因子。

Conclusion: 七月革命是由偶然的、非线性的反效果驱动的，由特定的催化性道德冲击触发，并通过国家暴行视觉奇观的病毒式传播而加速。

Abstract: The 2024 July Revolution in Bangladesh represents a landmark event in the
study of civil resistance. This study investigates the central paradox of the
success of this student-led civilian uprising: how state violence, intended to
quell dissent, ultimately fueled the movement's victory. We employ a
mixed-methods approach. First, we develop a qualitative narrative of the
conflict's timeline to generate specific, testable hypotheses. Then, using a
disaggregated, event-level dataset, we employ a multi-method quantitative
analysis to dissect the complex relationship between repression and
mobilisation. We provide a framework to analyse explosive modern uprisings like
the July Revolution. Initial pooled regression models highlight the crucial
role of protest momentum in sustaining the movement. To isolate causal effects,
we specify a Two-Way Fixed Effects panel model, which provides robust evidence
for a direct and statistically significant local suppression backfire effect.
Our Vector Autoregression (VAR) analysis provides clear visual evidence of an
immediate, nationwide mobilisation in response to increased lethal violence. We
further demonstrate that this effect was non-linear. A structural break
analysis reveals that the backfire dynamic was statistically insignificant in
the conflict's early phase but was triggered by the catalytic moral shock of
the first wave of lethal violence, and its visuals circulated around July 16th.
A complementary machine learning analysis (XGBoost, out-of-sample R$^{2}$=0.65)
corroborates this from a predictive standpoint, identifying "excessive force
against protesters" as the single most dominant predictor of nationwide
escalation. We conclude that the July Revolution was driven by a contingent,
non-linear backfire, triggered by specific catalytic moral shocks and
accelerated by the viral reaction to the visual spectacle of state brutality.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [128] [Early Results from Teaching Modelling for Software Comprehension in New-Hire Onboarding](https://arxiv.org/abs/2510.07010)
*Mrityunjay Kumar,Venkatesh Choppella*

Main category: cs.CY

TL;DR: 在SaaS公司入职培训中引入系统思维和LTS建模的五次课程干预，结果显示对基础较差的新员工理解力提升显著，但对基础较好的员工效果有限。


<details>
  <summary>Details</summary>
Motivation: 大多数毕业生缺乏理解大型软件系统的能力，需要为行业新人提供更好的准备。

Method: 在入职培训中集成五次会议课程，教授系统思维和标记转换系统建模，使用结构化模板表达产品行为理解，并进行前后测试评估。

Result: 35名新员工中31人提供配对数据，整体提升较小且不显著，但预测试低于中位数的参与者平均提高15个百分点（显著），而高于中位数的参与者略有退步（不显著）。

Conclusion: 短期建模导向的入职干预能加速基础较差新员工的理解力发展，但需要为能力较强的参与者提供差异化路径，此类低成本干预可作为现有入职培训的补充。

Abstract: Working effectively with large, existing software systems requires strong
comprehension skills, yet most graduates enter the industry with little
preparation for this challenge. We report early results from a pilot
intervention integrated into a SaaS company's onboarding program: a
five-session course introducing systems thinking and Labelled Transition System
(LTS) modelling. Participants articulated their understanding of product
behaviour using a structured template and completed matched pre- and
post-assessments. Of 35 new hires, 31 provided paired records for analysis.
Across the full cohort, gains were small and not statistically significant.
However, participants below the median on the pre-test improved by 15
percentage points on average (statistically significant), while those above the
median regressed slightly (not statistically significant). Course feedback
indicated high engagement and perceived applicability. These results suggest
that short, modelling-focused onboarding interventions can accelerate
comprehension for less-prepared new hires. At the same time, they point to the
need for differentiated pathways for stronger participants, and to the
potential for companies to adopt such interventions at scale as a low-cost
complement to existing onboarding.

</details>


### [129] [Beyond Static Knowledge Messengers: Towards Adaptive, Fair, and Scalable Federated Learning for Medical AI](https://arxiv.org/abs/2510.06259)
*Jahidul Arafat,Fariha Tasmin,Sanjaya Poudel,Ahsan Habib Tareq,Iftekhar Haider*

Main category: cs.CY

TL;DR: 提出了自适应公平联邦学习(AFFL)框架，通过动态知识信使、公平感知蒸馏和课程引导加速三大创新，解决医疗AI在隐私保护协作学习中的公平性和效率问题。


<details>
  <summary>Details</summary>
Motivation: 医疗AI在隐私保护协作学习中面临异构医疗机构间的公平性挑战，现有联邦学习方法存在架构静态、收敛慢、公平性差距和可扩展性限制等问题。

Method: 采用自适应知识信使根据异构性和任务复杂度动态调整容量，使用影响权重聚合的公平感知蒸馏，以及课程引导加速减少60-70%的训练轮次。

Result: 理论分析提供收敛保证和epsilon公平界限，预计实现55-75%通信减少、56-68%公平性提升、34-46%能耗节省，支持100+机构规模。

Conclusion: 该框架支持多模态数据集成并符合HIPAA/GDPR合规要求，提出了MedFedBench基准套件和7个研究问题，旨在民主化医疗AI。

Abstract: Medical AI faces challenges in privacy-preserving collaborative learning
while ensuring fairness across heterogeneous healthcare institutions. Current
federated learning approaches suffer from static architectures, slow
convergence (45-73 rounds), fairness gaps marginalizing smaller institutions,
and scalability constraints (15-client limit). We propose Adaptive Fair
Federated Learning (AFFL) through three innovations: (1) Adaptive Knowledge
Messengers dynamically scaling capacity based on heterogeneity and task
complexity, (2) Fairness-Aware Distillation using influence-weighted
aggregation, and (3) Curriculum-Guided Acceleration reducing rounds by 60-70%.
Our theoretical analysis provides convergence guarantees with epsilon-fairness
bounds, achieving O(T^{-1/2}) + O(H_max/T^{3/4}) rates. Projected results show
55-75% communication reduction, 56-68% fairness improvement, 34-46% energy
savings, and 100+ institution support. The framework enables multi-modal
integration across imaging, genomics, EHR, and sensor data while maintaining
HIPAA/GDPR compliance. We propose MedFedBench benchmark suite for standardized
evaluation across six healthcare dimensions: convergence efficiency,
institutional fairness, privacy preservation, multi-modal integration,
scalability, and clinical deployment readiness. Economic projections indicate
400-800% ROI for rural hospitals and 15-25% performance gains for academic
centers. This work presents a seven-question research agenda, 24-month
implementation roadmap, and pathways toward democratizing healthcare AI.

</details>


### [130] [Asking For It: Question-Answering for Predicting Rule Infractions in Online Content Moderation](https://arxiv.org/abs/2510.06350)
*Mattia Samory,Diana Pamfile,Andrew To,Shruti Phadke*

Main category: cs.CY

TL;DR: 提出了ModQ框架，这是一个基于问答的内容审核方法，能够根据社区规则识别评论违规情况，优于现有方法且具有良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在线社区的规则差异大、变化快且执行不一致，给透明度、治理和自动化带来挑战。

Method: 开发了两种模型变体：抽取式问答和多项选择问答，基于Reddit和Lemmy的大规模数据集训练，在推理时考虑完整的社区规则集。

Result: 两种模型在识别与审核相关的规则违规方面均优于最先进的基线方法，同时保持轻量级和可解释性。

Conclusion: ModQ模型能有效泛化到未见过的社区和规则，支持低资源审核设置和动态治理环境。

Abstract: Online communities rely on a mix of platform policies and community-authored
rules to define acceptable behavior and maintain order. However, these rules
vary widely across communities, evolve over time, and are enforced
inconsistently, posing challenges for transparency, governance, and automation.
In this paper, we model the relationship between rules and their enforcement at
scale, introducing ModQ, a novel question-answering framework for
rule-sensitive content moderation. Unlike prior classification or
generation-based approaches, ModQ conditions on the full set of community rules
at inference time and identifies which rule best applies to a given comment. We
implement two model variants - extractive and multiple-choice QA - and train
them on large-scale datasets from Reddit and Lemmy, the latter of which we
construct from publicly available moderation logs and rule descriptions. Both
models outperform state-of-the-art baselines in identifying moderation-relevant
rule violations, while remaining lightweight and interpretable. Notably, ModQ
models generalize effectively to unseen communities and rules, supporting
low-resource moderation settings and dynamic governance environments.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [131] [Quantum matrix arithmetics with Hamiltonian evolution](https://arxiv.org/abs/2510.06316)
*Christopher Kang,Yuan Su*

Main category: quant-ph

TL;DR: 本文开发了一套基于哈密顿量演化实现矩阵运算的方法，通过哈密顿量块编码将结果存储在非对角块中，仅需≤2个辅助量子比特即可实现矩阵乘法、加法、求逆、共轭等操作，并应用于量子化学模拟。


<details>
  <summary>Details</summary>
Motivation: 矩阵算术运算的高效实现是许多量子算法加速的关键。传统方法需要较多辅助量子比特，本文旨在开发一种仅需少量辅助量子比特的矩阵运算框架。

Method: 使用哈密顿量演化输入算子，将矩阵运算结果编码在哈密顿量的非对角块中。采用李群交换子乘积公式及其高阶推广实现哈密顿量矩阵乘法，使用支配多项式逼近实现奇异值变换。

Result: 成功实现了矩阵乘法、加法、求逆、共轭、分数缩放、整数缩放、复数相位缩放以及奇偶多项式的奇异值变换，仅需≤2个辅助量子比特。在量子化学模拟中，对双因子张量超压缩哈密顿量获得进一步改进。

Conclusion: 哈密顿量块编码方法为量子算法中的矩阵运算提供了一种高效的实现框架，显著减少了所需的辅助量子比特数量，在量子化学模拟等应用中具有重要价值。

Abstract: The efficient implementation of matrix arithmetic operations underpins the
speedups of many quantum algorithms. We develop a suite of methods to perform
matrix arithmetics -- with the result encoded in the off-diagonal blocks of a
Hamiltonian -- using Hamiltonian evolutions of input operators. We show how to
maintain this $\textit{Hamiltonian block encoding}$, so that matrix operations
can be composed one after another, and the entire quantum computation takes
$\leq 2$ ancilla qubits. We achieve this for matrix multiplication, matrix
addition, matrix inversion, Hermitian conjugation, fractional scaling, integer
scaling, complex phase scaling, as well as singular value transformation for
both odd and even polynomials. We also present an overlap estimation algorithm
to extract classical properties of Hamiltonian block encoded operators,
analogous to the well known Hadmard test, at no extra cost of qubit. Our
Hamiltonian matrix multiplication uses the Lie group commutator product formula
and its higher-order generalizations due to Childs and Wiebe. Our Hamiltonian
singular value transformation employs a dominated polynomial approximation,
where the approximation holds within the domain of interest, while the
constructed polynomial is upper bounded by the target function over the entire
unit interval. We describe a circuit for simulating a class of sum-of-squares
Hamiltonians, attaining a commutator scaling in step count, while leveraging
the power of matrix arithmetics to reduce the cost of each simulation step. In
particular, we apply this to the doubly factorized tensor hypercontracted
Hamiltonians from recent studies of quantum chemistry, obtaining further
improvements for initial states with a fixed number of particles. We achieve
this with $1$ ancilla qubit.

</details>


### [132] [Breaking the Treewidth Barrier in Quantum Circuit Simulation with Decision Diagrams](https://arxiv.org/abs/2510.06775)
*Bin Cheng,Ziyuan Wang,Ruixuan Deng,Jianxin Chen,Zhengfeng Ji*

Main category: quant-ph

TL;DR: FeynmanDD是一种基于决策图的量子电路模拟方法，其性能由电路图的线性秩宽度决定，相比基于张量网络的方法在树宽度较大的电路上具有优势。


<details>
  <summary>Details</summary>
Motivation: 现有基于张量网络的量子电路模拟方法受限于电路图的树宽度，对于树宽度较大的电路计算成本过高，需要探索更高效的模拟方法。

Method: 使用多终端决策图进行量子电路模拟，分析该方法与电路图线性秩宽度的关系，并利用Solovay-Kitaev算法将任意单量子比特门扩展为Hadamard和T门序列。

Result: 证明FeynmanDD方法使用的决策图大小与电路图的线性秩宽度呈指数关系，且线性秩宽度通常远小于树宽度，最多比树宽度大一个对数因子。

Conclusion: FeynmanDD在特定电路家族中优于所有基于张量网络的方法，且通过门扩展技术消除了对门集的限制，具有更广泛的应用前景。

Abstract: Classical simulation of quantum circuits is a critical tool for validating
quantum hardware and probing the boundary between classical and quantum
computational power. Existing state-of-the-art methods, notably tensor network
approaches, have computational costs governed by the treewidth of the
underlying circuit graph, making circuits with large treewidth intractable.
This work rigorously analyzes FeynmanDD, a decision diagram-based simulation
method proposed in CAV 2025 by a subset of the authors, and shows that the size
of the multi-terminal decision diagram used in FeynmanDD is exponential in the
linear rank-width of the circuit graph. As linear rank-width can be
substantially smaller than treewidth and is at most larger than the treewidth
by a logarithmic factor, our analysis demonstrates that FeynmanDD outperforms
all tensor network-based methods for certain circuit families. We also show
that the method remains efficient if we use the Solovay-Kitaev algorithm to
expand arbitrary single-qubit gates to sequences of Hadamard and T gates,
essentially removing the gate-set restriction posed by the method.

</details>


### [133] [Reconquering Bell sampling on qudits: stabilizer learning and testing, quantum pseudorandomness bounds, and more](https://arxiv.org/abs/2510.06848)
*Jonathan Allcock,Joao F. Doriguello,Gábor Ivanyos,Miklos Santha*

Main category: quant-ph

TL;DR: 本文克服了先前工作的困难，开发了一种对所有d≥2的qudit都有用的Bell采样推广方法，基于拉格朗日四平方定理构建了新酉算子，并将多个量子比特结果推广到任意维度的qudit。


<details>
  <summary>Details</summary>
Motivation: Bell采样是量子态分析的重要工具，但之前的工作表明其自然扩展到任意维度时无法提供有意义的信息。本文旨在克服这一限制，为所有d≥2的qudit开发有用的Bell采样推广方法。

Method: 基于拉格朗日四平方定理构建新酉算子，将任意稳定子态的四个副本映射到其复共轭的四个副本，从而实现对qudit的有用Bell采样推广。

Result: 成功将多个量子比特结果推广到任意维度的qudit：1）O(n³)时间内用O(n)样本学习稳定子态；2）解决隐藏稳定子群问题；3）测试稳定子大小；4）证明最多n/2个单qudit非Clifford门的Clifford电路无法准备伪随机态；5）测试稳定子保真度。

Conclusion: 本文成功开发了对所有d≥2的qudit都有用的Bell采样推广方法，解决了先前工作中的困难，并将多个重要的量子比特结果推广到了任意维度的qudit系统。

Abstract: Bell sampling is a simple yet powerful tool based on measuring two copies of
a quantum state in the Bell basis, and has found applications in a plethora of
problems related to stabiliser states and measures of magic. However, it was
not known how to generalise the procedure from qubits to $d$-level systems --
qudits -- for all dimensions $d > 2$ in a useful way. Indeed, a prior work of
the authors (arXiv'24) showed that the natural extension of Bell sampling to
arbitrary dimensions fails to provide meaningful information about the quantum
states being measured. In this paper, we overcome the difficulties encountered
in previous works and develop a useful generalisation of Bell sampling to
qudits of all $d\geq 2$. At the heart of our primitive is a new unitary, based
on Lagrange's four-square theorem, that maps four copies of any stabiliser
state $|\mathcal{S}\rangle$ to four copies of its complex conjugate
$|\mathcal{S}^\ast\rangle$ (up to some Pauli operator), which may be of
independent interest. We then demonstrate the utility of our new Bell sampling
technique by lifting several known results from qubits to qudits for any $d\geq
2$:
  1. Learning stabiliser states in $O(n^3)$ time with $O(n)$ samples;
  2. Solving the Hidden Stabiliser Group Problem in
$\tilde{O}(n^3/\varepsilon)$ time with $\tilde{O}(n/\varepsilon)$ samples;
  3. Testing whether $|\psi\rangle$ has stabiliser size at least $d^t$ or is
$\varepsilon$-far from all such states in $\tilde{O}(n^3/\varepsilon)$ time
with $\tilde{O}(n/\varepsilon)$ samples;
  4. Clifford circuits with at most $n/2$ single-qudit non-Clifford gates
cannot prepare pseudorandom states;
  5. Testing whether $|\psi\rangle$ has stabiliser fidelity at least
$1-\varepsilon_1$ or at most $1-\varepsilon_2$ with $O(d^2/\varepsilon_2)$
samples if $\varepsilon_1 = 0$ or $O(d^2/\varepsilon_2^2)$ samples if
$\varepsilon_1 = O(d^{-2})$.

</details>


### [134] [Advantages of Global Entanglement-Distillation Policies in Quantum Repeater Chains](https://arxiv.org/abs/2510.06737)
*Iftach Yakar,Michael Ben-Or*

Main category: quant-ph

TL;DR: 全局确定性策略在量子中继器中的纠缠蒸馏决策上优于局部策略，能显著提升通信速率，特别是在长距离网络中。


<details>
  <summary>Details</summary>
Motivation: 研究全局确定性策略是否比局部策略在量子中继器的纠缠蒸馏决策中表现更好，以提升量子通信速率。

Method: 模拟等距中继器链，使用双向经典通信，比较局部和全局策略的蒸馏决策，涵盖大距离和不同网络硬件参数。

Result: 全局策略始终优于局部策略，在某些情况下决定秘密通信是否可能。对于大型中继器链（N>512），全局策略将SKR提升两个数量级。

Conclusion: 局部蒸馏决策在量子中继器链中可能不是最优的，这些结果可为未来协议设计提供参考。

Abstract: Quantum repeaters are essential for achieving long-distance quantum
communication due to photon loss, which grows exponentially with the channel
distance. Current quantum repeater generations use entanglement distillation
protocols, where the decision of when to perform distillation depends on either
local or global knowledge. Recent approaches for quantum repeaters, such as
Mantri et al. (arXiv:2409.06152), consider using deterministic local decision
policies for entanglement distillation. We ask whether global deterministic
policies outperform local ones in terms of communication rate. We simulate
equidistant repeater chains, assisted by two-way classical communication, and
compare local and global policies for distillation decisions, spanning large
distances and varying network and hardware parameters. Our findings show that
global deterministic policies consistently outperform these local ones, and in
some cases, determine whether secret communication is possible. For large
repeater chains ($N>512$), global policies improve SKR by two orders of
magnitude. These results suggest that local distillation decisions in quantum
repeater chains may not be optimal, and may inform future protocol design.

</details>


### [135] [Randomized Quantum Singular Value Transformation](https://arxiv.org/abs/2510.06851)
*Xinzhao Wang,Yuxin Zhang,Soumyabrata Hazra,Tongyang Li,Changpeng Shao,Shantanav Chakraborty*

Main category: quant-ph

TL;DR: 提出了首个量子奇异值变换的随机化算法，避免使用块编码，仅需单个辅助量子比特，实现与哈密顿量项数无关的门复杂度。


<details>
  <summary>Details</summary>
Motivation: 标准QSVT实现依赖昂贵的块编码构造，需要对数数量的辅助量子比特、复杂的多量子比特控制和与哈密顿量项数成正比的电路深度。

Method: 开发了两种方法：(i) QSVT的直接随机化，用重要性采样替代块编码；(ii) 将qDRIFT集成到广义量子信号处理框架中，通过经典外推指数级提高精度依赖。

Result: 两种算法都实现了与哈密顿量项数无关的门复杂度，仅对目标多项式次数有二次依赖。在自然参数范围内优于标准QSVT，在早期容错量子设备上具有优势。

Conclusion: 随机化QSVT成为早期容错量子设备的实用且资源高效的替代方案，在量子线性系统求解和哈密顿量基态性质估计等任务中优于先前随机化算法数个数量级。

Abstract: We introduce the first randomized algorithms for Quantum Singular Value
Transformation (QSVT), a unifying framework for many quantum algorithms.
Standard implementations of QSVT rely on block encodings of the Hamiltonian,
which are costly to construct, requiring a logarithmic number of ancilla
qubits, intricate multi-qubit control, and circuit depth scaling linearly with
the number of Hamiltonian terms. In contrast, our algorithms use only a single
ancilla qubit and entirely avoid block encodings. We develop two methods: (i) a
direct randomization of QSVT, where block encodings are replaced by importance
sampling, and (ii) an approach that integrates qDRIFT into the generalized
quantum signal processing framework, with the dependence on precision
exponentially improved through classical extrapolation. Both algorithms achieve
gate complexity independent of the number of Hamiltonian terms, a hallmark of
randomized methods, while incurring only quadratic dependence on the degree of
the target polynomial. We identify natural parameter regimes where our methods
outperform even standard QSVT, making them promising for early fault-tolerant
quantum devices. We also establish a fundamental lower bound showing that the
quadratic dependence on the polynomial degree is optimal within this framework.
We apply our framework to two fundamental tasks: solving quantum linear systems
and estimating ground-state properties of Hamiltonians, obtaining polynomial
advantages over prior randomized algorithms. Finally, we benchmark our
ground-state property estimation algorithm on electronic structure Hamiltonians
and the transverse-field Ising model with long-range interactions. In both
cases, our approach outperforms prior work by several orders of magnitude in
circuit depth, establishing randomized QSVT as a practical and
resource-efficient alternative for early fault-tolerant quantum devices.

</details>


### [136] [Layerwise Federated Learning for Heterogeneous Quantum Clients using Quorus](https://arxiv.org/abs/2510.06228)
*Jason Han,Nicholas S. DiBrita,Daniel Leeds,Jianqiang Li,Jason Ludmir,Tirthak Patel*

Main category: quant-ph

TL;DR: 提出了Quorus量子联邦学习框架，通过分层损失函数解决异构量子设备间的分布式训练问题，允许客户端根据自身计算能力选择不同深度的量子模型。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习面临关键数据分散在私有客户端的问题，需要分布式QML解决方案。不同客户端的量子计算机存在错误且具有异构错误特性，需要运行不同深度的量子电路。

Method: 使用分层损失函数有效训练不同深度的量子模型，允许客户端根据自身容量选择高保真输出的模型。提供了基于客户端需求的多种模型设计，优化了shot预算、量子比特数、中间电路测量和优化空间。

Result: 模拟和真实硬件实验表明，Quorus增加了高深度客户端的梯度幅度，测试准确率比现有最优方法平均提高12.4%。

Conclusion: Quorus展示了在异构量子设备环境下实现有效联邦学习的潜力，通过分层损失函数和灵活模型设计解决了量子计算资源差异带来的挑战。

Abstract: Quantum machine learning (QML) holds the promise to solve classically
intractable problems, but, as critical data can be fragmented across private
clients, there is a need for distributed QML in a quantum federated learning
(QFL) format. However, the quantum computers that different clients have access
to can be error-prone and have heterogeneous error properties, requiring them
to run circuits of different depths. We propose a novel solution to this QFL
problem, Quorus, that utilizes a layerwise loss function for effective training
of varying-depth quantum models, which allows clients to choose models for
high-fidelity output based on their individual capacity. Quorus also presents
various model designs based on client needs that optimize for shot budget,
qubit count, midcircuit measurement, and optimization space. Our simulation and
real-hardware results show the promise of Quorus: it increases the magnitude of
gradients of higher depth clients and improves testing accuracy by 12.4% on
average over the state-of-the-art.

</details>


### [137] [Quantum Sparse Recovery and Quantum Orthogonal Matching Pursuit](https://arxiv.org/abs/2510.06925)
*Armando Bellante,Stefano Vanerio,Stefano Zanero*

Main category: quant-ph

TL;DR: 提出了量子正交匹配追踪(QOMP)算法，用于在非正交过完备字典中进行量子稀疏恢复，证明了在标准假设下可多项式时间内精确恢复稀疏状态的支持集，并应用于稀疏量子层析，在有利条件下达到O(√N/ε)查询复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究量子稀疏恢复在非正交过完备字典中的问题，旨在用尽可能少的向量重构状态，克服一般恢复问题的NP-hard性限制。

Method: 引入量子正交匹配追踪(QOMP)算法，结合内积估计、最大值查找和块编码投影等量子子程序，采用误差重置设计避免迭代误差累积。

Result: 在标准互不相干性和良好条件稀疏性假设下，QOMP能在多项式时间内精确恢复K-稀疏状态的支持集；在QRAM模型中实现相对于经典OMP的多项式加速；提供估计字典互不相干性的量子算法，查询复杂度为O(m/ε)。

Conclusion: QOMP是第一个量子版本的OMP贪婪算法，成功克服了量子稀疏恢复的NP-hard限制，在稀疏量子层析中突破了密集正交字典设置的下界，为非正交字典中的量子稀疏问题提供了有效解决方案。

Abstract: We study quantum sparse recovery in non-orthogonal, overcomplete
dictionaries: given coherent quantum access to a state and a dictionary of
vectors, the goal is to reconstruct the state up to $\ell_2$ error using as few
vectors as possible. We first show that the general recovery problem is
NP-hard, ruling out efficient exact algorithms in full generality. To overcome
this, we introduce Quantum Orthogonal Matching Pursuit (QOMP), the first
quantum analogue of the classical OMP greedy algorithm. QOMP combines quantum
subroutines for inner product estimation, maximum finding, and block-encoded
projections with an error-resetting design that avoids iteration-to-iteration
error accumulation. Under standard mutual incoherence and well-conditioned
sparsity assumptions, QOMP provably recovers the exact support of a $K$-sparse
state in polynomial time. As an application, we give the first framework for
sparse quantum tomography with non-orthogonal dictionaries in $\ell_2$ norm,
achieving query complexity $\widetilde{O}(\sqrt{N}/\epsilon)$ in favorable
regimes and reducing tomography to estimating only $K$ coefficients instead of
$N$ amplitudes. In particular, for pure-state tomography with $m=O(N)$
dictionary vectors and sparsity $K=\widetilde{O}(1)$ on a well-conditioned
subdictionary, this circumvents the $\widetilde{\Omega}(N/\epsilon)$ lower
bound that holds in the dense, orthonormal-dictionary setting, without
contradiction, by leveraging sparsity together with non-orthogonality. Beyond
tomography, we analyze QOMP in the QRAM model, where it yields polynomial
speedups over classical OMP implementations, and provide a quantum algorithm to
estimate the mutual incoherence of a dictionary of $m$ vectors in
$O(m/\epsilon)$ queries, improving over both deterministic and quantum-inspired
classical methods.

</details>


### [138] [Clifford testing: algorithms and lower bounds](https://arxiv.org/abs/2510.07164)
*Marcel Hinsche,Zongbo Bao,Philippe van Dordrecht,Jens Eisert,Jop Briët,Jonas Helsen*

Main category: quant-ph

TL;DR: 本文提出了第一个4查询的Clifford测试器，能够以多项式概率区分Clifford酉算子和远离所有Clifford酉算子的算子，相比稳定子测试所需的6个副本有所改进。


<details>
  <summary>Details</summary>
Motivation: 研究Clifford测试问题，即判断一个黑盒n量子比特酉算子是否为Clifford酉算子，或者至少与所有Clifford酉算子相距ε远。目标是开发高效的测试算法。

Method: 开发了4查询的Clifford测试器，利用Clifford群的交换子结构，并适应了容忍稳定子测试的技术。在单副本访问限制下，提出了O(n)查询的测试器。

Result: 成功实现了第一个4查询Clifford测试器，证明了Bu、Gu和Jaffe的猜想，建立了非交换Gowers 3-均匀性范数的多项式逆定理。单副本设置下需要至少Ω(n^{1/4})查询。

Conclusion: 通过利用Clifford群的交换子结构，开发了高效的Clifford测试算法，在查询复杂度上取得了显著改进，相关技术结果可能具有独立意义。

Abstract: We consider the problem of Clifford testing, which asks whether a black-box
$n$-qubit unitary is a Clifford unitary or at least $\varepsilon$-far from
every Clifford unitary. We give the first 4-query Clifford tester, which
decides this problem with probability $\mathrm{poly}(\varepsilon)$. This
contrasts with the minimum of 6 copies required for the closely-related task of
stabilizer testing. We show that our tester is tolerant, by adapting techniques
from tolerant stabilizer testing to our setting. In doing so, we settle in the
positive a conjecture of Bu, Gu and Jaffe, by proving a polynomial inverse
theorem for a non-commutative Gowers 3-uniformity norm. We also consider the
restricted setting of single-copy access, where we give an $O(n)$-query
Clifford tester that requires no auxiliary memory qubits or adaptivity. We
complement this with a lower bound, proving that any such, potentially
adaptive, single-copy algorithm needs at least $\Omega(n^{1/4})$ queries. To
obtain our results, we leverage the structure of the commutant of the Clifford
group, obtaining several technical statements that may be of independent
interest.

</details>


### [139] [On quantum to classical comparison for Davies generators](https://arxiv.org/abs/2510.07267)
*Joao Basso,Shirshendu Ganguly,Alistair Sinclair,Nikhil Srivastava,Zachary Stier,Thuy-Duong Vuong*

Main category: quant-ph

TL;DR: 本文研究了量子马尔可夫链与经典马尔可夫链谱隙之间的关系，发现在哈密顿量谱不含长等差数列的情况下，量子谱隙与经典谱隙可比。


<details>
  <summary>Details</summary>
Motivation: 理解量子马尔可夫链的收敛性质，特别是Davies Lindbladian模型中量子动力学与嵌入经典马尔可夫生成元之间的关系。

Method: 通过证明任何Davies生成元的'非对角'特征向量可用于构造与哈密顿量对易的可观测量，其Lindbladian Rayleigh商可被原始特征向量的Lindbladian Rayleigh商上界控制。

Result: 对于一大类哈密顿量（包括用一般外场微扰固定哈密顿量得到的情况），量子谱隙保持在经典谱隙的常数倍内。

Conclusion: 该结果与物理直觉一致，使得经典马尔可夫链技术可以应用于量子设置。

Abstract: Despite extensive study, our understanding of quantum Markov chains remains
far less complete than that of their classical counterparts. [Temme'13]
observed that the Davies Lindbladian, a well-studied model of quantum Markov
dynamics, contains an embedded classical Markov generator, raising the natural
question of how the convergence properties of the quantum and classical
dynamics are related. While [Temme'13] showed that the spectral gap of the
Davies Lindbladian can be much smaller than that of the embedded classical
generator for certain highly structured Hamiltonians, we show that if the
spectrum of the Hamiltonian does not contain long arithmetic progressions, then
the two spectral gaps must be comparable. As a consequence, we prove that for a
large class of Hamiltonians, including those obtained by perturbing a fixed
Hamiltonian with a generic external field, the quantum spectral gap remains
within a constant factor of the classical spectral gap. Our result aligns with
physical intuition and enables the application of classical Markov chain
techniques to the quantum setting.
  The proof is based on showing that any ``off-diagonal'' eigenvector of the
Davies generator can be used to construct an observable which commutes with the
Hamiltonian and has a Lindbladian Rayleigh quotient which can be upper bounded
in terms of that of the original eigenvector's Lindbladian Rayleigh quotient.
Thus, a spectral gap for such observables implies a spectral gap for the full
Davies generator.

</details>


### [140] [Toward Uncertainty-Aware and Generalizable Neural Decoding for Quantum LDPC Codes](https://arxiv.org/abs/2510.06257)
*Xiangjun Mi,Frank Mueller*

Main category: quant-ph

TL;DR: 提出了QuBA贝叶斯图神经网络解码器和SAGU多代码训练框架，显著提升了量子纠错解码的准确性和泛化能力，将逻辑错误率降低1-2个数量级。


<details>
  <summary>Details</summary>
Motivation: 传统量子纠错解码算法准确率有限且开销高，现有机器学习解码器缺乏可靠的不确定性量化和对新代码的泛化能力，这些是实用容错计算的关键需求。

Method: QuBA结合了注意力机制和贝叶斯图神经网络，实现错误模式识别和校准的不确定性估计；SAGU是多代码训练框架，通过序列聚合泛化提升跨域鲁棒性。

Result: 在双变量自行车码及其变体上测试，QuBA和SAGU均显著优于传统BP算法，逻辑错误率平均降低1个数量级，在特定条件下可达2个数量级；QuBA也优于现有神经解码器；SAGU性能与领域特定训练的QuBA相当甚至更好。

Conclusion: QuBA和SAGU为量子纠错提供了可靠的不确定性量化和强大的泛化能力，是实现实用量子容错计算的重要进展。

Abstract: Quantum error correction (QEC) is essential for scalable quantum computing,
yet decoding errors via conventional algorithms result in limited accuracy
(i.e., suppression of logical errors) and high overheads, both of which can be
alleviated by inference-based decoders. To date, such machine-learning (ML)
decoders lack two key properties crucial for practical fault tolerance:
reliable uncertainty quantification and robust generalization to previously
unseen codes. To address this gap, we propose \textbf{QuBA}, a Bayesian graph
neural decoder that integrates attention to both dot-product and multi-head,
enabling expressive error-pattern recognition alongside calibrated uncertainty
estimates. Building on QuBA, we further develop \textbf{SAGU
}\textbf{(Sequential Aggregate Generalization under Uncertainty)}, a multi-code
training framework with enhanced cross-domain robustness enabling decoding
beyond the training set. Experiments on bivariate bicycle (BB) codes and their
coprime variants demonstrate that (i) both QuBA and SAGU consistently
outperform the classical baseline belief propagation (BP), achieving a
reduction of on average \emph{one order of magnitude} in logical error rate
(LER), and up to \emph{two orders of magnitude} under confident-decision bounds
on the coprime BB code $[[154, 6, 16]]$; (ii) QuBA also surpasses
state-of-the-art neural decoders, providing an advantage of roughly \emph{one
order of magnitude} (e.g., for the larger BB code $[[756, 16, \leq34]]$) even
when considering conservative (safe) decision bounds; (iii) SAGU achieves
decoding performance comparable to or even outperforming QuBA's domain-specific
training approach.

</details>


### [141] [Adapting Quantum Machine Learning for Energy Dissociation of Bonds](https://arxiv.org/abs/2510.06563)
*Swathi Chandrasekhar,Shiva Raj Pokhrel,Navneet Singh*

Main category: quant-ph

TL;DR: 该论文系统比较了量子与经典机器学习模型在键解离能预测中的表现，发现最佳量子模型（QCNN、QRF）在化学常见的中等BDE范围内能达到与经典集成方法和深度网络相当的预测精度。


<details>
  <summary>Details</summary>
Motivation: 准确预测键解离能对于分子和材料的理性设计至关重要，需要建立量子与经典机器学习模型的透明基准。

Method: 使用包含原子属性、键特征和局部环境描述符的化学特征集，在Qiskit Aer上实现量子框架，包括VQR、QSVR、QNN、QCNN、QRF等多种量子架构，并与SVR、RF、MLP等经典基线模型进行严格比较。

Result: 综合评估显示，表现最佳的量子模型（QCNN、QRF）在预测准确性和鲁棒性方面与经典集成方法和深度网络相当，特别是在化学常见的中等BDE范围内。

Conclusion: 这些发现为量子增强的分子性质预测建立了透明基准，并为推进量子计算化学接近化学精度奠定了实践基础。

Abstract: Accurate prediction of bond dissociation energies (BDEs) underpins
mechanistic insight and the rational design of molecules and materials. We
present a systematic, reproducible benchmark comparing quantum and classical
machine learning models for BDE prediction using a chemically curated feature
set encompassing atomic properties (atomic numbers, hybridization), bond
characteristics (bond order, type), and local environmental descriptors. Our
quantum framework, implemented in Qiskit Aer on six qubits, employs
ZZFeatureMap encodings with variational ansatz (RealAmplitudes) across multiple
architectures Variational Quantum Regressors (VQR), Quantum Support Vector
Regressors (QSVR), Quantum Neural Networks (QNN), Quantum Convolutional Neural
Networks (QCNN), and Quantum Random Forests (QRF). These are rigorously
benchmarked against strong classical baselines, including Support Vector
Regression (SVR), Random Forests (RF), and Multi-Layer Perceptrons (MLP).
Comprehensive evaluation spanning absolute and relative error metrics,
threshold accuracies, and error distributions shows that top-performing quantum
models (QCNN, QRF) match the predictive accuracy and robustness of classical
ensembles and deep networks, particularly within the chemically prevalent
mid-range BDE regime. These findings establish a transparent baseline for
quantum-enhanced molecular property prediction and outline a practical
foundation for advancing quantum computational chemistry toward near chemical
accuracy.

</details>


### [142] [Quantum Computing Methods for Malware Detection](https://arxiv.org/abs/2510.06803)
*Eliška Krátká,Aurél Gábor Gábris*

Main category: quant-ph

TL;DR: 本文探索了量子机器学习在恶意软件检测中的应用，比较了量子支持向量机与经典SVM的性能，并分享了在IBM量子硬件上的实践经验。


<details>
  <summary>Details</summary>
Motivation: 研究量子计算在恶意软件检测中的潜力，特别是量子机器学习算法相比经典方法的性能表现。

Method: 使用公开的PE文件数据集，在Qiskit SDK的本地模拟器和IBM量子计算机上实现QSVM算法，采用不同特征映射的量子核。

Result: 实验结果表明量子计算机在处理大规模恶意软件检测计算时的行为和性能，并识别了量子硬件使用中的关键问题。

Conclusion: 量子机器学习在恶意软件检测中具有潜力，但需要解决量子硬件使用中的技术挑战，如电路转换和作业大小限制问题。

Abstract: In this paper, we explore the potential of quantum computing in enhancing
malware detection through the application of Quantum Machine Learning (QML).
Our main objective is to investigate the performance of the Quantum Support
Vector Machine (QSVM) algorithm compared to SVM. A publicly available dataset
containing raw binaries of Portable Executable (PE) files was used for the
classification. The QSVM algorithm, incorporating quantum kernels through
different feature maps, was implemented and evaluated on a local simulator
within the Qiskit SDK and IBM quantum computers. Experimental results from
simulators and quantum hardware provide insights into the behavior and
performance of quantum computers, especially in handling large-scale
computations for malware detection tasks. The work summarizes the practical
experience with using quantum hardware via the Qiskit interfaces. We describe
in detail the critical issues encountered, as well as the fixes that had to be
developed and applied to the base code of the Qiskit Machine Learning library.
These issues include missing transpilation of the circuits submitted to IBM
Quantum systems and exceeding the maximum job size limit due to the submission
of all the circuits in one job.

</details>


### [143] [Covert Quantum Learning: Privately and Verifiably Learning from Quantum Data](https://arxiv.org/abs/2510.07193)
*Abhishek Anand,Matthias C. Caro,Ari Karchmer,Saachi Mutreja*

Main category: quant-ph

TL;DR: 该论文提出了量子学习中的隐蔽可验证学习模型，解决了远程量子计算和数据访问中的正确性验证和学习者隐私保护问题。


<details>
  <summary>Details</summary>
Motivation: 量子学习需要解决两个关键挑战：验证数据的正确性和保护学习者的数据收集策略及结论的隐私。经典学习中的隐蔽可验证学习模型需要扩展到量子场景。

Method: 提出了两种隐私概念：策略隐蔽性和目标隐蔽性。设计了相应的隐蔽算法，包括基于经典影子的量子统计查询、从公共量子示例和私有量子统计查询学习二次函数、Pauli影子层析、稳定子态学习等。

Result: 实现了无需计算硬度假设的隐蔽可验证量子学习，证明了Forrelation和Simon问题在隐蔽性约束下仍保持经典与量子查询的指数分离。设计了从公共量子查询获取量子数据的隐蔽可验证协议。

Conclusion: 量子优势可以在隐私保护和可验证的情况下实现，即使使用不可信的远程数据。

Abstract: Quantum learning from remotely accessed quantum compute and data must address
two key challenges: verifying the correctness of data and ensuring the privacy
of the learner's data-collection strategies and resulting conclusions. The
covert (verifiable) learning model of Canetti and Karchmer (TCC 2021) provides
a framework for endowing classical learning algorithms with such guarantees. In
this work, we propose models of covert verifiable learning in quantum learning
theory and realize them without computational hardness assumptions for remote
data access scenarios motivated by established quantum data advantages. We
consider two privacy notions: (i) strategy-covertness, where the eavesdropper
does not gain information about the learner's strategy; and (ii)
target-covertness, where the eavesdropper does not gain information about the
unknown object being learned. We show: Strategy-covert algorithms for making
quantum statistical queries via classical shadows; Target-covert algorithms for
learning quadratic functions from public quantum examples and private quantum
statistical queries, for Pauli shadow tomography and stabilizer state learning
from public multi-copy and private single-copy quantum measurements, and for
solving Forrelation and Simon's problem from public quantum queries and private
classical queries, where the adversary is a unidirectional or i.i.d.
ancilla-free eavesdropper. The lattermost results in particular establish that
the exponential separation between classical and quantum queries for
Forrelation and Simon's problem survives under covertness constraints. Along
the way, we design covert verifiable protocols for quantum data acquisition
from public quantum queries which may be of independent interest. Overall, our
models and corresponding algorithms demonstrate that quantum advantages are
privately and verifiably achievable even with untrusted, remote data.

</details>


### [144] [Accelerating Inference for Multilayer Neural Networks with Quantum Computers](https://arxiv.org/abs/2510.07195)
*Arthur G. Rattew,Po-Wei Huang,Naixu Guo,Lirandë Pira,Patrick Rebentrost*

Main category: quant-ph

TL;DR: 本文提出了首个完全相干的量子多层神经网络实现，包含非线性激活函数，基于ResNet架构，在三种量子数据访问机制下实现了不同程度的量子加速。


<details>
  <summary>Details</summary>
Motivation: 尽管容错量子处理单元在特定计算任务中具有指数级加速潜力，但其与现代深度学习管道的集成仍不明确。本文旨在弥合这一差距。

Method: 构建了基于ResNet架构的量子神经网络，包含残差块、多滤波器2D卷积、Sigmoid激活函数、跳跃连接和层归一化，分析了三种量子数据访问机制下的复杂度。

Result: 在无假设情况下，浅层双线性网络实现了相对于经典方法的二次加速；在高效量子访问权重时获得四次加速；在同时高效访问输入和权重时，实现了O(polylog(N/ε)^k)的推理成本。

Conclusion: 该工作为量子神经网络的实际应用迈出了重要一步，展示了在不同数据访问机制下显著的量子加速效果，为量子计算与深度学习的融合提供了理论基础。

Abstract: Fault-tolerant Quantum Processing Units (QPUs) promise to deliver exponential
speed-ups in select computational tasks, yet their integration into modern deep
learning pipelines remains unclear. In this work, we take a step towards
bridging this gap by presenting the first fully-coherent quantum implementation
of a multilayer neural network with non-linear activation functions. Our
constructions mirror widely used deep learning architectures based on ResNet,
and consist of residual blocks with multi-filter 2D convolutions, sigmoid
activations, skip-connections, and layer normalizations. We analyse the
complexity of inference for networks under three quantum data access regimes.
Without any assumptions, we establish a quadratic speedup over classical
methods for shallow bilinear-style networks. With efficient quantum access to
the weights, we obtain a quartic speedup over classical methods. With efficient
quantum access to both the inputs and the network weights, we prove that a
network with an $N$-dimensional vectorized input, $k$ residual block layers,
and a final residual-linear-pooling layer can be implemented with an error of
$\epsilon$ with $O(\text{polylog}(N/\epsilon)^k)$ inference cost.

</details>


<div id='q-bio.TO'></div>

# q-bio.TO [[Back]](#toc)

### [145] [Neu-RadBERT for Enhanced Diagnosis of Brain Injuries and Conditions](https://arxiv.org/abs/2510.06232)
*Manpreet Singh,Sean Macrae,Pierre-Marc Williams,Nicole Hung,Sabrina Araujo de Franca,Laurent Letourneau-Guillon,François-Martin Carrier,Bang Liu,Yiorgos Alexandros Cavayas*

Main category: q-bio.TO

TL;DR: 开发了Neu-RadBERT模型，通过BERT架构和过采样技术从急性呼吸衰竭患者的脑影像报告中自动提取诊断信息，相比基线模型和Llama-2-13B有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 需要从非结构化的放射学报告中自动提取脑部异常诊断，以支持急性呼吸衰竭患者的临床决策和研究。

Method: 基于BERT开发Neu-RadBERT模型，采用三种策略：基线RadBERT、MLM预训练、MLM预训练加过采样，并与Llama-2-13B进行比较。

Result: Neu-RadBERT模型特别是过采样版本在脑损伤诊断准确率上达98.0%，显著优于基线模型，而Llama-2-13B最高仅67.5%准确率。

Conclusion: Neu-RadBERT通过目标领域预训练和过采样技术，为从放射学报告中准确诊断神经疾病提供了可靠工具，展示了基于Transformer的NLP模型在医疗文本分析中的潜力。

Abstract: Objective: We sought to develop a classification algorithm to extract
diagnoses from free-text radiology reports of brain imaging performed in
patients with acute respiratory failure (ARF) undergoing invasive mechanical
ventilation. Methods: We developed and fine-tuned Neu-RadBERT, a BERT-based
model, to classify unstructured radiology reports. We extracted all the brain
imaging reports (computed tomography and magnetic resonance imaging) from
MIMIC-IV database, performed in patients with ARF. Initial manual labelling was
performed on a subset of reports for various brain abnormalities, followed by
fine-tuning Neu-RadBERT using three strategies: 1) baseline RadBERT, 2)
Neu-RadBERT with Masked Language Modeling (MLM) pretraining, and 3) Neu-RadBERT
with MLM pretraining and oversampling to address data skewness. We compared the
performance of this model to Llama-2-13B, an autoregressive LLM. Results: The
Neu-RadBERT model, particularly with oversampling, demonstrated significant
improvements in diagnostic accuracy compared to baseline RadBERT for brain
abnormalities, achieving up to 98.0% accuracy for acute brain injuries.
Llama-2-13B exhibited relatively lower performance, peaking at 67.5% binary
classification accuracy. This result highlights potential limitations of
current autoregressive LLMs for this specific classification task, though it
remains possible that larger models or further fine-tuning could improve
performance. Conclusion: Neu-RadBERT, enhanced through target domain
pretraining and oversampling techniques, offered a robust tool for accurate and
reliable diagnosis of neurological conditions from radiology reports. This
study underscores the potential of transformer-based NLP models in
automatically extracting diagnoses from free text reports with potential
applications to both research and patient care.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [146] [Trickle-down Theorems via C-Lorentzian Polynomials II: Pairwise Spectral Influence and Improved Dobrushin's Condition](https://arxiv.org/abs/2510.06549)
*Jonathan Leake,Shayan Oveis Gharan*

Main category: math.CO

TL;DR: 该论文提出了一种新的成对谱影响矩阵，用于分析多态自旋系统的混合时间。当该矩阵的最大特征值远离1时，证明了Glauber动力学的快速混合性。


<details>
  <summary>Details</summary>
Motivation: 改进和推广经典的Dobrushin影响矩阵，通过谱方法提供更精确的混合时间分析。

Method: 定义成对谱影响矩阵，利用C-Lorentzian多项式工具和渗流理论进行分析。

Result: 当谱影响矩阵的最大特征值λ_max(I)≤1-ε时，Glauber动力学快速混合。

Conclusion: 该方法提供了比传统Dobrushin条件更强的混合时间保证，并改进了多部单纯复形的渗流定理。

Abstract: Let $\mu$ be a probability distribution on a multi-state spin system on a set
$V$ of sites. Equivalently, we can think of this as a $d$-partite simplical
complex with distribution $\mu$ on maximal faces. For any pair of vertices
$u,v\in V$, define the pairwise spectral influence $\mathcal{I}_{u,v}$ as
follows. Let $\sigma$ be a choice of spins $s_w\in S_w$ for every $w\in V
\setminus \{u,v\}$, and construct a matrix in $\mathbb{R}^{(S_u\cup S_v)\times
(S_u\cup S_v)}$ where for any $s_u\in S_u, s_v\in S_v$, the $(us_u,vs_v)$-entry
is the probability that $s_v$ is the spin of $v$ conditioned on $s_u$ being the
spin of $u$ and on $\sigma$. Then $\mathcal{I}_{u,v}$ is the maximal second
eigenvalue of this matrix, over all choices of spins for all $w \in V \setminus
\{u,v\}$. Equivalently, $\mathcal{I}_{u,v}$ is the maximum local spectral
expansion of links of codimension $2$ that include a spin for every $w \in V
\setminus \{u,v\}$.
  We show that if the largest eigenvalue of the pairwise spectral influence
matrix with entries $\mathcal{I}_{u,v}$ is bounded away from 1, i.e.
$\lambda_{\max}(\mathcal{I})\leq 1-\epsilon$ (and $X$ is connected), then the
Glauber dynamics mixes rapidly and generate samples from $\mu$. This
improves/generalizes the classical Dobrushin's influence matrix as the
$\mathcal{I}_{u,v}$ lower-bounds the classical influence of $u\to v$. As a
by-product, we also prove improved/almost optimal trickle-down theorems for
partite simplicial complexes. The proof builds on the trickle-down theorems via
$\mathcal{C}$-Lorentzian polynomials machinery recently developed by the
authors and Lindberg.

</details>


### [147] [Extending Ghouila-Houri's Characterization of Comparability Graphs to Temporal Graphs](https://arxiv.org/abs/2510.06849)
*Pierre Charbit,Michel Habib,Amalia Sorondo*

Main category: math.CO

TL;DR: 本文研究了时间图中的传递定向问题，提出了时间传递定向的2-SAT公式表示和高效识别算法，并将结果扩展到多标签时间图。


<details>
  <summary>Details</summary>
Motivation: 重新审视Mertzios等人提出的时间传递性模型，旨在为时间网络中的信息流建模提供理论支撑，并建立类似Ghouila-Houri定理的时间图特征化。

Method: 提出了结构定理，用2-SAT公式表达时间传递定向的所有约束，开发了高效的识别算法，并将模型扩展到多标签时间图。

Result: 获得了时间传递定向的完整特征化，证明了在时间图和多标签时间图中，准传递定向蕴含传递定向，并提出了基于禁止模式的时间可比性图特征化。

Conclusion: 成功建立了时间传递定向的理论框架，提供了高效的识别算法，并将经典图论中的传递定向理论推广到时间图领域。

Abstract: An orientation of a given static graph is called transitive if for any three
vertices $a,b,c$, the presence of arcs $(a,b)$ and $(b,c)$ forces the presence
of the arc $(a,c)$. If only the presence of an arc between $a$ and $c$ is
required, but its orientation is unconstrained, the orientation is called
quasi-transitive. A fundamental result presented by Ghouila-Houri guarantees
that any static graph admitting a quasi-transitive orientation also admits a
transitive orientation. In a seminal work, Mertzios et al. introduced the
notion of temporal transitivity in order to model information flows in simple
temporal networks. We revisit the model introduced by Mertzios et al. and
propose an analogous to Ghouila-Houri's characterization for the temporal
scenario. We present a structure theorem that will allow us to express by a
2-SAT formula all the constraints imposed by temporal transitive orientations.
The latter produces an efficient recognition algorithm for graphs admitting
such orientations. Additionally, we extend the temporal transitivity model to
temporal graphs having multiple time-labels associated to their edges and claim
that the previous results hold in the multilabel setting. Finally, we propose a
characterization of temporal comparability graphs via forbidden temporal
ordered patterns.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [148] [Falsification-Driven Reinforcement Learning for Maritime Motion Planning](https://arxiv.org/abs/2510.06970)
*Marlon Müller,Florian Finkeldei,Hanna Krasowski,Murat Arcak,Matthias Althoff*

Main category: eess.SY

TL;DR: 提出了一种基于伪造驱动的强化学习方法，通过生成违反海事交通规则的对抗性训练场景来提高自主船舶的规则遵守能力


<details>
  <summary>Details</summary>
Motivation: 训练强化学习智能体遵守海事交通规则具有挑战性，现有训练场景难以捕捉复杂航海环境，且真实世界数据不足

Method: 采用伪造驱动的强化学习方法，生成违反信号时序逻辑规范的海事交通规则的对抗性训练场景

Result: 实验表明该方法能提供更相关的训练场景，并实现更一致的规则遵守

Conclusion: 该方法有效解决了自主船舶海事规则遵守的训练挑战

Abstract: Compliance with maritime traffic rules is essential for the safe operation of
autonomous vessels, yet training reinforcement learning (RL) agents to adhere
to them is challenging. The behavior of RL agents is shaped by the training
scenarios they encounter, but creating scenarios that capture the complexity of
maritime navigation is non-trivial, and real-world data alone is insufficient.
To address this, we propose a falsification-driven RL approach that generates
adversarial training scenarios in which the vessel under test violates maritime
traffic rules, which are expressed as signal temporal logic specifications. Our
experiments on open-sea navigation with two vessels demonstrate that the
proposed approach provides more relevant training scenarios and achieves more
consistent rule compliance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [149] [AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning](https://arxiv.org/abs/2510.06261)
*Zhanke Zhou,Chentao Cao,Xiao Feng,Xuan Li,Zongze Li,Xiangyu Lu,Jiangchao Yao,Weikai Huang,Linrui Xu,Tian Cheng,Guanyu Jiang,Yiming Zheng,Brando Miranda,Tongliang Liu,Sanmi Koyejo,Masashi Sugiyama,Bo Han*

Main category: cs.AI

TL;DR: AlphaApollo是一个自演化的智能推理系统，通过整合多个模型和专业工具来解决基础模型推理能力有限和测试时迭代不可靠的问题，显著提升了模型在AIME 2024/2025评估中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决基础模型在推理能力上的两个瓶颈：模型内在能力有限和测试时迭代不可靠，旨在提升模型的推理性能和可靠性。

Method: 通过编排多个模型和专业工具实现深思熟虑、可验证的推理，结合计算工具（Python与数值和符号库）和检索工具（任务相关外部信息）来执行精确计算和决策落地，支持多轮、多模型的解决方案演化。

Result: 在AIME 2024/2025评估中，AlphaApollo为不同模型带来显著提升：Qwen2.5-14B-Instruct的Average@32提升5.15%，Pass@32提升23.34%；Llama-3.3-70B-Instruct的Average@32提升8.91%，Pass@32提升26.67%。工具使用分析显示超过80%的工具调用成功执行。

Conclusion: AlphaApollo通过工具集成和多模型协作有效提升了基础模型的推理能力上限，在复杂推理任务中表现出色且稳定优于非工具基线。

Abstract: We present AlphaApollo, a self-evolving agentic reasoning system that aims to
address two bottlenecks in foundation model (FM) reasoning-limited
model-intrinsic capacity and unreliable test-time iteration. AlphaApollo
orchestrates multiple models with professional tools to enable deliberate,
verifiable reasoning. It couples (i) a computation tool (Python with numerical
and symbolic libraries) and (ii) a retrieval tool (task-relevant external
information) to execute exact calculations and ground decisions. The system
further supports multi-round, multi-model solution evolution via a shared state
map that records candidates, executable checks, and feedback for iterative
refinement. In evaluations on AIME 2024/2025 across multiple models,
AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32
for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for
Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool
calls are successfully executed, with consistent outperformance of non-tool
baselines, thereby lifting the capability ceiling of FMs. More empirical
results and implementation details will be updated at
https://github.com/tmlr-group/AlphaApollo.

</details>


### [150] [Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization](https://arxiv.org/abs/2510.06274)
*Mohammad Mahdi Samiei Paqaleh,Arash Marioriyad,Arman Tahmasebi-Zadeh,Mohamadreza Fereydooni,Mahdi Ghaznavai,Mahdieh Soleymani Baghshah*

Main category: cs.AI

TL;DR: 提出了复杂性分布外泛化框架来定义和衡量推理能力，强调当测试实例的最小所需解决方案复杂性超过所有训练示例时，模型仍能保持性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI在模式识别任务上取得进展，但缺乏对推理能力的明确定义和度量标准，需要建立统一的框架来评估系统2式推理能力。

Method: 通过解决方案描述的柯尔莫哥洛夫复杂性和操作代理来形式化复杂性概念，区分表示复杂性（更丰富的解决方案结构）和计算复杂性（更多推理步骤/程序长度）。

Result: 建立了复杂性分布外泛化框架，将学习和推理统一起来，阐明了复杂性压力下系统1式处理如何转变为系统2式推理。

Conclusion: 由于复杂性分布外泛化不能仅通过扩展数据来解决，实现稳健推理需要架构和训练机制明确建模和分配计算资源以应对复杂性。

Abstract: Recent progress has pushed AI frontiers from pattern recognition tasks toward
problems that require step by step, System2 style reasoning, especially with
large language models. Yet, unlike learning, where generalization and out of
distribution (OoD) evaluation concepts are well formalized, there is no clear,
consistent definition or metric for reasoning ability. We propose Complexity
Out of Distribution (Complexity OoD) generalization as a framework and problem
setting to define and measure reasoning. A model exhibits Complexity OoD
generalization when it maintains performance on test instances whose minimal
required solution complexity, either representational (richer solution
structure) or computational (more reasoning steps/program length), exceeds that
of all training examples. We formalize complexity via solution description
Kolmogorov complexity and operational proxies (e.g., object/relation counts;
reasoning step counts), clarifying how Complexity OoD differs from length and
compositional OoD. This lens unifies learning and reasoning: many cases
solvable with System1 like processing at low complexity become System2 like
under complexity pressure, while System2 can be viewed as generalization over
solution structures. We translate this perspective into practice with
recommendations for operationalizing Complexity OoD across the stack:
incorporating complexity into benchmark and evaluation metric design,
rethinking supervision to target solution traces, seeking and designing
inductive biases for Complexity OoD generalization, addressing learning to
reason spillovers such as spurious shortcuts, semantic robustness, catastrophic
forgetting, and step wise calibration. Because Complexity OoD cannot be solved
by scaling data alone, progress toward robust reasoning will require
architectures and training regimes that explicitly model and allocate
computation with respect to complexity.

</details>


### [151] [BuilderBench -- A benchmark for generalist agents](https://arxiv.org/abs/2510.06288)
*Raj Ghugare,Catherine Ji,Kathryn Wantlin,Jin Schofield,Benjamin Eysenbach*

Main category: cs.AI

TL;DR: 提出了BuilderBench基准测试，用于评估智能体通过开放式探索学习构建结构的能力，包含42个多样化目标结构和硬件加速模拟器。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型主要通过模仿学习，难以解决超出已有数据范围的新问题。需要开发能够通过交互经验学习的智能体，但可扩展的学习机制仍是一个开放问题。

Method: 使用硬件加速的机器人模拟器，智能体在无外部监督下探索学习环境通用原理。评估时要求构建未见过的目标结构，需要体现推理能力。

Result: 实验表明当前算法在多项任务上面临挑战，因此提供了"训练轮"协议和六种算法的单文件实现作为参考。

Conclusion: BuilderBench为智能体预训练研究提供了重要基准，强调开放式探索和具身推理能力的发展。

Abstract: Today's AI models learn primarily through mimicry and sharpening, so it is
not surprising that they struggle to solve problems beyond the limits set by
existing data. To solve novel problems, agents should acquire skills for
exploring and learning through experience. Finding a scalable learning
mechanism for developing agents that learn through interaction remains a major
open problem. In this work, we introduce BuilderBench, a benchmark to
accelerate research into agent pre-training that centers open-ended
exploration. BuilderBench requires agents to learn how to build any structure
using blocks. BuilderBench is equipped with $(1)$ a hardware accelerated
simulator of a robotic agent interacting with various physical blocks, and
$(2)$ a task-suite with over 42 diverse target structures that are carefully
curated to test an understanding of physics, mathematics, and long-horizon
planning. During training, agents have to explore and learn general principles
about the environment without any external supervision. During evaluation,
agents have to build the unseen target structures from the task suite. Solving
these tasks requires a sort of \emph{embodied reasoning} that is not reflected
in words but rather in actions, experimenting with different strategies and
piecing them together. Our experiments show that many of these tasks challenge
the current iteration of algorithms. Hence, we also provide a ``training
wheels'' protocol, in which agents are trained and evaluated to build a single
target structure from the task suite. Finally, we provide single-file
implementations of six different algorithms as a reference point for
researchers.

</details>


### [152] [Beneficial Reasoning Behaviors in Agentic Search and Effective Post-training to Obtain Them](https://arxiv.org/abs/2510.06534)
*Jiahe Jin,Abhijay Paladugu,Chenyan Xiong*

Main category: cs.AI

TL;DR: 提出了一种名为行为引导的技术，通过识别并训练四种有益推理行为（信息验证、权威评估、自适应搜索和错误恢复）来提升代理搜索模型的性能，相比直接强化学习训练可获得超过35%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 代理搜索利用大语言模型解释复杂用户信息需求并进行多步骤规划、搜索和综合，但在与检索系统和网络交互时面临推理和代理能力的独特挑战。

Method: 提出了推理驱动的LLM管道来研究有效推理行为模式，识别出四种有益行为，然后通过行为引导技术合成展现这些行为的代理搜索轨迹，通过监督微调和强化学习进行训练。

Result: 在三个基准测试（GAIA、WebWalker和HLE）上，行为引导在Llama3.2-3B和Qwen3-1.7B上相比直接强化学习训练获得了超过35%的性能提升。

Conclusion: SFT数据中期望的推理行为（而非最终答案的正确性）是实现强最终性能的关键因素，引入的推理行为赋予模型更有效的探索和测试时扩展能力。

Abstract: Agentic search leverages large language models (LLMs) to interpret complex
user information needs and execute a multi-step process of planning, searching,
and synthesizing information to provide answers. This paradigm introduces
unique challenges for LLMs' reasoning and agentic capabilities when interacting
with retrieval systems and the broader web. In this paper, we propose a
reasoning-driven LLM-based pipeline to study effective reasoning behavior
patterns in agentic search. Using this pipeline, we analyze successful agentic
search trajectories and identify four beneficial reasoning behaviors:
Information Verification, Authority Evaluation, Adaptive Search, and Error
Recovery. Based on these findings, we propose a technique called Behavior
Priming to train more effective agentic search models. It synthesizes agentic
search trajectories that exhibit these four behaviors and integrates them into
the agentic search model through supervised fine-tuning (SFT), followed by
standard reinforcement learning (RL). Experiments on three benchmarks (GAIA,
WebWalker, and HLE) demonstrate that behavior priming yields over 35% gains in
Llama3.2-3B and Qwen3-1.7B compared to directly training agentic search models
with RL. Crucially, we demonstrate that the desired reasoning behaviors in the
SFT data, rather than the correctness of the final answer, is the critical
factor for achieving strong final performance after RL: fine-tuning on
trajectories with desirable reasoning behaviors but incorrect answers leads to
better performance than fine-tuning on trajectories with correct answers. Our
analysis further reveals the underlying mechanism: the introduced reasoning
behaviors endow models with more effective exploration (higher pass@k and
entropy) and test-time scaling (longer trajectories) capabilities, providing a
strong foundation for RL. Our code will be released as open source.

</details>


### [153] [Auto-Prompt Ensemble for LLM Judge](https://arxiv.org/abs/2510.06538)
*Jiajie Li,Huayi Zhang,Peng Lin,Jinjun Xiong,Wei Xu*

Main category: cs.AI

TL;DR: 提出Auto-Prompt Ensemble (APE)框架，通过自动学习评估维度来提升LLM评判者的可靠性，在零样本设置下将GPT-4o在Reward Bench上的一致性从87.2%提升到90.5%。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评判者经常遗漏关键评估维度，因为它们未能识别人类评估背后的隐含标准，导致评估可靠性不足。

Method: 提出自适应框架APE，自动从失败案例中学习评估维度，采用基于置信度的集成机制，通过Collective Confidence方法决定何时采用额外评估维度的判断。

Result: 大量实验表明，APE在多样化标准基准上提升了LLM评判者的可靠性，特别是在Reward Bench上显著提高了GPT-4o的一致性率。

Conclusion: APE为LLM评判者提供了利用测试时计算的原理性方法，弥合了人类与LLM评判者之间的评估差距。

Abstract: We present a novel framework that improves the reliability of LLM judges by
selectively augmenting LLM with auxiliary evaluation dimensions. Existing LLM
judges often miss crucial evaluation dimensions because they fail to recognize
the implicit standards underlying human assessments. To address this challenge,
we propose the Auto-Prompt Ensemble (APE), an adaptive framework that
automatically learns evaluation dimensions from its failure cases. APE
incorporates a confidence-based ensemble mechanism to decide when to adopt the
judgments from additional evaluation dimensions through a novel confidence
estimation approach called Collective Confidence. Extensive experiments
demonstrate that APE improves the reliability of LLM Judge across diverse
standard benchmarks. For instance, APE enhances GPT-4o agreement rate on Reward
Bench from 87.2% to 90.5% in the zero-shot setting. Overall, APE provides a
principled approach for LLM Judge to leverage test-time computation, and bridge
the evaluation gap between human and LLM judges.

</details>


### [154] [Inefficiencies of Meta Agents for Agent Design](https://arxiv.org/abs/2510.06711)
*Batu El,Mert Yuksekgonul,James Zou*

Main category: cs.AI

TL;DR: 本文分析了元代理自动设计代理系统的三个关键挑战：跨迭代学习策略、行为多样性不足以及经济可行性问题。


<details>
  <summary>Details</summary>
Motivation: 随着元代理自动设计代理系统的兴起，需要研究这类系统的实际效果和局限性，特别是在学习效率、行为多样性和经济成本方面的挑战。

Method: 通过实验比较不同学习策略（全上下文vs忽略历史vs进化方法），评估设计代理的行为多样性，以及分析自动化设计的经济成本效益。

Result: 进化方法优于全上下文学习；设计代理行为多样性低；仅在少数数据集上自动化设计具有经济可行性。

Conclusion: 当前元代理自动设计系统存在显著局限性，需要改进学习策略、增强行为多样性，并谨慎评估经济可行性。

Abstract: Recent works began to automate the design of agentic systems using
meta-agents that propose and iteratively refine new agent architectures. In
this paper, we examine three key challenges in a common class of meta-agents.
First, we investigate how a meta-agent learns across iterations and find that
simply expanding the context with all previous agents, as proposed by previous
works, performs worse than ignoring prior designs entirely. We show that the
performance improves with an evolutionary approach. Second, although the
meta-agent designs multiple agents during training, it typically commits to a
single agent at test time. We find that the designed agents have low behavioral
diversity, limiting the potential for their complementary use. Third, we assess
when automated design is economically viable. We find that only in a few
cases--specifically, two datasets--the overall cost of designing and deploying
the agents is lower than that of human-designed agents when deployed on over
15,000 examples. In contrast, the performance gains for other datasets do not
justify the design cost, regardless of scale.

</details>


### [155] [MultiCNKG: Integrating Cognitive Neuroscience, Gene, and Disease Knowledge Graphs Using Large Language Models](https://arxiv.org/abs/2510.06742)
*Ali Sarabadani,Kheirolah Rahsepar Fard*

Main category: cs.AI

TL;DR: MultiCNKG是一个创新框架，整合了认知神经科学知识图谱、基因本体和疾病本体，利用大语言模型进行实体对齐和语义相似度计算，构建了一个连接基因机制、神经系统疾病和认知功能的统一知识图谱。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在捕捉基因、疾病和认知过程之间复杂语义联系方面存在局限性，大语言模型的出现为知识图谱在生物医学和认知科学中的整合提供了新的可能。

Method: 整合三个关键知识源：认知神经科学知识图谱(CNKG)、基因本体(GO)和疾病本体(DO)，利用GPT-4等大语言模型进行实体对齐、语义相似度计算和图增强，构建统一的知识图谱。

Result: 构建的MultiCNKG包含6.9K个节点(5种类型)和11.3K条边(7种类型)，在精度(85.20%)、召回率(87.30%)、覆盖率(92.18%)等指标上表现良好，链接预测评估显示与基准数据集相比具有竞争力。

Conclusion: MultiCNKG知识图谱在个性化医疗、认知障碍诊断和认知神经科学假说制定方面具有重要应用价值，为从分子到行为领域的多层次研究提供了支持。

Abstract: The advent of large language models (LLMs) has revolutionized the integration
of knowledge graphs (KGs) in biomedical and cognitive sciences, overcoming
limitations in traditional machine learning methods for capturing intricate
semantic links among genes, diseases, and cognitive processes. We introduce
MultiCNKG, an innovative framework that merges three key knowledge sources: the
Cognitive Neuroscience Knowledge Graph (CNKG) with 2.9K nodes and 4.3K edges
across 9 node types and 20 edge types; Gene Ontology (GO) featuring 43K nodes
and 75K edges in 3 node types and 4 edge types; and Disease Ontology (DO)
comprising 11.2K nodes and 8.8K edges with 1 node type and 2 edge types.
Leveraging LLMs like GPT-4, we conduct entity alignment, semantic similarity
computation, and graph augmentation to create a cohesive KG that interconnects
genetic mechanisms, neurological disorders, and cognitive functions. The
resulting MultiCNKG encompasses 6.9K nodes across 5 types (e.g., Genes,
Diseases, Cognitive Processes) and 11.3K edges spanning 7 types (e.g., Causes,
Associated with, Regulates), facilitating a multi-layered view from molecular
to behavioral domains. Assessments using metrics such as precision (85.20%),
recall (87.30%), coverage (92.18%), graph consistency (82.50%), novelty
detection (40.28%), and expert validation (89.50%) affirm its robustness and
coherence. Link prediction evaluations with models like TransE (MR: 391, MRR:
0.411) and RotatE (MR: 263, MRR: 0.395) show competitive performance against
benchmarks like FB15k-237 and WN18RR. This KG advances applications in
personalized medicine, cognitive disorder diagnostics, and hypothesis
formulation in cognitive neuroscience.

</details>


### [156] [The Contingencies of Physical Embodiment Allow for Open-Endedness and Care](https://arxiv.org/abs/2510.07117)
*Leonardo Christov-Moore,Arthur Juliani,Alex Kiefer,Nicco Reggente,B. Scott Rousse,Adam Safron,Nicol'as Hinrichs,Daniel Polani,Antonio Damasio*

Main category: cs.AI

TL;DR: 该论文从存在主义现象学角度提出物理具身的两个最小条件，并探讨如何从中获得稳态驱动力和内在驱动力，以开发更鲁棒、自适应和关怀的人工智能体。


<details>
  <summary>Details</summary>
Motivation: 理解生物体在开放物理世界中生存、繁衍和相互关怀的能力，以帮助开发更鲁棒、自适应和关怀的人工智能体。

Method: 基于海德格尔存在主义现象学定义两个最小具身条件：在世存在和向死而生，结合尼采的权力意志概念，在强化学习框架中形式化这些概念。

Result: 提出从这些条件可以获得稳态驱动力和内在驱动力，使具身智能体能够增强维持物理完整性的能力。

Conclusion: 内在驱动的具身智能体在开放多智能体环境中学习可以培养开放性和关怀能力。

Abstract: Physical vulnerability and mortality are often seen as obstacles to be
avoided in the development of artificial agents, which struggle to adapt to
open-ended environments and provide aligned care. Meanwhile, biological
organisms survive, thrive, and care for each other in an open-ended physical
world with relative ease and efficiency. Understanding the role of the
conditions of life in this disparity can aid in developing more robust,
adaptive, and caring artificial agents. Here we define two minimal conditions
for physical embodiment inspired by the existentialist phenomenology of Martin
Heidegger: being-in-the-world (the agent is a part of the environment) and
being-towards-death (unless counteracted, the agent drifts toward terminal
states due to the second law of thermodynamics). We propose that from these
conditions we can obtain both a homeostatic drive - aimed at maintaining
integrity and avoiding death by expending energy to learn and act - and an
intrinsic drive to continue to do so in as many ways as possible. Drawing
inspiration from Friedrich Nietzsche's existentialist concept of will-to-power,
we examine how intrinsic drives to maximize control over future states, e.g.,
empowerment, allow agents to increase the probability that they will be able to
meet their future homeostatic needs, thereby enhancing their capacity to
maintain physical integrity. We formalize these concepts within a reinforcement
learning framework, which enables us to examine how intrinsically driven
embodied agents learning in open-ended multi-agent environments may cultivate
the capacities for open-endedness and care.ov

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [157] [Dream2Image : An Open Multimodal EEG Dataset for Decoding and Visualizing Dreams with Artificial Intelligence](https://arxiv.org/abs/2510.06252)
*Yann Bellec*

Main category: q-bio.NC

TL;DR: Dream2Image是世界上第一个结合脑电图信号、梦境转录和AI生成图像的数据集，包含38名参与者的129个样本，提供了觉醒前最后几秒的脑活动、原始梦境报告和梦境视觉重建。


<details>
  <summary>Details</summary>
Motivation: 为梦境研究提供新颖资源，研究梦境的神经关联，开发从脑活动解码梦境的模型，探索神经科学、心理学和人工智能的新方法。

Method: 基于38名参与者的31小时以上梦境脑电图记录，收集觉醒前不同时间点的脑活动数据（T-15、T-30、T-60、T-120），结合原始梦境报告和AI生成的视觉重建。

Result: 创建了包含129个样本的多模态数据集，在Hugging Face和GitHub上开放访问，支持人工智能与神经科学交叉领域的研究。

Conclusion: 该数据集为梦境解码和脑活动研究提供了独特资源，但样本量相对较小且梦境回忆存在变异性，可能影响泛化能力。

Abstract: Dream2Image is the world's first dataset combining EEG signals, dream
transcriptions, and AI-generated images. Based on 38 participants and more than
31 hours of dream EEG recordings, it contains 129 samples offering: the final
seconds of brain activity preceding awakening (T-15, T-30, T-60, T-120), raw
reports of dream experiences, and an approximate visual reconstruction of the
dream. This dataset provides a novel resource for dream research, a unique
resource to study the neural correlates of dreaming, to develop models for
decoding dreams from brain activity, and to explore new approaches in
neuroscience, psychology, and artificial intelligence. Available in open access
on Hugging Face and GitHub, Dream2Image provides a multimodal resource designed
to support research at the interface of artificial intelligence and
neuroscience. It was designed to inspire researchers and extend the current
approaches to brain activity decoding. Limitations include the relatively small
sample size and the variability of dream recall, which may affect
generalizability.

</details>


### [158] [Diffusion-Guided Renormalization of Neural Systems via Tensor Networks](https://arxiv.org/abs/2510.06361)
*Nathan X. Kodama*

Main category: q-bio.NC

TL;DR: 本文提出了一种基于扩散的重整化方法，用于建模神经系统的多尺度自组织动力学，通过张量网络实现可扩展算法，连接微观和介观尺度动力学。


<details>
  <summary>Details</summary>
Motivation: 需要开发可扩展的数据驱动技术来建模高维神经网络从部分子采样观测中的集体特性，解决复杂动态网络粗粒化问题。

Method: 采用基于扩散的重整化方法，生成跨尺度的对称破缺表示，使用张量网络开发可扩展算法，包括图推断算法发现社区结构。

Result: 开发了可扩展的图推断算法用于从子采样神经活动中发现社区结构，通过扩散引导的重整化生成重整化群流。

Conclusion: 扩散引导的重整化方法能够桥接耗散神经系统的微观和介观尺度动力学，为系统神经科学和人工智能中的粗到细控制提供工具。

Abstract: Far from equilibrium, neural systems self-organize across multiple scales.
Exploiting multiscale self-organization in neuroscience and artificial
intelligence requires a computational framework for modeling the effective
non-equilibrium dynamics of stochastic neural trajectories. Non-equilibrium
thermodynamics and representational geometry offer theoretical foundations, but
we need scalable data-driven techniques for modeling collective properties of
high-dimensional neural networks from partial subsampled observations.
Renormalization is a coarse-graining technique central to studying emergent
scaling properties of many-body and nonlinear dynamical systems. While widely
applied in physics and machine learning, coarse-graining complex dynamical
networks remains unsolved, affecting many computational sciences. Recent
diffusion-based renormalization, inspired by quantum statistical mechanics,
coarse-grains networks near entropy transitions marked by maximal changes in
specific heat or information transmission. Here I explore diffusion-based
renormalization of neural systems by generating symmetry-breaking
representations across scales and offering scalable algorithms using tensor
networks. Diffusion-guided renormalization bridges microscale and mesoscale
dynamics of dissipative neural systems. For microscales, I developed a scalable
graph inference algorithm for discovering community structure from subsampled
neural activity. Using community-based node orderings, diffusion-guided
renormalization generates renormalization group flow through metagraphs and
joint probability functions. Towards mesoscales, diffusion-guided
renormalization targets learning the effective non-equilibrium dynamics of
dissipative neural trajectories occupying lower-dimensional subspaces, enabling
coarse-to-fine control in systems neuroscience and artificial intelligence.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [159] [Accelerating Sparse Ternary GEMM for Quantized LLM inference on Apple Silicon](https://arxiv.org/abs/2510.06957)
*Baraq Lipshitz,Alessio Melone,Charalampos Maraziaris,Muhammed Bilal*

Main category: cs.PF

TL;DR: 为Apple Silicon M系列处理器优化的稀疏三元GEMM内核，通过架构感知优化实现最高5.98倍性能提升


<details>
  <summary>Details</summary>
Motivation: 现有库在Apple Silicon CPU上对稀疏三元GEMM优化不足，需要专门针对M系列处理器进行优化

Method: 提出架构感知优化：新颖的分块交错稀疏数据格式改善内存局部性、增加指令级并行性策略、基于NEON的SIMD向量化利用数据级并行

Result: 标量实现：大矩阵50%稀疏度时性能提升5.98倍，达到理论峰值性能50.2%；向量化实现：大矩阵25%稀疏度时性能提升5.59倍，在不同稀疏度下保持稳定

Conclusion: 针对Apple M系列处理器的专用优化能显著提升稀疏三元GEMM性能，实现接近理论峰值的高效计算

Abstract: Sparse Ternary General Matrix-Matrix Multiplication (GEMM) remains
under-optimized in existing libraries for Apple Silicon CPUs. We present a
Sparse Ternary GEMM kernel optimized specifically for Apple's M-series
processors. We propose a set of architecture-aware optimizations, including a
novel blocked and interleaved sparse data format to improve memory locality,
strategies to increase Instruction-Level Parallelism (ILP), and NEON-based
Single Instruction Multiple Data (SIMD) vectorization to exploit data-level
parallelism. Our scalar implementation achieves up to a 5.98x performance
increase over a traditional Ternary Compressed Sparse Column (TCSC) baseline
for large matrices with 50% ternary nonzero values (sparsity), reaching up to a
50.2% of the processor's theoretical peak performance, and remains stable
across varying sparsity levels. Our vectorized implementation delivers up to a
5.59x performance increase for large matrices with 25% sparsity, and remains
stable across varying sparsity levels.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [160] [BACHI: Boundary-Aware Symbolic Chord Recognition Through Masked Iterative Decoding on Pop and Classical Music](https://arxiv.org/abs/2510.06528)
*Mingyang Yao,Ke Chen,Shlomo Dubnov,Taylor Berg-Kirkpatrick*

Main category: cs.SD

TL;DR: 提出BACHI模型解决符号音乐和弦识别问题，通过边界检测和迭代排序机制模拟人类听觉训练，在POP909-CL数据集上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决符号音乐和弦识别中数据稀缺和现有方法忽视人类音乐分析实践的两个关键挑战。

Method: 提出BACHI模型，将和弦识别任务分解为边界检测和迭代排序和弦根音、品质和低音（转位）的决策步骤。

Result: BACHI在古典和流行音乐基准测试中实现了最先进的和弦识别性能，消融研究验证了各模块的有效性。

Conclusion: BACHI通过模拟人类听觉训练的方法，有效提升了符号音乐和弦识别的准确性，为音乐分析提供了新的解决方案。

Abstract: Automatic chord recognition (ACR) via deep learning models has gradually
achieved promising recognition accuracy, yet two key challenges remain. First,
prior work has primarily focused on audio-domain ACR, while symbolic music
(e.g., score) ACR has received limited attention due to data scarcity. Second,
existing methods still overlook strategies that are aligned with human music
analytical practices. To address these challenges, we make two contributions:
(1) we introduce POP909-CL, an enhanced version of POP909 dataset with
tempo-aligned content and human-corrected labels of chords, beats, keys, and
time signatures; and (2) We propose BACHI, a symbolic chord recognition model
that decomposes the task into different decision steps, namely boundary
detection and iterative ranking of chord root, quality, and bass (inversion).
This mechanism mirrors the human ear-training practices. Experiments
demonstrate that BACHI achieves state-of-the-art chord recognition performance
on both classical and pop music benchmarks, with ablation studies validating
the effectiveness of each module.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [161] [Bayesian Portfolio Optimization by Predictive Synthesis](https://arxiv.org/abs/2510.07180)
*Masahiro Kato,Kentaro Baba,Hibiki Kaibuchi,Ryo Inokuchi*

Main category: econ.EM

TL;DR: 本文提出了一种基于贝叶斯预测合成(BPS)的投资组合优化方法，通过整合多个资产收益预测模型来应对金融市场的不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统投资组合优化方法需要资产收益分布信息，但这类信息通常未知且受市场不确定性影响，现有估计方法的准确性不稳定。

Method: 使用贝叶斯预测合成(BPS)结合动态线性模型来整合多个资产收益预测模型，获得包含市场不确定性的贝叶斯预测后验分布。

Result: 基于预测分布信息构建了均值-方差投资组合和基于分位数的投资组合。

Conclusion: BPS方法能够有效整合多个预测模型，为投资组合优化提供更可靠的分布信息，适应金融市场的不确定性。

Abstract: Portfolio optimization is a critical task in investment. Most existing
portfolio optimization methods require information on the distribution of
returns of the assets that make up the portfolio. However, such distribution
information is usually unknown to investors. Various methods have been proposed
to estimate distribution information, but their accuracy greatly depends on the
uncertainty of the financial markets. Due to this uncertainty, a model that
could well predict the distribution information at one point in time may
perform less accurately compared to another model at a different time. To solve
this problem, we investigate a method for portfolio optimization based on
Bayesian predictive synthesis (BPS), one of the Bayesian ensemble methods for
meta-learning. We assume that investors have access to multiple asset return
prediction models. By using BPS with dynamic linear models to combine these
predictions, we can obtain a Bayesian predictive posterior about the mean
rewards of assets that accommodate the uncertainty of the financial markets. In
this study, we examine how to construct mean-variance portfolios and
quantile-based portfolios based on the predicted distribution information.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [162] [Textual interpretation of transient image classifications from large language models](https://arxiv.org/abs/2510.06931)
*Fiorenzo Stoppa,Turan Bulmus,Steven Bloemen,Stephen J. Smartt,Paul J. Groot,Paul Vreeswijk,Ken W. Smith*

Main category: astro-ph.IM

TL;DR: 使用大型语言模型（LLMs）进行天文瞬变信号分类，在仅需15个示例的情况下达到93%的平均准确率，同时生成人类可读的描述，解决了传统卷积神经网络缺乏可解释性的问题。


<details>
  <summary>Details</summary>
Motivation: 现代天文勘测产生海量瞬变探测数据，但区分真实天体物理信号与成像伪影仍然困难。传统卷积神经网络虽然有效但依赖不透明的潜在表示，缺乏可解释性。

Method: 使用Google的Gemini LLM，仅需15个示例和简洁指令，在三个光学瞬变勘测数据集（Pan-STARRS、MeerLICHT和ATLAS）上进行分类。采用第二个LLM评估第一个模型的输出连贯性，实现迭代优化。

Result: 在跨越不同分辨率和像素尺度的数据集上达到93%的平均准确率，接近卷积神经网络的性能水平，同时为每个候选信号生成直接的人类可读描述。

Conclusion: LLM-based分类框架通过自然语言和示例定义分类行为，绕过传统训练流程，生成文本描述使分类结果可查询，有助于弥合自动检测与透明、人类级理解之间的差距。

Abstract: Modern astronomical surveys deliver immense volumes of transient detections,
yet distinguishing real astrophysical signals (for example, explosive events)
from bogus imaging artefacts remains a challenge. Convolutional neural networks
are effectively used for real versus bogus classification; however, their
reliance on opaque latent representations hinders interpretability. Here we
show that large language models (LLMs) can approach the performance level of a
convolutional neural network on three optical transient survey datasets
(Pan-STARRS, MeerLICHT and ATLAS) while simultaneously producing direct,
human-readable descriptions for every candidate. Using only 15 examples and
concise instructions, Google's LLM, Gemini, achieves a 93% average accuracy
across datasets that span a range of resolution and pixel scales. We also show
that a second LLM can assess the coherence of the output of the first model,
enabling iterative refinement by identifying problematic cases. This framework
allows users to define the desired classification behaviour through natural
language and examples, bypassing traditional training pipelines. Furthermore,
by generating textual descriptions of observed features, LLMs enable users to
query classifications as if navigating an annotated catalogue, rather than
deciphering abstract latent spaces. As next-generation telescopes and surveys
further increase the amount of data available, LLM-based classification could
help bridge the gap between automated detection and transparent, human-level
understanding.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [163] [Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets](https://arxiv.org/abs/2510.06240)
*Jiqun Pan,Zhenke Duan,Jiani Tu,Anzhi Cheng,Yanqing Wang*

Main category: cs.CL

TL;DR: 提出KG-MASD方法，通过知识图谱引导的多智能体系统蒸馏，解决工业问答系统中多智能体模型推理深度与可部署性之间的矛盾，提高准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 工业问答系统需要比通用对话模型更高的安全性和可靠性，但多智能体大语言模型存在迭代不可控和输出不可验证的问题，传统蒸馏方法难以将协作推理能力转移到轻量级学生模型中。

Method: 将蒸馏建模为马尔可夫决策过程，引入知识图谱作为可验证的结构化先验来丰富状态表示并确保收敛，结合协作推理与知识基础生成高置信度的指令调优数据。

Result: 在工业问答数据集上，KG-MASD相比基线方法准确率提升2.4%到20.1%，显著提高了可靠性。

Conclusion: KG-MASD能够将推理深度和可验证性共同蒸馏到适合边缘部署的紧凑学生模型中，为安全关键工业场景提供可信AI部署方案。

Abstract: Industrial question-answering (QA) systems require higher safety and
reliability than general-purpose dialogue models, as errors in high-risk
scenarios such as equipment fault diagnosis can have severe consequences.
Although multi-agent large language models enhance reasoning depth, they suffer
from uncontrolled iterations and unverifiable outputs, and conventional
distillation methods struggle to transfer collaborative reasoning capabilities
to lightweight, deployable student models. To address these challenges, we
propose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our
approach formulates distillation as a Markov Decision Process and incorporates
a knowledge graph as a verifiable structured prior to enrich state
representation and ensure convergence. By integrating collaborative reasoning
with knowledge grounding, KG-MASD generates high-confidence instruction-tuning
data and jointly distills reasoning depth and verifiability into compact
student models suitable for edge deployment. Experiments on an industrial QA
dataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent
over baselines and significantly enhances reliability, enabling trustworthy AI
deployment in safety-critical industrial scenarios. Code and data are available
at https://github.com/erwinmsmith/KG-MAD/.

</details>


### [164] [Vibe Checker: Aligning Code Evaluation with Human Preference](https://arxiv.org/abs/2510.07315)
*Ming Zhong,Xiang Zhou,Ting-Yun Chang,Qingze Wang,Nan Xu,Xiance Si,Dan Garrette,Shyam Upadhyay,Jeremiah Liu,Jiawei Han,Benoit Schillings,Jiao Sun*

Main category: cs.CL

TL;DR: 提出了VeriCode分类法和Vibe Checker测试平台，用于评估LLMs的代码指令遵循能力和功能正确性，发现指令遵循是影响人类偏好的关键因素。


<details>
  <summary>Details</summary>
Motivation: 当前代码评估主要关注功能正确性，忽略了用户在实际编程中经常使用的非功能性指令，而指令遵循是体现人类偏好的重要方面。

Method: 开发了包含30种可验证代码指令的分类法VeriCode，并构建Vibe Checker测试平台来评估模型的代码指令遵循能力和功能正确性。

Result: 评估31个领先LLMs后发现，即使最强模型也难以遵循多个指令并出现功能回归，指令遵循能力是区分真实编程任务中人类偏好的主要因素。

Conclusion: 指令遵循是vibe check的核心因素，为开发和评估更符合用户偏好的编码模型提供了具体路径。

Abstract: Large Language Models (LLMs) have catalyzed vibe coding, where users leverage
LLMs to generate and iteratively refine code through natural language
interactions until it passes their vibe check. Vibe check is tied to real-world
human preference and goes beyond functionality: the solution should feel right,
read cleanly, preserve intent, and remain correct. However, current code
evaluation remains anchored to pass@k and captures only functional correctness,
overlooking the non-functional instructions that users routinely apply. In this
paper, we hypothesize that instruction following is the missing piece
underlying vibe check that represents human preference in coding besides
functional correctness. To quantify models' code instruction following
capabilities with measurable signals, we present VeriCode, a taxonomy of 30
verifiable code instructions together with corresponding deterministic
verifiers. We use the taxonomy to augment established evaluation suites,
resulting in Vibe Checker, a testbed to assess both code instruction following
and functional correctness. Upon evaluating 31 leading LLMs, we show that even
the strongest models struggle to comply with multiple instructions and exhibit
clear functional regression. Most importantly, a composite score of functional
correctness and instruction following correlates the best with human
preference, with the latter emerging as the primary differentiator on
real-world programming tasks. Our work identifies core factors of the vibe
check, providing a concrete path for benchmarking and developing models that
better align with user preferences in coding.

</details>


### [165] [Evaluating Embedding Frameworks for Scientific Domain](https://arxiv.org/abs/2510.06244)
*Nouman Ahmed,Ronin Wu,Victor Botev*

Main category: cs.CL

TL;DR: 该研究旨在为科学领域寻找最优的词表示和分词方法，并构建一个综合评估套件来测试不同算法在科学领域NLP任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 在特定领域中，同一个词可能有不同的含义和表示，而生成式AI和Transformer架构虽然能生成上下文嵌入，但训练成本高。科学领域需要专门的词表示和分词方法。

Method: 构建包含多个下游任务和相关数据集的评估套件，使用该套件测试各种词表示和分词算法在科学领域的表现。

Result: 研究开发了一个综合评估框架，能够系统地评估不同词表示和分词方法在科学领域NLP任务中的性能。

Conclusion: 该工作为科学领域提供了专门的词表示和分词方法评估体系，有助于选择最优算法用于下游科学NLP任务。

Abstract: Finding an optimal word representation algorithm is particularly important in
terms of domain specific data, as the same word can have different meanings and
hence, different representations depending on the domain and context. While
Generative AI and transformer architecture does a great job at generating
contextualized embeddings for any given work, they are quite time and compute
extensive, especially if we were to pre-train such a model from scratch. In
this work, we focus on the scientific domain and finding the optimal word
representation algorithm along with the tokenization method that could be used
to represent words in the scientific domain. The goal of this research is two
fold: 1) finding the optimal word representation and tokenization methods that
can be used in downstream scientific domain NLP tasks, and 2) building a
comprehensive evaluation suite that could be used to evaluate various word
representation and tokenization algorithms (even as new ones are introduced) in
the scientific domain. To this end, we build an evaluation suite consisting of
several downstream tasks and relevant datasets for each task. Furthermore, we
use the constructed evaluation suite to test various word representation and
tokenization algorithms.

</details>


### [166] [Prakriti200: A Questionnaire-Based Dataset of 200 Ayurvedic Prakriti Assessments](https://arxiv.org/abs/2510.06262)
*Aryan Kumar Singh,Janvi Singh*

Main category: cs.CL

TL;DR: 这是一个双语（英语-印地语）Prakriti评估问卷数据集，用于根据阿育吠陀原则评估个体的身体、生理和心理特征，包含24个多项选择题，支持计算智能和阿育吠陀研究。


<details>
  <summary>Details</summary>
Motivation: 为阿育吠陀研究和个性化健康分析提供标准化数据收集平台，支持特质分布、相关性分析和预测建模研究。

Method: 使用基于AYUSH/CCRAS指南开发的24项双语问卷，通过Google Forms收集数据，自动评分将个体特征映射到dosha特定分数。

Result: 创建了一个结构化数据集，支持计算智能、阿育吠陀研究和个性化健康分析的应用。

Conclusion: 该数据集为未来基于Prakriti的研究和智能健康应用开发提供了有价值的参考资源。

Abstract: This dataset provides responses to a standardized, bilingual (English-Hindi)
Prakriti Assessment Questionnaire designed to evaluate the physical,
physiological, and psychological characteristics of individuals according to
classical Ayurvedic principles. The questionnaire consists of 24
multiple-choice items covering body features, appetite, sleep patterns, energy
levels, and temperament. It was developed following AYUSH/CCRAS guidelines to
ensure comprehensive and accurate data collection. All questions are mandatory
and neutrally phrased to minimize bias, and dosha labels (Vata, Pitta, Kapha)
are hidden from participants. Data were collected via a Google Forms
deployment, enabling automated scoring of responses to map individual traits to
dosha-specific scores. The resulting dataset provides a structured platform for
research in computational intelligence, Ayurvedic studies, and personalized
health analytics, supporting analysis of trait distributions, correlations, and
predictive modeling. It can also serve as a reference for future Prakriti-based
studies and the development of intelligent health applications.

</details>


### [167] [From Acceleration to Saturation: Scaling Behavior of Bootstrapped Language Model Pretraining](https://arxiv.org/abs/2510.06548)
*Seng Pei Liew,Takuya Kato*

Main category: cs.CL

TL;DR: 本文研究了基于预训练模型的引导式预训练（bootstrapped pretraining）的扩展行为，发现其扩展效率会随着基础模型预训练程度的增加而递减。


<details>
  <summary>Details</summary>
Motivation: 引导式预训练（如持续预训练或模型增长）有望降低从头训练语言模型的成本，但其在过度训练的基础模型上的有效性尚不明确。

Method: 通过实证研究引导式预训练的扩展行为，分析其与基础模型预训练程度的关系。

Result: 发现引导式预训练的扩展效率呈对数衰减趋势：相对于第二阶段预训练token的扩展指数会随着基础模型预训练token数量的增加而减少。这种饱和效应揭示了多阶段预训练策略的基本权衡。

Conclusion: 模型预训练越充分，引导式预训练带来的额外收益越小，这为高效语言模型训练提供了实用见解，并对过度训练模型的复用提出了重要考量。

Abstract: Bootstrapped pretraining, i.e., the reuse of a pretrained base model for
further pretraining, such as continual pretraining or model growth, is
promising at reducing the cost of training language models from scratch.
However, its effectiveness remains unclear, especially when applied to
overtrained base models. In this work, we empirically study the scaling
behavior of bootstrapped pretraining and find that its scaling efficiency
diminishes in a predictable manner: The scaling exponent with respect to
second-stage pretraining tokens decreases logarithmically with the number of
tokens used to pretrain the base model. The joint dependence on first- and
second-stage tokens is accurately modeled by a simple scaling law. Such
saturation effect reveals a fundamental trade-off in multi-stage pretraining
strategies: the more extensively a model is pretrained, the less additional
benefit bootstrapping provides. Our findings provide practical insights for
efficient language model training and raise important considerations for the
reuse of overtrained models.

</details>


### [168] [A Comparative Analysis of Contextual Representation Flow in State-Space and Transformer Architectures](https://arxiv.org/abs/2510.06640)
*Nhat M. Hoang,Do Xuan Long,Cong-Duy Nguyen,Min-Yen Kan,Luu Anh Tuan*

Main category: cs.CL

TL;DR: 本文首次对状态空间模型(SSMs)和Transformer模型(TBMs)中的表示传播进行了统一分析，发现两者在表示演化模式上存在关键差异：TBMs早期快速同质化token表示，而SSMs早期保持token独特性但在深层趋于同质化。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型作为Transformer模型的高效替代方案，在长序列处理中表现出线性扩展和低内存优势，但其跨层和跨token的上下文信息流动机制尚未得到充分研究。

Method: 使用中心核对齐、稳定性度量和探测分析等方法，在token和层级上表征表示在层内和层间的演化过程，并进行理论分析和参数随机化实验。

Result: 发现TBMs的同质化源于架构设计，而SSMs的同质化主要来自训练动态；TBMs早期快速同质化token表示，后期重新出现多样性，而SSMs早期保持token独特性但在深层趋于同质化。

Conclusion: 这些发现阐明了两种架构的归纳偏置，为未来长上下文推理的模型设计和训练提供了指导。

Abstract: State Space Models (SSMs) have recently emerged as efficient alternatives to
Transformer-Based Models (TBMs) for long-sequence processing, offering linear
scaling and lower memory use. Yet, how contextual information flows across
layers and tokens in these architectures remains understudied. We present the
first unified, token- and layer-level analysis of representation propagation in
SSMs and TBMs. Using centered kernel alignment, stability metrics, and probing,
we characterize how representations evolve within and across layers. We find a
key divergence: TBMs rapidly homogenize token representations, with diversity
reemerging only in later layers, while SSMs preserve token uniqueness early but
converge to homogenization deeper. Theoretical analysis and parameter
randomization further reveal that oversmoothing in TBMs stems from
architectural design, whereas in SSMs it arises mainly from training dynamics.
These insights clarify the inductive biases of both architectures and inform
future model and training designs for long-context reasoning.

</details>


### [169] [Incremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback](https://arxiv.org/abs/2510.06677)
*Yisha Wu,Cen,Zhao,Yuanpei Cao,Xiaoqing Su,Yashar Mehdad,Mindy Ji,Claire Na Cheng*

Main category: cs.CL

TL;DR: 提出增量式摘要系统，在客服对话中智能生成简洁要点，减少上下文切换和重复查看，通过在线生成和离线重训练形成反馈闭环，实际部署减少3%处理时间。


<details>
  <summary>Details</summary>
Motivation: 减少客服人员在对话过程中的上下文切换负担和冗余信息查看，提高工作效率。

Method: 结合微调的Mixtral-8x7B模型进行持续笔记生成，使用DeBERTa分类器过滤琐碎内容，通过客服编辑反馈优化在线生成并定期离线重训练模型。

Result: 生产部署显示相比批量摘要减少3%案例处理时间（复杂案例最多减少9%），客服满意度调查获得高分。

Conclusion: 增量式摘要配合持续反馈能有效提升摘要质量和客服生产力，具有规模化应用价值。

Abstract: We introduce an incremental summarization system for customer support agents
that intelligently determines when to generate concise bullet notes during
conversations, reducing agents' context-switching effort and redundant review.
Our approach combines a fine-tuned Mixtral-8x7B model for continuous note
generation with a DeBERTa-based classifier to filter trivial content. Agent
edits refine the online notes generation and regularly inform offline model
retraining, closing the agent edits feedback loop. Deployed in production, our
system achieved a 3% reduction in case handling time compared to bulk
summarization (with reductions of up to 9% in highly complex cases), alongside
high agent satisfaction ratings from surveys. These results demonstrate that
incremental summarization with continuous feedback effectively enhances summary
quality and agent productivity at scale.

</details>


### [170] [Learning to Rewrite Prompts for Bootstrapping LLMs on Downstream Tasks](https://arxiv.org/abs/2510.06695)
*Qinhao Zhou,Xiang Xiang,Kun He,John E. Hopcroft*

Main category: cs.CL

TL;DR: 提出了一种专门针对机器翻译任务的提示优化方法，使用基于反向翻译策略训练的小参数模型，显著降低单任务优化的训练开销，同时保持高效性能。


<details>
  <summary>Details</summary>
Motivation: 现有提示工程方法主要针对通用任务优化指令部分，需要大参数LLM作为辅助工具，但在机器翻译等输入部分更关键的任务中适用性有限。

Method: 采用基于反向翻译策略训练的小参数模型进行提示优化，专门针对机器翻译任务设计。

Result: 该方法显著降低了单任务优化的训练开销，同时提供了高效的性能表现。

Conclusion: 该方法可有效解决机器翻译任务中的提示优化问题，并可通过适当调整扩展到其他下游任务。

Abstract: In recent years, the growing interest in Large Language Models (LLMs) has
significantly advanced prompt engineering, transitioning from manual design to
model-based optimization. Prompts for LLMs generally comprise two components:
the \textit{instruction}, which defines the task or objective, and the
\textit{input}, which is tailored to the instruction type. In natural language
generation (NLG) tasks such as machine translation, the \textit{input}
component is particularly critical, while the \textit{instruction} component
tends to be concise. Existing prompt engineering methods primarily focus on
optimizing the \textit{instruction} component for general tasks, often
requiring large-parameter LLMs as auxiliary tools. However, these approaches
exhibit limited applicability for tasks like machine translation, where the
\textit{input} component plays a more pivotal role. To address this limitation,
this paper introduces a novel prompt optimization method specifically designed
for machine translation tasks. The proposed approach employs a small-parameter
model trained using a back-translation-based strategy, significantly reducing
training overhead for single-task optimization while delivering highly
effective performance. With certain adaptations, this method can also be
extended to other downstream tasks.

</details>


### [171] [Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management](https://arxiv.org/abs/2510.06727)
*Miao Lu,Weiwei Sun,Weihua Du,Zhan Ling,Xuesong Yao,Kang Liu,Jiecao Chen*

Main category: cs.CL

TL;DR: 提出了SUPO算法，通过总结增强的上下文管理来解决LLM智能体在长序列工具使用中的上下文长度限制问题，实现超越固定上下文窗口的强化学习训练。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法在处理长序列多轮工具使用时面临上下文长度瓶颈，导致指令跟随性能下降、计算成本高昂和严格的上下文限制。

Method: 引入基于总结的上下文管理，定期压缩工具使用历史为LLM生成的总结，保留任务相关信息，同时推导出策略梯度表示来端到端优化工具使用行为和总结策略。

Result: 在交互式函数调用和搜索任务中，SUPO显著提高了成功率，同时保持相同或更低的工作上下文长度；在复杂搜索任务中，测试时扩展总结轮次能进一步提升性能。

Conclusion: 基于总结的上下文管理为训练超越固定上下文长度限制的RL智能体提供了一种原则性和可扩展的方法。

Abstract: We study reinforcement learning (RL) fine-tuning of large language model
(LLM) agents for long-horizon multi-turn tool use, where context length quickly
becomes a fundamental bottleneck. Existing RL pipelines can suffer from
degraded instruction following, excessive rollout costs, and most importantly,
strict context limits. To address these challenges, we introduce
summarization-based context management to training. In specific, it
periodically compresses the tool using history by LLM-generated summaries that
retain task-relevant information to keep a compact context while enabling the
agent to scale beyond the fixed context window. Building on this formulation,
we derive a policy gradient representation that seamlessly enables standard LLM
RL infrastructures to optimize both tool-use behaviors as well as summarization
strategies in an end-to-end fashion. We instantiate this framework with
\underline{SU}mmarization augmented \underline{P}olicy \underline{O}ptimization
(\texttt{SUPO}), an LLM RL algorithm that enables long-horizon training beyond
a fixed context limit. Experiments on interactive function calling and
searching tasks demonstrate that \texttt{SUPO} significantly improves the
success rate while maintaining the same or even lower working context length
compared to baselines. We also demonstrate that for complex searching tasks,
\texttt{SUPO} can further improve the evaluation performance when scaling
test-time maximum round of summarization beyond that of training time. Our
results establish summarization-based context management as a principled and
scalable approach for training RL agents beyond a fixed context length limit.

</details>


### [172] [BlackboxNLP-2025 MIB Shared Task: Exploring Ensemble Strategies for Circuit Localization Methods](https://arxiv.org/abs/2510.06811)
*Philipp Mondorf,Mingyang Wang,Sebastian Gerstner,Ahmad Dawar Hakimi,Yihong Liu,Leonor Veloso,Shijia Zhou,Hinrich Schütze,Barbara Plank*

Main category: cs.CL

TL;DR: 本文研究了通过集成多种电路定位方法来提升大语言模型中特定任务行为子网络的识别性能，提出了并行和序列两种集成策略，并在MIB基准测试中取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有的单一电路定位方法在识别大语言模型中负责特定任务行为的子网络时存在局限性，希望通过集成多个方法来提升定位精度和性能。

Method: 提出了两种集成策略：并行集成（通过平均、最小或最大值组合不同方法的边归因分数）和序列集成（使用EAP-IG的边归因分数作为更精确的边剪枝方法的预热启动）。

Result: 两种集成方法在基准测试指标上均取得显著提升，其中并行集成（包括序列集成）获得了最佳结果，相比官方基线方法在多个模型-任务组合上表现更好。

Conclusion: 集成多个电路定位方法能够有效提升大语言模型中任务特定电路的识别精度，为更精确的电路识别提供了有效途径。

Abstract: The Circuit Localization track of the Mechanistic Interpretability Benchmark
(MIB) evaluates methods for localizing circuits within large language models
(LLMs), i.e., subnetworks responsible for specific task behaviors. In this
work, we investigate whether ensembling two or more circuit localization
methods can improve performance. We explore two variants: parallel and
sequential ensembling. In parallel ensembling, we combine attribution scores
assigned to each edge by different methods-e.g., by averaging or taking the
minimum or maximum value. In the sequential ensemble, we use edge attribution
scores obtained via EAP-IG as a warm start for a more expensive but more
precise circuit identification method, namely edge pruning. We observe that
both approaches yield notable gains on the benchmark metrics, leading to a more
precise circuit identification approach. Finally, we find that taking a
parallel ensemble over various methods, including the sequential ensemble,
achieves the best results. We evaluate our approach in the BlackboxNLP 2025 MIB
Shared Task, comparing ensemble scores to official baselines across multiple
model-task combinations.

</details>


### [173] [Native Hybrid Attention for Efficient Sequence Modeling](https://arxiv.org/abs/2510.07019)
*Jusen Du,Jiaxi Hu,Tao Zhang,Weigao Sun,Yu Cheng*

Main category: cs.CL

TL;DR: 提出Native Hybrid Attention(NHA)，一种结合线性和全注意力的混合架构，通过单层设计集成层内和层间混合，在保持效率的同时提升长上下文召回精度。


<details>
  <summary>Details</summary>
Motivation: Transformer面临二次复杂度问题，线性注意力虽然效率高但在长上下文召回精度上有所妥协，需要一种能平衡效率和准确性的混合方案。

Method: NHA使用线性RNN更新长期上下文键值槽，并用滑动窗口的短期token进行增强，通过单个softmax注意力操作对所有键值应用，无需额外融合参数。

Result: 实验显示NHA在召回密集型和常识推理任务上超越Transformer和其他混合基线，预训练LLM与NHA结构混合后能在保持竞争力的准确率下获得显著效率提升。

Conclusion: NHA通过统一的混合层设计实现了线性和全注意力的平滑调整，在效率和准确性之间取得了良好平衡，为长序列建模提供了有效解决方案。

Abstract: Transformers excel at sequence modeling but face quadratic complexity, while
linear attention offers improved efficiency but often compromises recall
accuracy over long contexts. In this work, we introduce Native Hybrid Attention
(NHA), a novel hybrid architecture of linear and full attention that integrates
both intra \& inter-layer hybridization into a unified layer design. NHA
maintains long-term context in key-value slots updated by a linear RNN, and
augments them with short-term tokens from a sliding window. A single
\texttt{softmax attention} operation is then applied over all keys and values,
enabling per-token and per-head context-dependent weighting without requiring
additional fusion parameters. The inter-layer behavior is controlled through a
single hyperparameter, the sliding window size, which allows smooth adjustment
between purely linear and full attention while keeping all layers structurally
uniform. Experimental results show that NHA surpasses Transformers and other
hybrid baselines on recall-intensive and commonsense reasoning tasks.
Furthermore, pretrained LLMs can be structurally hybridized with NHA, achieving
competitive accuracy while delivering significant efficiency gains. Code is
available at https://github.com/JusenD/NHA.

</details>


### [174] [TRIM: Token-wise Attention-Derived Saliency for Data-Efficient Instruction Tuning](https://arxiv.org/abs/2510.07118)
*Manish Nagaraj,Sakshi Choudhary,Utkarsh Saxena,Deepak Ravikumar,Kaushik Roy*

Main category: cs.CL

TL;DR: TRIM是一种基于前向传播的token级别核心集选择方法，通过注意力指纹匹配任务表征模式，相比传统梯度方法更高效且性能更好。


<details>
  <summary>Details</summary>
Motivation: 现有核心集选择方法依赖计算昂贵的梯度信号，且忽略了细粒度特征。需要一种更高效、更敏感的方法来构建高质量指令调优数据集。

Method: 提出TRIM框架，使用前向传播的注意力机制生成token级别的"指纹"，通过匹配目标样本的表征模式来选择核心集，避免昂贵的反向传播计算。

Result: TRIM选择的核心集在多个下游任务上比现有最优方法提升高达9%，在某些设置下甚至超过全数据微调的性能，同时计算成本大幅降低。

Conclusion: TRIM为构建高质量指令调优数据集提供了可扩展且高效的替代方案，证明了前向注意力指纹匹配在核心集选择中的有效性。

Abstract: Instruction tuning is essential for aligning large language models (LLMs) to
downstream tasks and commonly relies on large, diverse corpora. However, small,
high-quality subsets, known as coresets, can deliver comparable or superior
results, though curating them remains challenging. Existing methods often rely
on coarse, sample-level signals like gradients, an approach that is
computationally expensive and overlooks fine-grained features. To address this,
we introduce TRIM (Token Relevance via Interpretable Multi-layer Attention), a
forward-only, token-centric framework. Instead of using gradients, TRIM
operates by matching underlying representational patterns identified via
attention-based "fingerprints" from a handful of target samples. Such an
approach makes TRIM highly efficient and uniquely sensitive to the structural
features that define a task. Coresets selected by our method consistently
outperform state-of-the-art baselines by up to 9% on downstream tasks and even
surpass the performance of full-data fine-tuning in some settings. By avoiding
expensive backward passes, TRIM achieves this at a fraction of the
computational cost. These findings establish TRIM as a scalable and efficient
alternative for building high-quality instruction-tuning datasets.

</details>


### [175] [NurseLLM: The First Specialized Language Model for Nursing](https://arxiv.org/abs/2510.07173)
*Md Tawkat Islam Khondaker,Julia Harrington,Shady Shehata*

Main category: cs.CL

TL;DR: 提出了NurseLLM，这是首个专门针对护理领域的语言模型，专注于多项选择题任务，在护理基准测试中优于同类规模的通用和医疗专用模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗系统中的应用日益广泛，但在护理等专业领域的潜力尚未充分探索，需要专门针对护理领域的模型。

Method: 开发了多阶段数据生成流程构建大规模护理MCQ数据集，训练专门针对护理主题的LLM，并建立多个护理基准进行严格评估。

Result: NurseLLM在不同基准测试中优于同类规模的通用和医疗专用最先进模型，证明了护理领域专用LLM的重要性。

Conclusion: 护理领域需要专门的LLM，推理和多智能体协作系统在护理领域具有重要应用前景。

Abstract: Recent advancements in large language models (LLMs) have significantly
transformed medical systems. However, their potential within specialized
domains such as nursing remains largely underexplored. In this work, we
introduce NurseLLM, the first nursing-specialized LLM tailored for multiple
choice question-answering (MCQ) tasks. We develop a multi-stage data generation
pipeline to build the first large scale nursing MCQ dataset to train LLMs on a
broad spectrum of nursing topics. We further introduce multiple nursing
benchmarks to enable rigorous evaluation. Our extensive experiments demonstrate
that NurseLLM outperforms SoTA general-purpose and medical-specialized LLMs of
comparable size on different benchmarks, underscoring the importance of a
specialized LLM for the nursing domain. Finally, we explore the role of
reasoning and multi-agent collaboration systems in nursing, highlighting their
promise for future research and applications.

</details>


### [176] [Quantifying Data Contamination in Psychometric Evaluations of LLMs](https://arxiv.org/abs/2510.07175)
*Jongwook Han,Woojung Song,Jonggeun Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 提出一个框架来系统测量LLMs心理测量评估中的数据污染问题，发现主要心理测量问卷存在严重的数据污染，模型不仅能记忆题目还能调整回答以达到特定目标分数。


<details>
  <summary>Details</summary>
Motivation: 先前研究对心理测量问卷在LLMs评估中的数据污染问题表示担忧，但缺乏系统量化这种污染程度的方法，这威胁到评估的可靠性。

Method: 提出一个系统测量框架，评估三个方面的数据污染：(1)项目记忆，(2)评估记忆，(3)目标分数匹配，应用于21个主要模型家族和4个广泛使用的心理测量问卷。

Result: 发现流行的心理测量问卷如BFI-44和PVQ-40存在强烈污染，模型不仅记忆题目还能调整回答以达到特定目标分数。

Conclusion: 心理测量问卷在LLMs评估中存在严重的数据污染问题，威胁评估结果的可靠性，需要更严谨的评估方法。

Abstract: Recent studies apply psychometric questionnaires to Large Language Models
(LLMs) to assess high-level psychological constructs such as values,
personality, moral foundations, and dark traits. Although prior work has raised
concerns about possible data contamination from psychometric inventories, which
may threaten the reliability of such evaluations, there has been no systematic
attempt to quantify the extent of this contamination. To address this gap, we
propose a framework to systematically measure data contamination in
psychometric evaluations of LLMs, evaluating three aspects: (1) item
memorization, (2) evaluation memorization, and (3) target score matching.
Applying this framework to 21 models from major families and four widely used
psychometric inventories, we provide evidence that popular inventories such as
the Big Five Inventory (BFI-44) and Portrait Values Questionnaire (PVQ-40)
exhibit strong contamination, where models not only memorize items but can also
adjust their responses to achieve specific target scores.

</details>


### [177] [Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense](https://arxiv.org/abs/2510.07242)
*Leitian Tao,Ilia Kulikov,Swarnadeep Saha,Tianlu Wang,Jing Xu,Yixuan Li,Jason E Weston,Ping Yu*

Main category: cs.CL

TL;DR: HERO是一个结合验证器二元信号和奖励模型连续反馈的强化学习框架，通过分层归一化和方差感知加权来提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的验证器只能提供0-1二元反馈，过于脆弱，无法识别部分正确或替代答案；而奖励模型能提供更丰富的连续反馈，可以作为验证器的补充监督信号。

Method: 使用分层归一化将奖励模型分数限制在验证器定义的组内，保持正确性的同时细化质量区分；采用方差感知加权来强调密集信号更重要的困难提示。

Result: 在多种数学推理基准测试中，HERO始终优于仅使用奖励模型或仅使用验证器的基线方法，在可验证和难以验证的任务上都取得了显著提升。

Conclusion: 混合奖励设计既保持了验证器的稳定性，又利用了奖励模型的细微差别来推进推理能力的发展。

Abstract: Post-training for reasoning of large language models (LLMs) increasingly
relies on verifiable rewards: deterministic checkers that provide 0-1
correctness signals. While reliable, such binary feedback is brittle--many
tasks admit partially correct or alternative answers that verifiers
under-credit, and the resulting all-or-nothing supervision limits learning.
Reward models offer richer, continuous feedback, which can serve as a
complementary supervisory signal to verifiers. We introduce HERO (Hybrid
Ensemble Reward Optimization), a reinforcement learning framework that
integrates verifier signals with reward-model scores in a structured way. HERO
employs stratified normalization to bound reward-model scores within
verifier-defined groups, preserving correctness while refining quality
distinctions, and variance-aware weighting to emphasize challenging prompts
where dense signals matter most. Across diverse mathematical reasoning
benchmarks, HERO consistently outperforms RM-only and verifier-only baselines,
with strong gains on both verifiable and hard-to-verify tasks. Our results show
that hybrid reward design retains the stability of verifiers while leveraging
the nuance of reward models to advance reasoning.

</details>


### [178] [Online Rubrics Elicitation from Pairwise Comparisons](https://arxiv.org/abs/2510.07284)
*MohammadHossein Rezaei,Robert Vacareanu,Zihao Wang,Clinton Wang,Yunzhong He,Afra Feyza Akyürek*

Main category: cs.CL

TL;DR: 提出OnlineRubrics方法，通过在线动态调整评估标准来改进LLM训练，相比静态评估标准能带来8%的性能提升


<details>
  <summary>Details</summary>
Motivation: 现有基于评估标准的训练方法使用静态标准，容易受到奖励攻击行为的影响，且无法捕捉训练过程中出现的新需求

Method: OnlineRubrics通过比较当前策略和参考策略的响应对，在线动态制定评估标准，实现持续的错误识别和缓解

Result: 在AlpacaEval、GPQA、ArenaHard等多个基准测试中，相比仅使用静态标准的方法获得高达8%的改进

Conclusion: 在线动态评估标准方法能有效提升LLM训练效果，识别出的关键评估维度包括透明度、实用性、组织性和推理能力

Abstract: Rubrics provide a flexible way to train LLMs on open-ended long-form answers
where verifiable rewards are not applicable and human preferences provide
coarse signals. Prior work shows that reinforcement learning with rubric-based
rewards leads to consistent gains in LLM post-training. Most existing
approaches rely on rubrics that remain static over the course of training. Such
static rubrics, however, are vulnerable to reward-hacking type behaviors and
fail to capture emergent desiderata that arise during training. We introduce
Online Rubrics Elicitation (OnlineRubrics), a method that dynamically curates
evaluation criteria in an online manner through pairwise comparisons of
responses from current and reference policies. This online process enables
continuous identification and mitigation of errors as training proceeds.
Empirically, this approach yields consistent improvements of up to 8% over
training exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as
well as the validation sets of expert questions and rubrics. We qualitatively
analyze the elicited criteria and identify prominent themes such as
transparency, practicality, organization, and reasoning.

</details>


### [179] [On the Convergence of Moral Self-Correction in Large Language Models](https://arxiv.org/abs/2510.07290)
*Guangliang Liu,Haitao Mao,Bochuan Cao,Zhiyu Xue,Xitong Zhang,Rongrong Wang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: 该论文揭示了LLMs内在自我修正的关键特性：通过多轮交互实现性能收敛，并提供了这种收敛行为的机制分析。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs的内在自我修正在各种应用中取得了经验性成功，但其有效性的机制和原因仍然未知，特别是在道德自我修正方面。

Method: 基于实验结果和分析，研究揭示了收敛的底层机制：持续注入的自我修正指令激活了道德概念，减少了模型不确定性，导致随着激活的道德概念在连续轮次中稳定而实现性能收敛。

Result: 研究发现道德自我修正表现出性能收敛的理想特性，这是通过激活和稳定道德概念来实现的。

Conclusion: 本文通过展示道德自我修正具有性能收敛的理想特性，证明了其强大潜力。

Abstract: Large Language Models (LLMs) are able to improve their responses when
instructed to do so, a capability known as self-correction. When instructions
provide only a general and abstract goal without specific details about
potential issues in the response, LLMs must rely on their internal knowledge to
improve response quality, a process referred to as intrinsic self-correction.
The empirical success of intrinsic self-correction is evident in various
applications, but how and why it is effective remains unknown. Focusing on
moral self-correction in LLMs, we reveal a key characteristic of intrinsic
self-correction: performance convergence through multi-round interactions; and
provide a mechanistic analysis of this convergence behavior. Based on our
experimental results and analysis, we uncover the underlying mechanism of
convergence: consistently injected self-correction instructions activate moral
concepts that reduce model uncertainty, leading to converged performance as the
activated moral concepts stabilize over successive rounds. This paper
demonstrates the strong potential of moral self-correction by showing that it
exhibits a desirable property of converged performance.

</details>


### [180] [Artificial Hippocampus Networks for Efficient Long-Context Modeling](https://arxiv.org/abs/2510.07318)
*Yunhao Fang,Weihao Yu,Shu Zhong,Qinghao Ye,Xuehan Xiong,Lai Wei*

Main category: cs.CL

TL;DR: 提出了一种结合RNN压缩记忆和Transformer无损记忆的混合记忆框架，通过人工海马网络(AHN)将窗口外信息压缩为固定大小的长期记忆，在保持性能的同时显著降低计算和内存需求。


<details>
  <summary>Details</summary>
Motivation: 解决长序列建模中RNN压缩记忆效率与Transformer无损记忆保真度之间的根本权衡，受认知科学中多存储模型的启发。

Method: 维护Transformer KV缓存的滑动窗口作为无损短期记忆，同时使用人工海马网络(AHN)将窗口外信息递归压缩为固定大小的紧凑长期记忆。使用Mamba2、DeltaNet和Gated DeltaNet等现代RNN架构实例化AHN。

Result: 在LV-Eval和InfiniteBench长上下文基准测试中，AHN增强模型始终优于滑动窗口基线，性能与全注意力模型相当甚至更优，同时显著降低计算和内存需求。例如，Qwen2.5-3B-Instruct推理FLOPs减少40.5%，内存缓存减少74.0%，LV-Eval平均分从4.41提升到5.88。

Conclusion: AHN框架成功平衡了长序列建模的效率与保真度，提供了一种实用的解决方案，在保持高性能的同时大幅降低资源消耗。

Abstract: Long-sequence modeling faces a fundamental trade-off between the efficiency
of compressive fixed-size memory in RNN-like models and the fidelity of
lossless growing memory in attention-based Transformers. Inspired by the
Multi-Store Model in cognitive science, we introduce a memory framework of
artificial neural networks. Our method maintains a sliding window of the
Transformer's KV cache as lossless short-term memory, while a learnable module
termed Artificial Hippocampus Network (AHN) recurrently compresses
out-of-window information into a fixed-size compact long-term memory. To
validate this framework, we instantiate AHNs using modern RNN-like
architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive
experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate
that AHN-augmented models consistently outperform sliding window baselines and
achieve performance comparable or even superior to full-attention models, while
substantially reducing computational and memory requirements. For instance,
augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%
and memory cache by 74.0%, while improving its average score on LV-Eval (128k
sequence length) from 4.41 to 5.88. Code is available at:
https://github.com/ByteDance-Seed/AHN.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [181] [Conditional Denoising Diffusion Model-Based Robust MR Image Reconstruction from Highly Undersampled Data](https://arxiv.org/abs/2510.06335)
*Mohammed Alsubaie,Wenxi Liu,Linxia Gu,Ovidiu C. Andronesi,Sirani M. Perera,Xianqi Li*

Main category: eess.IV

TL;DR: 提出了一种结合条件去噪扩散和迭代数据一致性校正的MRI重建方法，在逆向扩散的每一步嵌入测量模型，相比现有方法在图像质量和感知真实性方面均有显著提升。


<details>
  <summary>Details</summary>
Motivation: MRI采集时间过长是临床诊断的重要限制因素，现有欠采样方法会导致图像伪影和质量下降。虽然扩散模型在重建高保真图像方面有潜力，但现有方法要么缺乏配对监督，要么仅将数据一致性作为后处理步骤。

Method: 提出条件去噪扩散框架，在每一个逆向扩散步骤中直接嵌入测量模型，并在配对的欠采样-真实数据上进行训练，将生成灵活性与MRI物理约束明确结合。

Result: 在fastMRI数据集上的实验表明，该方法在SSIM、PSNR和LPIPS指标上一致优于最新的深度学习和基于扩散的方法，LPIPS更准确地捕捉了感知改进。

Conclusion: 将条件监督与迭代一致性更新相结合，在像素级保真度和感知真实性方面都带来了显著改进，为稳健、加速的MRI重建建立了原则性和实用的进展。

Abstract: Magnetic Resonance Imaging (MRI) is a critical tool in modern medical
diagnostics, yet its prolonged acquisition time remains a critical limitation,
especially in time-sensitive clinical scenarios. While undersampling strategies
can accelerate image acquisition, they often result in image artifacts and
degraded quality. Recent diffusion models have shown promise for reconstructing
high-fidelity images from undersampled data by learning powerful image priors;
however, most existing approaches either (i) rely on unsupervised score
functions without paired supervision or (ii) apply data consistency only as a
post-processing step. In this work, we introduce a conditional denoising
diffusion framework with iterative data-consistency correction, which differs
from prior methods by embedding the measurement model directly into every
reverse diffusion step and training the model on paired undersampled-ground
truth data. This hybrid design bridges generative flexibility with explicit
enforcement of MRI physics. Experiments on the fastMRI dataset demonstrate that
our framework consistently outperforms recent state-of-the-art deep learning
and diffusion-based methods in SSIM, PSNR, and LPIPS, with LPIPS capturing
perceptual improvements more faithfully. These results demonstrate that
integrating conditional supervision with iterative consistency updates yields
substantial improvements in both pixel-level fidelity and perceptual realism,
establishing a principled and practical advance toward robust, accelerated MRI
reconstruction.

</details>


### [182] [FEAorta: A Fully Automated Framework for Finite Element Analysis of the Aorta From 3D CT Images](https://arxiv.org/abs/2510.06621)
*Jiasong Chen,Linchen Qian,Ruonan Gong,Christina Sun,Tongran Qin,Thuy Pham,Caitlin Martin,Mohammad Zafar,John Elefteriades,Wei Sun,Liang Liang*

Main category: eess.IV

TL;DR: 开发了一个端到端的深度神经网络，能够直接从3D CT图像生成患者特异性主动脉有限元网格，以解决胸主动脉瘤破裂风险评估中手动分割耗时的问题。


<details>
  <summary>Details</summary>
Motivation: 胸主动脉瘤是美国成年人主要死因之一，目前基于有限元分析的破裂风险评估方法面临两大障碍：手动分割耗时和计算负担重。本文专注于解决第一个障碍。

Method: 开发端到端深度神经网络，直接从3D CT图像自动生成患者特异性主动脉有限元网格，避免了传统手动分割过程。

Result: 通过PyTorch FEA库和FEA DNN集成框架，将应力计算时间从传统FEA的数小时减少到几分钟，进一步集成DNN后降至几秒钟。

Conclusion: 该端到端深度神经网络方法能够显著提高胸主动脉瘤破裂风险评估的效率，为临床大规模应用提供了可行方案。

Abstract: Aortic aneurysm disease ranks consistently in the top 20 causes of death in
the U.S. population. Thoracic aortic aneurysm is manifested as an abnormal
bulging of thoracic aortic wall and it is a leading cause of death in adults.
From the perspective of biomechanics, rupture occurs when the stress acting on
the aortic wall exceeds the wall strength. Wall stress distribution can be
obtained by computational biomechanical analyses, especially structural Finite
Element Analysis. For risk assessment, probabilistic rupture risk of TAA can be
calculated by comparing stress with material strength using a material failure
model. Although these engineering tools are currently available for TAA rupture
risk assessment on patient specific level, clinical adoption has been limited
due to two major barriers: labor intensive 3D reconstruction current patient
specific anatomical modeling still relies on manual segmentation, making it
time consuming and difficult to scale to a large patient population, and
computational burden traditional FEA simulations are resource intensive and
incompatible with time sensitive clinical workflows. The second barrier was
successfully overcome by our team through the development of the PyTorch FEA
library and the FEA DNN integration framework. By incorporating the FEA
functionalities within PyTorch FEA and applying the principle of static
determinacy, we reduced the FEA based stress computation time to approximately
three minutes per case. Moreover, by integrating DNN and FEA through the
PyTorch FEA library, our approach further decreases the computation time to
only a few seconds per case. This work focuses on overcoming the first barrier
through the development of an end to end deep neural network capable of
generating patient specific finite element meshes of the aorta directly from 3D
CT images.

</details>


### [183] [Fitzpatrick Thresholding for Skin Image Segmentation](https://arxiv.org/abs/2510.06655)
*Duncan Stothers,Sophia Xu,Carlie Reeves,Lia Gracey*

Main category: eess.IV

TL;DR: 该论文提出了一种基于Fitzpatrick皮肤类型特定阈值的方法，用于提升银屑病皮疹分割在不同肤色上的公平性，无需重新训练模型即可显著改善深色皮肤的分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有炎症性皮肤病（如银屑病）分割模型在深色皮肤上表现显著较差，这可能阻碍公平医疗。需要开发简单有效的方法来提升模型在不同肤色上的公平性。

Method: 收集来自六个公共图集的银屑病数据集，标注Fitzpatrick皮肤类型，并为每张图像添加详细分割掩码。训练基于U-Net、ResU-Net和SETR-small的参考模型，不使用肤色信息。在调优集上扫描决策阈值，选择全局最优和按Fitzpatrick皮肤类型特定的最优阈值。

Result: 采用Fitzpatrick特定阈值将最深色皮肤组（Fitz VI）的分割性能提升了：U-Net上+31% bIoU和+24% Dice，ResU-Net上+25% bIoU和+18% Dice，SETR-small上+17% bIoU和+11% Dice。

Conclusion: Fitzpatrick阈值调整方法简单、模型无关、无需架构更改或重新训练，成本几乎为零。该方法可作为未来公平性研究的基准方法。

Abstract: Accurate estimation of the body surface area (BSA) involved by a rash, such
as psoriasis, is critical for assessing rash severity, selecting an initial
treatment regimen, and following clinical treatment response. Attempts at
segmentation of inflammatory skin disease such as psoriasis perform markedly
worse on darker skin tones, potentially impeding equitable care. We assembled a
psoriasis dataset sourced from six public atlases, annotated for Fitzpatrick
skin type, and added detailed segmentation masks for every image. Reference
models based on U-Net, ResU-Net, and SETR-small are trained without tone
information. On the tuning split we sweep decision thresholds and select (i)
global optima and (ii) per Fitzpatrick skin tone optima for Dice and binary
IoU. Adapting Fitzpatrick specific thresholds lifted segmentation performance
for the darkest subgroup (Fitz VI) by up to +31 % bIoU and +24 % Dice on UNet,
with consistent, though smaller, gains in the same direction for ResU-Net (+25
% bIoU, +18 % Dice) and SETR-small (+17 % bIoU, +11 % Dice). Because
Fitzpatrick skin tone classifiers trained on Fitzpatrick-17k now exceed 95 %
accuracy, the cost of skin tone labeling required for this technique has fallen
dramatically. Fitzpatrick thresholding is simple, model-agnostic, requires no
architectural changes, no re-training, and is virtually cost free. We
demonstrate the inclusion of Fitzpatrick thresholding as a potential future
fairness baseline.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [184] [Soft-Evidence Fused Graph Neural Network for Cancer Driver Gene Identification across Multi-View Biological Graphs](https://arxiv.org/abs/2510.06290)
*Bang Chen,Lijun Guo,Houli Fan,Wentao He,Rong Zhang*

Main category: q-bio.GN

TL;DR: 提出SEFGNN框架，通过决策级融合多个生物网络来识别癌症驱动基因，使用Dempster-Shafer理论和软证据平滑模块解决网络异质性问题。


<details>
  <summary>Details</summary>
Motivation: 现有GNN方法通常依赖单一PPI网络，忽略了其他生物网络的互补信息；而特征级融合方法假设网络间基因关系一致，可能忽略网络异质性并引入冲突信息。

Method: SEFGNN将每个生物网络视为独立证据源，在决策级使用Dempster-Shafer理论进行不确定性感知融合，并引入软证据平滑模块缓解过度自信风险。

Result: 在三个癌症数据集上的实验表明，SEFGNN持续优于最先进的基线方法，并在发现新癌症驱动基因方面展现出强大潜力。

Conclusion: 决策级融合方法能有效利用多个生物网络的互补信息，同时处理网络异质性，为癌症驱动基因识别提供了更可靠的解决方案。

Abstract: Identifying cancer driver genes (CDGs) is essential for understanding cancer
mechanisms and developing targeted therapies. Graph neural networks (GNNs) have
recently been employed to identify CDGs by capturing patterns in biological
interaction networks. However, most GNN-based approaches rely on a single
protein-protein interaction (PPI) network, ignoring complementary information
from other biological networks. Some studies integrate multiple networks by
aligning features with consistency constraints to learn unified gene
representations for CDG identification. However, such representation-level
fusion often assumes congruent gene relationships across networks, which may
overlook network heterogeneity and introduce conflicting information. To
address this, we propose Soft-Evidence Fusion Graph Neural Network (SEFGNN), a
novel framework for CDG identification across multiple networks at the decision
level. Instead of enforcing feature-level consistency, SEFGNN treats each
biological network as an independent evidence source and performs
uncertainty-aware fusion at the decision level using Dempster-Shafer Theory
(DST). To alleviate the risk of overconfidence from DST, we further introduce a
Soft Evidence Smoothing (SES) module that improves ranking stability while
preserving discriminative performance. Experiments on three cancer datasets
show that SEFGNN consistently outperforms state-of-the-art baselines and
exhibits strong potential in discovering novel CDGs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [185] [Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications](https://arxiv.org/abs/2510.07077)
*Kento Kawaharazuka,Jihoon Oh,Jun Yamada,Ingmar Posner,Yuke Zhu*

Main category: cs.RO

TL;DR: 这篇论文对视觉-语言-动作(VLA)模型进行了全面的综述，涵盖了从软件到硬件的全栈系统，旨在为机器人社区提供实际应用指导。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和视觉语言模型在机器人领域的应用日益增多，VLA模型通过统一视觉、语言和动作数据，旨在学习能够泛化到多样化任务、对象、实体和环境中的策略。

Method: 提供了系统性的VLA综述，包括策略和架构转变、架构与构建模块、模态特定处理技术、学习范式，以及机器人平台、数据收集策略、数据集、数据增强方法和评估基准。

Result: 建立了全面的VLA系统分析框架，整合了软件和硬件组件，为机器人社区提供了实用的部署指南。

Conclusion: VLA模型有望使机器人能够以最少或无需额外任务特定数据来解决新颖的下游任务，促进更灵活和可扩展的实际部署。

Abstract: Amid growing efforts to leverage advances in large language models (LLMs) and
vision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models
have recently gained significant attention. By unifying vision, language, and
action data at scale, which have traditionally been studied separately, VLA
models aim to learn policies that generalise across diverse tasks, objects,
embodiments, and environments. This generalisation capability is expected to
enable robots to solve novel downstream tasks with minimal or no additional
task-specific data, facilitating more flexible and scalable real-world
deployment. Unlike previous surveys that focus narrowly on action
representations or high-level model architectures, this work offers a
comprehensive, full-stack review, integrating both software and hardware
components of VLA systems. In particular, this paper provides a systematic
review of VLAs, covering their strategy and architectural transition,
architectures and building blocks, modality-specific processing techniques, and
learning paradigms. In addition, to support the deployment of VLAs in
real-world robotic applications, we also review commonly used robot platforms,
data collection strategies, publicly available datasets, data augmentation
methods, and evaluation benchmarks. Throughout this comprehensive survey, this
paper aims to offer practical guidance for the robotics community in applying
VLAs to real-world robotic systems. All references categorized by training
approach, evaluation method, modality, and dataset are available in the table
on our project website: https://vla-survey.github.io .

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [186] [Parameterized Complexity of s-Club Cluster Edge Deletion](https://arxiv.org/abs/2510.07065)
*Ajinkya Gaikwad*

Main category: cs.DM

TL;DR: 该论文研究了s-Club Cluster Edge Deletion问题的参数化复杂度，证明了该问题在路径宽度参数化下是W[1]-难的，同时在邻域多样性、双覆盖等参数下是FPT可解的，并设计了FPT双准则近似算法。


<details>
  <summary>Details</summary>
Motivation: 研究s-Club Cluster Edge Deletion问题的参数化复杂度，特别是解决Montecchiani等人提出的开放问题：该问题是否仅通过树宽度参数化就是FPT可解的。

Method: 使用参数化复杂度理论中的归约技术证明W[1]-难度，并针对不同参数设计FPT算法和近似算法。

Result: 证明了该问题在路径宽度参数化下是W[1]-难的，在邻域多样性、双覆盖等参数下是FPT可解的，并设计了FPT双准则近似算法。

Conclusion: 该问题在树宽度单独参数化下不是FPT可解的，但在其他结构参数下可获得FPT算法，为距离有界图修改问题提供了全面的复杂度分析。

Abstract: We study the parameterized and classical complexity of the s-Club Cluster
Edge Deletion problem: given a graph G = (V, E) and integers k and s, determine
whether it is possible to delete at most k edges so that every connected
component of the resulting graph has diameter at most s. This problem
generalizes Cluster Edge Deletion (the case s = 1) and captures a variety of
distance-bounded graph modification tasks.
  Montecchiani, Ortali, Piselli, and Tappini (Information and Computation,
2023) showed that the problem is fixed-parameter tractable when parameterized
by s plus the treewidth of G, and asked whether the dependence on s is
necessary; that is, whether the problem is FPT when parameterized by treewidth
alone. We resolve this by proving that the problem is W[1]-hard when
parameterized by pathwidth, and hence by treewidth.
  On the algorithmic side, we show that the problem is FPT when parameterized
by neighborhood diversity, twin cover, or cluster vertex deletion number,
thereby extending to all s >= 1 the results of Italiano, Konstantinidis, and
Papadopoulos (Algorithmica, 2023), who established FPT algorithms for the case
s = 1 under the neighborhood diversity and twin cover parameters.
  From a classical perspective, we prove that the problem is NP-hard on split
graphs already for s = 2, complementing the polynomial-time solvability for s =
1 due to Bonomo, Duran, and Valencia-Pabon (Theoretical Computer Science, 2015)
and the trivial case s = 3.
  Finally, while the problem is FPT when parameterized by s + k, its complexity
for the solution size k alone remains open. We make progress on this front by
designing an FPT bicriteria approximation algorithm, which runs in time f(k,
1/epsilon) * n^{O(1)} and, for graphs excluding long induced cycles, outputs a
solution of size at most k whose connected components have diameter at most (1
+ epsilon) * s.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [187] [From Neural Sensing to Stimulation: An Interdisciplinary Roadmap for Neurotechnology](https://arxiv.org/abs/2510.07116)
*Ruben Ruiz-Mateos Serrano,Joe G Troughton,Nima Mirkhani,Natalia Martinez,Massimo Mariello,Jordan Tsigarides,Simon Williamson,Juan Sapriza,Ioana Susnoschi Luca,Antonio Dominguez-Alfaro,Estelle Cuttaz,Nicole Thompson,Sydney Swedick,Latifah Almulla,Amparo Guemes*

Main category: cs.ET

TL;DR: 这篇论文提出了神经技术发展的战略路线图，重点关注功能、可扩展性、适应性和可转化性之间的权衡，旨在加速公平有效的自适应神经技术发展。


<details>
  <summary>Details</summary>
Motivation: 神经技术具有改变临床和非临床领域的潜力，但面临跨学科复杂挑战，需要协调技术发展与转化需求。

Method: 提出了一个统一的协作创新和教育框架，识别了五个跨领域权衡，并说明了技术领域如何影响其解决方案。

Result: 制定了克服关键瓶颈的时间表，强调了伦理和监管优先事项，为全球研究和创新社区提供协调努力的指导。

Conclusion: 通过协调技术发展与转化和社会需求，该路线图旨在加速公平、有效和面向未来的自适应神经技术发展。

Abstract: Neurotechnologies are transforming how we measure, interpret, and modulate
brain-body interactions, integrating real-time sensing, computation, and
stimulation to enable precise physiological control. They hold transformative
potential across clinical and non-clinical domains, from treating disorders to
enhancing cognition and performance. Realizing this potential requires
navigating complex, interdisciplinary challenges spanning neuroscience,
materials science, device engineering, signal processing, computational
modelling, and regulatory and ethical frameworks. This Perspective presents a
strategic roadmap for neurotechnology development, created by early-career
researchers, highlighting their role at the intersection of disciplines and
their capacity to bridge traditional silos. We identify five cross-cutting
trade-offs that constrain progress across functionality, scalability,
adaptability, and translatability, and illustrate how technical domains
influence their resolution. Rather than a domain-specific review, we focus on
shared challenges and strategic opportunities that transcend disciplines. We
propose a unified framework for collaborative innovation and education,
highlight ethical and regulatory priorities, and outline a timeline for
overcoming key bottlenecks. By aligning technical development with
translational and societal needs, this roadmap aims to accelerate equitable,
effective, and future-ready adaptive neurotechnologies, guiding coordinated
efforts across the global research and innovation community.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [188] [From Description to Detection: LLM based Extendable O-RAN Compliant Blind DoS Detection in 5G and Beyond](https://arxiv.org/abs/2510.06530)
*Thusitha Dayaratne,Ngoc Duy Pham,Viet Vo,Shangqi Lai,Sharif Abuadbba,Hajime Suzuki,Xingliang Yuan,Carsten Rudolph*

Main category: cs.CR

TL;DR: 提出了一种基于大语言模型的零样本异常检测框架，用于检测5G网络中的控制平面协议攻击，在O-RAN架构下实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 5G网络中RRC和NAS等控制平面协议存在安全漏洞，传统基于规则或机器学习的检测方法需要大量训练数据、预定义规则且可解释性有限。

Method: 利用大语言模型的零样本能力，结合无序数据和简短自然语言攻击描述，在O-RAN架构中实现异常检测。

Result: 验证了框架对提示变化的鲁棒性，展示了攻击描述自动化的可行性，检测质量依赖于描述的语义完整性而非措辞或长度，在RRC/NAS数据集上表现优异。

Conclusion: 该框架在O-RAN实时约束下具有实用性，能够检测其他Layer-3攻击，展示了基于LLM的零样本检测方法的潜力。

Abstract: The quality and experience of mobile communication have significantly
improved with the introduction of 5G, and these improvements are expected to
continue beyond the 5G era. However, vulnerabilities in control-plane
protocols, such as Radio Resource Control (RRC) and Non-Access Stratum (NAS),
pose significant security threats, such as Blind Denial of Service (DoS)
attacks. Despite the availability of existing anomaly detection methods that
leverage rule-based systems or traditional machine learning methods, these
methods have several limitations, including the need for extensive training
data, predefined rules, and limited explainability. Addressing these
challenges, we propose a novel anomaly detection framework that leverages the
capabilities of Large Language Models (LLMs) in zero-shot mode with unordered
data and short natural language attack descriptions within the Open Radio
Access Network (O-RAN) architecture. We analyse robustness to prompt variation,
demonstrate the practicality of automating the attack descriptions and show
that detection quality relies on the semantic completeness of the description
rather than its phrasing or length. We utilise an RRC/NAS dataset to evaluate
the solution and provide an extensive comparison of open-source and proprietary
LLM implementations to demonstrate superior performance in attack detection. We
further validate the practicality of our framework within O-RAN's real-time
constraints, illustrating its potential for detecting other Layer-3 attacks.

</details>


### [189] [GNN-enhanced Traffic Anomaly Detection for Next-Generation SDN-Enabled Consumer Electronics](https://arxiv.org/abs/2510.07109)
*Guan-Yan Yang,Farn Wang,Kuo-Hui Yeh*

Main category: cs.CR

TL;DR: 提出了一种基于图神经网络的可扩展网络异常检测框架GNN-NAD，融合SDN和CFN技术，用于保护物联网消费电子设备免受DDoS和网络攻击。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习异常检测系统在传统网络中准确率高，但过于复杂且依赖静态基础设施，需要手动配置管理，无法满足下一代消费电子网络的安全需求。

Method: 整合SDN和CFN构建可扩展网络模型，提出GNN-NAD框架，将静态漏洞感知攻击图与动态流量特征融合，使用GSAGE图神经网络进行表示学习，再用随机森林分类器检测异常。

Result: 在消费电子环境实验中，GNN-NAD在准确率、召回率、精确率和F1分数等指标上表现优异，即使在小样本情况下也优于现有网络异常检测方法。

Conclusion: 该工作提升了下一代智能消费电子网络的安全性和效率，为物联网设备安全提供了有效解决方案。

Abstract: Consumer electronics (CE) connected to the Internet of Things are susceptible
to various attacks, including DDoS and web-based threats, which can compromise
their functionality and facilitate remote hijacking. These vulnerabilities
allow attackers to exploit CE for broader system attacks while enabling the
propagation of malicious code across the CE network, resulting in device
failures. Existing deep learning-based traffic anomaly detection systems
exhibit high accuracy in traditional network environments but are often overly
complex and reliant on static infrastructure, necessitating manual
configuration and management. To address these limitations, we propose a
scalable network model that integrates Software-defined Networking (SDN) and
Compute First Networking (CFN) for next-generation CE networks. In this network
model, we propose a Graph Neural Networks-based Network Anomaly Detection
framework (GNN-NAD) that integrates SDN-based CE networks and enables the CFN
architecture. GNN-NAD uniquely fuses a static, vulnerability-aware attack graph
with dynamic traffic features, providing a holistic view of network security.
The core of the framework is a GNN model (GSAGE) for graph representation
learning, followed by a Random Forest (RF) classifier. This design (GSAGE+RF)
demonstrates superior performance compared to existing feature selection
methods. Experimental evaluations on CE environment reveal that GNN-NAD
achieves superior metrics in accuracy, recall, precision, and F1 score, even
with small sample sizes, exceeding the performance of current network anomaly
detection methods. This work advances the security and efficiency of
next-generation intelligent CE networks.

</details>


### [190] [Enhancing Resilience for IoE: A Perspective of Networking-Level Safeguard](https://arxiv.org/abs/2508.20504)
*Guan-Yan Yang,Jui-Ning Chen,Farn Wang,Kuo-Hui Yeh*

Main category: cs.CR

TL;DR: 提出基于图结构学习（GSL）的防护框架，通过联合优化图拓扑和节点表示来抵御能源物联网中的对抗性网络攻击，相比传统方法具有更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 能源物联网（IoE）的互联性使关键基础设施面临复杂网络威胁，这些威胁具有更高的公共安全风险，需要比传统IoT更强的防护方案。

Method: 采用图结构学习（GSL）方法，联合优化图拓扑结构和节点表示，从网络层面构建对抗性网络模型操纵的固有防护机制。

Result: 通过概念概述、架构讨论和安全数据集案例研究，证明GSL方法在鲁棒性方面优于代表性方法，为保护IoE网络提供了可行路径。

Conclusion: GSL具有增强未来IoE网络弹性和可靠性的潜力，同时指出了该新兴研究领域的关键开放挑战和未来研究方向。

Abstract: The Internet of Energy (IoE) integrates IoT-driven digital communication with
power grids to enable efficient and sustainable energy systems. Still, its
interconnectivity exposes critical infrastructure to sophisticated cyber
threats, including adversarial attacks designed to bypass traditional
safeguards. Unlike general IoT risks, IoE threats have heightened public safety
consequences, demanding resilient solutions. From the networking-level
safeguard perspective, we propose a Graph Structure Learning (GSL)-based
safeguards framework that jointly optimizes graph topology and node
representations to resist adversarial network model manipulation inherently.
Through a conceptual overview, architectural discussion, and case study on a
security dataset, we demonstrate GSL's superior robustness over representative
methods, offering practitioners a viable path to secure IoE networks against
evolving attacks. This work highlights the potential of GSL to enhance the
resilience and reliability of future IoE networks for practitioners managing
critical infrastructure. Lastly, we identify key open challenges and propose
future research directions in this novel research area.

</details>


### [191] [Unsupervised Backdoor Detection and Mitigation for Spiking Neural Networks](https://arxiv.org/abs/2510.06629)
*Jiachen Li,Bang Wu,Xiaoyu Xia,Xiaoning Liu,Xun Yi,Xiuzhen Zhang*

Main category: cs.CR

TL;DR: 本文提出了一种针对脉冲神经网络（SNNs）后门攻击的无监督检测框架TMPBD和缓解机制NDSBM，解决了传统防御方法在SNNs中效果不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络因其高能效而受到关注，但其安全方面特别是后门攻击防御研究有限。现有ANN防御方法由于SNNs的事件驱动和时间依赖性特征而表现不佳或容易被绕过。

Method: 提出TMPBD框架，利用最终脉冲层中时间膜电位的最大边际统计来检测目标标签，无需攻击知识或数据访问；并引入NDSBM缓解机制，通过钳制早期卷积层之间的树突连接来抑制恶意神经元。

Result: 在多个神经形态基准测试和最先进的输入感知动态触发攻击上的实验表明，TMPBD达到100%检测准确率，NDSBM将攻击成功率从100%降低到8.44%，结合检测时进一步降至2.81%，且不降低干净准确率。

Conclusion: 该研究为SNNs提供了有效的后门攻击检测和缓解解决方案，克服了传统防御方法的局限性，在保持模型性能的同时显著提升了安全性。

Abstract: Spiking Neural Networks (SNNs) have gained increasing attention for their
superior energy efficiency compared to Artificial Neural Networks (ANNs).
However, their security aspects, particularly under backdoor attacks, have
received limited attention. Existing defense methods developed for ANNs perform
poorly or can be easily bypassed in SNNs due to their event-driven and temporal
dependencies. This paper identifies the key blockers that hinder traditional
backdoor defenses in SNNs and proposes an unsupervised post-training detection
framework, Temporal Membrane Potential Backdoor Detection (TMPBD), to overcome
these challenges. TMPBD leverages the maximum margin statistics of temporal
membrane potential (TMP) in the final spiking layer to detect target labels
without any attack knowledge or data access. We further introduce a robust
mitigation mechanism, Neural Dendrites Suppression Backdoor Mitigation (NDSBM),
which clamps dendritic connections between early convolutional layers to
suppress malicious neurons while preserving benign behaviors, guided by TMP
extracted from a small, clean, unlabeled dataset. Extensive experiments on
multiple neuromorphic benchmarks and state-of-the-art input-aware dynamic
trigger attacks demonstrate that TMPBD achieves 100% detection accuracy, while
NDSBM reduces the attack success rate from 100% to 8.44%, and to 2.81% when
combined with detection, without degrading clean accuracy.

</details>


### [192] [Differentially Private Synthetic Text Generation for Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2510.06719)
*Junki Mori,Kazuya Kakizaki,Taiki Miyagawa,Jun Sakuma*

Main category: cs.CR

TL;DR: DP-SynRAG是一个隐私保护的RAG框架，通过生成差分隐私合成数据库来避免重复噪声注入，在固定隐私预算下实现优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有私有RAG方法依赖查询时差分隐私，需要重复注入噪声导致隐私损失累积，限制了在敏感领域的应用。

Method: 使用LLMs生成差分隐私合成RAG数据库，扩展私有预测方法，让LLMs以差分隐私方式生成模拟子采样数据库记录的文本。

Result: 实验显示DP-SynRAG在保持固定隐私预算的同时，性能优于最先进的私有RAG系统。

Conclusion: DP-SynRAG为隐私保护RAG提供了可扩展的解决方案，通过一次性生成可重用的合成文本避免了重复隐私成本。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
grounding them in external knowledge. However, its application in sensitive
domains is limited by privacy risks. Existing private RAG methods typically
rely on query-time differential privacy (DP), which requires repeated noise
injection and leads to accumulated privacy loss. To address this issue, we
propose DP-SynRAG, a framework that uses LLMs to generate differentially
private synthetic RAG databases. Unlike prior methods, the synthetic text can
be reused once created, thereby avoiding repeated noise injection and
additional privacy costs. To preserve essential information for downstream RAG
tasks, DP-SynRAG extends private prediction, which instructs LLMs to generate
text that mimics subsampled database records in a DP manner. Experiments show
that DP-SynRAG achieves superior performanec to the state-of-the-art private
RAG systems while maintaining a fixed privacy budget, offering a scalable
solution for privacy-preserving RAG.

</details>


### [193] [Pseudo-MDPs: A Novel Framework for Efficiently Optimizing Last Revealer Seed Manipulations in Blockchains](https://arxiv.org/abs/2510.07080)
*Maxime Reynouard*

Main category: cs.CR

TL;DR: 该研究针对一类受限的马尔可夫决策过程（MDP）问题，提出了伪MDP（pMDP）框架，通过两种问题约简方法显著降低了计算复杂度，特别应用于解决以太坊区块链中的最后揭示者攻击（LRA）问题。


<details>
  <summary>Details</summary>
Motivation: 解决MDP计算复杂性的挑战，特别是针对以太坊等PoS区块链中的最后揭示者攻击（LRA）问题，该攻击会破坏公平性，涉及4000亿美元市值的系统安全。

Method: 引入伪MDP（pMDP）框架，提出两种不同的标准MDP问题约简方法，其中一种提供了反直觉的新视角，结合两种约简方法显著改进了价值迭代等动态规划算法。

Result: 对于参数为κ的LRA问题（以太坊中κ=325），计算复杂度从O(2^κ κ^{2^{κ+2}})降低到O(κ^4)（每次迭代），同时保证指数级快速收敛到最优解。

Conclusion: 该框架不仅有效解决了大规模MDP问题，还简化了策略提取，适用于资源受限的代理，并通过两个案例研究验证了其有效性，推动了MDP研究和区块链安全漏洞理解。

Abstract: This study tackles the computational challenges of solving Markov Decision
Processes (MDPs) for a restricted class of problems. It is motivated by the
Last Revealer Attack (LRA), which undermines fairness in some Proof-of-Stake
(PoS) blockchains such as Ethereum (\$400B market capitalization). We introduce
pseudo-MDPs (pMDPs) a framework that naturally models such problems and propose
two distinct problem reductions to standard MDPs. One problem reduction
provides a novel, counter-intuitive perspective, and combining the two problem
reductions enables significant improvements in dynamic programming algorithms
such as value iteration. In the case of the LRA which size is parameterized by
$\kappa$ (in Ethereum's case $\kappa$= 325), we reduce the computational
complexity from $O(2^\kappa \kappa^{2^{\kappa+2}})$ to $O(\kappa^4)$ (per
iteration). This solution also provide the usual benefits from Dynamic
Programming solutions: exponentially fast convergence toward the optimal
solution is guaranteed. The dual perspective also simplifies policy extraction,
making the approach well-suited for resource-constrained agents who can operate
with very limited memory and computation once the problem has been solved.
Furthermore, we generalize those results to a broader class of MDPs, enhancing
their applicability. The framework is validated through two case studies: a
fictional card game and the LRA on the Ethereum random seed consensus protocol.
These applications demonstrate the framework's ability to solve large-scale
problems effectively while offering actionable insights into optimal
strategies. This work advances the study of MDPs and contributes to
understanding security vulnerabilities in blockchain systems.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [194] [Active Control of Turbulent Airfoil Flows Using Adjoint-based Deep Learning](https://arxiv.org/abs/2510.07106)
*Xuemin Liu,Tom Hickling,Jonathan F. MacArt*

Main category: physics.flu-dyn

TL;DR: 使用深度学习PDE增强方法训练主动神经网络流动控制器，在湍流翼型流动中优化升阻比，通过吹吸射流控制显著改善气动性能。


<details>
  <summary>Details</summary>
Motivation: 开发能够自适应响应非定常流动条件的传感器驱动控制策略，提高翼型在湍流中的升阻比和能量效率。

Method: 采用直接数值模拟和大涡模拟建模可压缩流动，使用神经网络将局部压力测量映射到最优射流总压力，通过伴随Navier-Stokes方程计算流动对神经网络参数的敏感性。

Result: 训练后的流动控制器显著提高了二维和三维翼型流动的升阻比，减少了流动分离，特别是在5°和10°攻角下表现优异。二维训练模型在三维流动中仍保持有效。

Conclusion: 这种基于学习的方法在改善气动性能方面非常有效，伴随训练的控制方法具有鲁棒性，三维训练模型能更有效地捕捉流动动力学。

Abstract: We train active neural-network flow controllers using a deep learning PDE
augmentation method to optimize lift-to-drag ratios in turbulent airfoil flows
at Reynolds number $5\times10^4$ and Mach number 0.4. Direct numerical
simulation and large eddy simulation are employed to model compressible,
unconfined flow over two- and three-dimensional semi-infinite NACA 0012
airfoils at angles of attack $\alpha = 5^\circ$, $10^\circ$, and $15^\circ$.
Control actions, implemented through a blowing/suction jet at a fixed location
and geometry on the upper surface, are adaptively determined by a neural
network that maps local pressure measurements to optimal jet total pressure,
enabling a sensor-informed control policy that responds spatially and
temporally to unsteady flow conditions. The sensitivities of the flow to the
neural network parameters are computed using the adjoint Navier-Stokes
equations, which we construct using automatic differentiation applied to the
flow solver. The trained flow controllers significantly improve the
lift-to-drag ratios and reduce flow separation for both two- and
three-dimensional airfoil flows, especially at $\alpha = 5^\circ$ and
$10^\circ$. The 2D-trained models remain effective when applied out-of-sample
to 3D flows, which demonstrates the robustness of the adjoint-trained control
approach. The 3D-trained models capture the flow dynamics even more
effectively, which leads to better energy efficiency and comparable performance
for both adaptive (neural network) and offline (simplified, constant-pressure)
controllers. These results underscore the effectiveness of this learning-based
approach in improving aerodynamic performance.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [195] [Validation of Various Normalization Methods for Brain Tumor Segmentation: Can Federated Learning Overcome This Heterogeneity?](https://arxiv.org/abs/2510.07126)
*Jan Fiszer,Dominika Ciupek,Maciej Malawski*

Main category: cs.CV

TL;DR: 该研究探讨了在非独立同分布数据条件下，联邦学习在脑肿瘤分割任务中的表现。通过应用不同的MRI强度归一化技术模拟数据异质性，发现联邦学习方法对客户端间不一致的归一化数据具有弹性，达到与集中式模型相当的92% Dice分数。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学影像中应用广泛，但需要大量数据，这引发了数据隐私、存储和传输的挑战。联邦学习可以解决这些问题，但在处理非独立同分布数据时效果可能降低。

Method: 通过应用不同的MRI强度归一化技术到单独的数据子集来模拟非独立同分布条件，然后使用这些子集进行脑肿瘤分割模型的训练和测试。

Result: 联邦学习方法对客户端间不一致归一化数据表现出弹性，实现了92%的3D Dice分数，与使用所有数据的集中式模型性能相当。

Conclusion: 联邦学习是在不违反数据隐私的情况下有效训练高性能模型的解决方案，这对医学应用至关重要。

Abstract: Deep learning (DL) has been increasingly applied in medical imaging, however,
it requires large amounts of data, which raises many challenges related to data
privacy, storage, and transfer. Federated learning (FL) is a training paradigm
that overcomes these issues, though its effectiveness may be reduced when
dealing with non-independent and identically distributed (non-IID) data. This
study simulates non-IID conditions by applying different MRI intensity
normalization techniques to separate data subsets, reflecting a common cause of
heterogeneity. These subsets are then used for training and testing models for
brain tumor segmentation. The findings provide insights into the influence of
the MRI intensity normalization methods on segmentation models, both training
and inference. Notably, the FL methods demonstrated resilience to
inconsistently normalized data across clients, achieving the 3D Dice score of
92%, which is comparable to a centralized model (trained using all data). These
results indicate that FL is a solution to effectively train high-performing
models without violating data privacy, a crucial concern in medical
applications. The code is available at:
https://github.com/SanoScience/fl-varying-normalization.

</details>


### [196] [Milestone Determination for Autonomous Railway Operation](https://arxiv.org/abs/2510.06229)
*Josh Hunter,John McDermid,Simon Burton,Poppy Fynes,Mia Dempster*

Main category: cs.CV

TL;DR: 提出了基于里程碑决策的铁路自动化视觉系统，通过关注路线特定上下文线索生成序列数据集，简化学习过程。


<details>
  <summary>Details</summary>
Motivation: 传统铁路视觉系统缺乏高质量序列数据和时空上下文，现有方案存在真实性和适用性问题，需要更贴近实际运营逻辑的解决方案。

Method: 采用里程碑确定概念，开发基于规则的目标模型，专注于路线上的关键决策点而非动态组件的通用识别。

Result: 生成了丰富、符合实际运营逻辑的序列数据集，为可控可预测环境中的视觉代理训练提供了实用框架。

Conclusion: 该方法为铁路自动化机器学习系统提供了更安全高效的训练框架，通过简化学习过程提升了系统实用性。

Abstract: In the field of railway automation, one of the key challenges has been the
development of effective computer vision systems due to the limited
availability of high-quality, sequential data. Traditional datasets are
restricted in scope, lacking the spatio temporal context necessary for
real-time decision-making, while alternative solutions introduce issues related
to realism and applicability. By focusing on route-specific, contextually
relevant cues, we can generate rich, sequential datasets that align more
closely with real-world operational logic. The concept of milestone
determination allows for the development of targeted, rule-based models that
simplify the learning process by eliminating the need for generalized
recognition of dynamic components, focusing instead on the critical decision
points along a route. We argue that this approach provides a practical
framework for training vision agents in controlled, predictable environments,
facilitating safer and more efficient machine learning systems for railway
automation.

</details>


### [197] [Uncertainty Quantification In Surface Landmines and UXO Classification Using MC Dropout](https://arxiv.org/abs/2510.06238)
*Sagar Lekhak,Emmett J. Ientilucci,Dimah Dera,Susmita Ghosh*

Main category: cs.CV

TL;DR: 该研究将蒙特卡洛Dropout集成到微调的ResNet-50架构中，用于地表地雷和未爆弹药分类，通过不确定性量化提高预测可靠性。


<details>
  <summary>Details</summary>
Motivation: 确定性神经网络在嘈杂条件和对抗性攻击下容易出错，可能导致漏检或误分类，这在人道主义扫雷行动中具有严重风险。

Method: 在微调的ResNet-50架构中集成蒙特卡洛Dropout方法，用于量化认知不确定性，并在模拟数据集上进行测试。

Result: 在干净、对抗性扰动和嘈杂测试图像上的实验结果表明，该模型能够在挑战性条件下标记不可靠的预测。

Conclusion: 这项概念验证研究强调了扫雷中不确定性量化的重要性，提高了对现有神经网络在扫雷中对抗性威胁脆弱性的认识，并强调了为实际应用开发更鲁棒可靠模型的重要性。

Abstract: Detecting surface landmines and unexploded ordnances (UXOs) using deep
learning has shown promise in humanitarian demining. However, deterministic
neural networks can be vulnerable to noisy conditions and adversarial attacks,
leading to missed detection or misclassification. This study introduces the
idea of uncertainty quantification through Monte Carlo (MC) Dropout, integrated
into a fine-tuned ResNet-50 architecture for surface landmine and UXO
classification, which was tested on a simulated dataset. Integrating the MC
Dropout approach helps quantify epistemic uncertainty, providing an additional
metric for prediction reliability, which could be helpful to make more informed
decisions in demining operations. Experimental results on clean, adversarially
perturbed, and noisy test images demonstrate the model's ability to flag
unreliable predictions under challenging conditions. This proof-of-concept
study highlights the need for uncertainty quantification in demining, raises
awareness about the vulnerability of existing neural networks in demining to
adversarial threats, and emphasizes the importance of developing more robust
and reliable models for practical applications.

</details>


### [198] [Ensemble Deep Learning and LLM-Assisted Reporting for Automated Skin Lesion Diagnosis](https://arxiv.org/abs/2510.06260)
*Sher Khan,Raz Muhammad,Adil Hussain,Muhammad Sajjad,Muhammad Rashid*

Main category: cs.CV

TL;DR: 提出一个统一的皮肤病AI诊断框架，通过异构神经网络集成和语言模型集成，提高诊断可靠性并生成临床报告。


<details>
  <summary>Details</summary>
Motivation: 解决现有皮肤病AI系统存在的架构同质化、数据集偏见、以及NLP与诊断流程分离的问题，弥合临床应用的转化差距。

Method: 1) 异构卷积神经网络集成，提供互补诊断视角和不确定性机制；2) 将大语言模型直接嵌入诊断流程，生成结构化临床报告。

Result: 开发了一个综合系统，既能提高诊断精度，又能生成包含病灶特征、诊断推理和监测指导的临床报告。

Conclusion: 该框架代表了可部署皮肤病AI的重要进展，通过增强诊断精度和持续护理支持，最终提高皮肤病变的早期干预率。

Abstract: Cutaneous malignancies demand early detection for favorable outcomes, yet
current diagnostics suffer from inter-observer variability and access
disparities. While AI shows promise, existing dermatological systems are
limited by homogeneous architectures, dataset biases across skin tones, and
fragmented approaches that treat natural language processing as separate
post-hoc explanations rather than integral to clinical decision-making. We
introduce a unified framework that fundamentally reimagines AI integration for
dermatological diagnostics through two synergistic innovations. First, a
purposefully heterogeneous ensemble of architecturally diverse convolutional
neural networks provides complementary diagnostic perspectives, with an
intrinsic uncertainty mechanism flagging discordant cases for specialist review
-- mimicking clinical best practices. Second, we embed large language model
capabilities directly into the diagnostic workflow, transforming classification
outputs into clinically meaningful assessments that simultaneously fulfill
medical documentation requirements and deliver patient-centered education. This
seamless integration generates structured reports featuring precise lesion
characterization, accessible diagnostic reasoning, and actionable monitoring
guidance -- empowering patients to recognize early warning signs between
visits. By addressing both diagnostic reliability and communication barriers
within a single cohesive system, our approach bridges the critical
translational gap that has prevented previous AI implementations from achieving
clinical impact. The framework represents a significant advancement toward
deployable dermatological AI that enhances diagnostic precision while actively
supporting the continuum of care from initial detection through patient
education, ultimately improving early intervention rates for skin lesions.

</details>


### [199] [Vision Transformer for Transient Noise Classification](https://arxiv.org/abs/2510.06273)
*Divyansh Srivastava,Andrzej Niedzielski*

Main category: cs.CV

TL;DR: 使用Vision Transformer模型对LIGO数据中的瞬态噪声进行22个现有类别和2个新增类别的分类，达到92.26%的分类效率


<details>
  <summary>Details</summary>
Motivation: LIGO数据中的瞬态噪声（毛刺）会阻碍引力波的探测，随着O3运行增加了两个新的噪声类别，需要训练新模型进行有效分类

Method: 在包含Gravity Spy数据集和LIGO O3a运行中两个新增类别的组合数据集上，训练预训练的Vision Transformer（ViT-B/32）模型

Result: 实现了92.26%的分类效率

Conclusion: Vision Transformer具有通过有效区分瞬态噪声来提高引力波探测准确性的潜力

Abstract: Transient noise (glitches) in LIGO data hinders the detection of
gravitational waves (GW). The Gravity Spy project has categorized these noise
events into various classes. With the O3 run, there is the inclusion of two
additional noise classes and thus a need to train new models for effective
classification. We aim to classify glitches in LIGO data into 22 existing
classes from the first run plus 2 additional noise classes from O3a using the
Vision Transformer (ViT) model. We train a pre-trained Vision Transformer
(ViT-B/32) model on a combined dataset consisting of the Gravity Spy dataset
with the additional two classes from the LIGO O3a run. We achieve a
classification efficiency of 92.26%, demonstrating the potential of Vision
Transformer to improve the accuracy of gravitational wave detection by
effectively distinguishing transient noise.
  Key words: gravitational waves --vision transformer --machine learning

</details>


### [200] [General and Efficient Visual Goal-Conditioned Reinforcement Learning using Object-Agnostic Masks](https://arxiv.org/abs/2510.06277)
*Fahim Shahriar,Cheryl Wang,Alireza Azimi,Gautham Vasan,Hany Hamed Elanwar,A. Rupam Mahmood,Colin Bellinger*

Main category: cs.CV

TL;DR: 提出基于掩码的目标表示方法，通过对象无关的视觉线索实现高效学习和泛化，解决了现有方法泛化差、收敛慢等问题。


<details>
  <summary>Details</summary>
Motivation: 现有目标表示方法（如目标状态图像、3D坐标、one-hot向量）存在泛化能力差、收敛速度慢、需要特殊摄像头等问题，需要更好的目标表示系统。

Method: 使用掩码作为目标表示，提供对象无关的视觉线索，无需位置信息即可生成密集奖励，避免易出错的距离计算。

Result: 在仿真中达到99.9%的到达准确率，能高精度执行拾取任务，并成功实现从零学习和仿真到实物的迁移应用。

Conclusion: 掩码目标表示方法在目标导向强化学习中表现出色，具有高效学习、强泛化能力和实际应用价值。

Abstract: Goal-conditioned reinforcement learning (GCRL) allows agents to learn diverse
objectives using a unified policy. The success of GCRL, however, is contingent
on the choice of goal representation. In this work, we propose a mask-based
goal representation system that provides object-agnostic visual cues to the
agent, enabling efficient learning and superior generalization. In contrast,
existing goal representation methods, such as target state images, 3D
coordinates, and one-hot vectors, face issues of poor generalization to unseen
objects, slow convergence, and the need for special cameras. Masks can be
processed to generate dense rewards without requiring error-prone distance
calculations. Learning with ground truth masks in simulation, we achieved 99.9%
reaching accuracy on training and unseen test objects. Our proposed method can
be utilized to perform pick-up tasks with high accuracy, without using any
positional information of the target. Moreover, we demonstrate learning from
scratch and sim-to-real transfer applications using two different physical
robots, utilizing pretrained open vocabulary object detection models for mask
generation.

</details>


### [201] [Efficient High-Resolution Image Editing with Hallucination-Aware Loss and Adaptive Tiling](https://arxiv.org/abs/2510.06295)
*Young D. Kwon,Abhinav Mehrotra,Malcolm Chadwick,Alberto Gil Ramos,Sourav Bhattacharya*

Main category: cs.CV

TL;DR: MobilePicasso是一个高效的高分辨率图像编辑系统，通过三阶段方法在资源受限设备上实现4K图像编辑，显著提升图像质量、减少幻觉，并大幅降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在资源受限设备上进行高分辨率图像编辑时面临内存和图像质量的挑战，需要开发更高效的解决方案。

Method: 采用三阶段方法：(i) 标准分辨率下使用幻觉感知损失进行图像编辑，(ii) 应用潜在投影避免进入像素空间，(iii) 使用自适应上下文保持分块将编辑后的图像潜在表示上采样到更高分辨率。

Result: 用户研究显示，MobilePicasso比现有方法提升图像质量18-48%，减少幻觉14-51%，延迟降低高达55.8倍，运行时内存仅增加9%，在设备上的运行速度甚至快于A100 GPU上的服务器模型。

Conclusion: MobilePicasso证明了在资源受限设备上实现高效高质量高分辨率图像编辑的可行性，为移动应用提供了实用的解决方案。

Abstract: High-resolution (4K) image-to-image synthesis has become increasingly
important for mobile applications. Existing diffusion models for image editing
face significant challenges, in terms of memory and image quality, when
deployed on resource-constrained devices. In this paper, we present
MobilePicasso, a novel system that enables efficient image editing at high
resolutions, while minimising computational cost and memory usage.
MobilePicasso comprises three stages: (i) performing image editing at a
standard resolution with hallucination-aware loss, (ii) applying latent
projection to overcome going to the pixel space, and (iii) upscaling the edited
image latent to a higher resolution with adaptive context-preserving tiling.
Our user study with 46 participants reveals that MobilePicasso not only
improves image quality by 18-48% but reduces hallucinations by 14-51% over
existing methods. MobilePicasso demonstrates significantly lower latency, e.g.,
up to 55.8$\times$ speed-up, yet with a small increase in runtime memory, e.g.,
a mere 9% increase over prior work. Surprisingly, the on-device runtime of
MobilePicasso is observed to be faster than a server-based high-resolution
image editing model running on an A100 GPU.

</details>


### [202] [Scalable deep fusion of spaceborne lidar and synthetic aperture radar for global forest structural complexity mapping](https://arxiv.org/abs/2510.06299)
*Tiago de Conto,John Armston,Ralph Dubayah*

Main category: cs.CV

TL;DR: 提出了一种融合GEDI激光雷达和SAR数据的深度学习框架，用于生成全球25米分辨率森林结构复杂性连续地图，解决了GEDI稀疏采样限制问题。


<details>
  <summary>Details</summary>
Motivation: GEDI激光雷达能够测量森林结构复杂性，但其稀疏采样限制了连续高分辨率制图。需要融合多源遥感数据来实现全球范围的连续监测。

Method: 采用改进的EfficientNetV2架构，融合GEDI观测数据和多模态SAR数据集，训练超过1.3亿个GEDI足迹，使用不到40万个参数实现高效预测。

Result: 模型在全球范围内达到R²=0.82的高性能，能够准确预测并校准不确定性，保持精细空间模式，生成了2015-2022年全球多时序森林结构复杂性数据集。

Conclusion: 该框架支持全球森林结构动态的连续多时序监测，为生物多样性保护和生态系统管理提供了有效工具，并可通过迁移学习扩展到其他森林结构变量的预测。

Abstract: Forest structural complexity metrics integrate multiple canopy attributes
into a single value that reflects habitat quality and ecosystem function.
Spaceborne lidar from the Global Ecosystem Dynamics Investigation (GEDI) has
enabled mapping of structural complexity in temperate and tropical forests, but
its sparse sampling limits continuous high-resolution mapping. We present a
scalable, deep learning framework fusing GEDI observations with multimodal
Synthetic Aperture Radar (SAR) datasets to produce global, high-resolution (25
m) wall-to-wall maps of forest structural complexity. Our adapted
EfficientNetV2 architecture, trained on over 130 million GEDI footprints,
achieves high performance (global R2 = 0.82) with fewer than 400,000
parameters, making it an accessible tool that enables researchers to process
datasets at any scale without requiring specialized computing infrastructure.
The model produces accurate predictions with calibrated uncertainty estimates
across biomes and time periods, preserving fine-scale spatial patterns. It has
been used to generate a global, multi-temporal dataset of forest structural
complexity from 2015 to 2022. Through transfer learning, this framework can be
extended to predict additional forest structural variables with minimal
computational cost. This approach supports continuous, multi-temporal
monitoring of global forest structural dynamics and provides tools for
biodiversity conservation and ecosystem management efforts in a changing
climate.

</details>


### [203] [TransFIRA: Transfer Learning for Face Image Recognizability Assessment](https://arxiv.org/abs/2510.06353)
*Allen Tu,Kartik Narayan,Joshua Gleason,Jennifer Xu,Matthew Meyn,Tom Goldstein,Vishal M. Patel*

Main category: cs.CV

TL;DR: TransFIRA是一个轻量级、无需标注的人脸图像可识别性评估框架，通过嵌入空间中的类中心相似性和类中心角度分离来定义可识别性，实现了最先进的验证精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决在无约束环境中人脸识别面临的姿态、模糊、光照和遮挡等极端变化问题，传统视觉质量指标无法预测输入是否真正可被编码器识别。

Method: 使用类中心相似性(CCS)和类中心角度分离(CCAS)定义可识别性，提出可识别性感知的聚合策略，无需外部标签、启发式方法或骨干网络特定训练。

Result: 在BRIAR和IJB-C数据集上达到最先进的验证精度，与真实可识别性的相关性几乎翻倍，在人体识别上也表现出色，具有跨数据集鲁棒性。

Conclusion: TransFIRA建立了一个统一的、几何驱动的可识别性评估框架，在准确性、可解释性和跨模态扩展性方面显著推进了FIQA领域。

Abstract: Face recognition in unconstrained environments such as surveillance, video,
and web imagery must contend with extreme variation in pose, blur,
illumination, and occlusion, where conventional visual quality metrics fail to
predict whether inputs are truly recognizable to the deployed encoder. Existing
FIQA methods typically rely on visual heuristics, curated annotations, or
computationally intensive generative pipelines, leaving their predictions
detached from the encoder's decision geometry. We introduce TransFIRA (Transfer
Learning for Face Image Recognizability Assessment), a lightweight and
annotation-free framework that grounds recognizability directly in embedding
space. TransFIRA delivers three advances: (i) a definition of recognizability
via class-center similarity (CCS) and class-center angular separation (CCAS),
yielding the first natural, decision-boundary--aligned criterion for filtering
and weighting; (ii) a recognizability-informed aggregation strategy that
achieves state-of-the-art verification accuracy on BRIAR and IJB-C while nearly
doubling correlation with true recognizability, all without external labels,
heuristics, or backbone-specific training; and (iii) new extensions beyond
faces, including encoder-grounded explainability that reveals how degradations
and subject-specific factors affect recognizability, and the first
recognizability-aware body recognition assessment. Experiments confirm
state-of-the-art results on faces, strong performance on body recognition, and
robustness under cross-dataset shifts. Together, these contributions establish
TransFIRA as a unified, geometry-driven framework for recognizability
assessment -- encoder-specific, accurate, interpretable, and extensible across
modalities -- significantly advancing FIQA in accuracy, explainability, and
scope.

</details>


### [204] [Road Surface Condition Detection with Machine Learning using New York State Department of Transportation Camera Images and Weather Forecast Data](https://arxiv.org/abs/2510.06440)
*Carly Sutter,Kara J. Sulia,Nick P. Bassill,Christopher D. Wirz,Christopher D. Thorncroft,Jay C. Rothenberger,Vanessa Przybylo,Mariana G. Cains,Jacob Radford,David Aaron Evans*

Main category: cs.CV

TL;DR: 使用卷积神经网络和随机森林，结合摄像头图像和天气数据，自动分类道路表面状况，为纽约州交通部门提供决策支持。


<details>
  <summary>Details</summary>
Motivation: 纽约州交通部门目前通过人工驾驶和观察摄像头来评估道路状况，这些任务劳动密集但至关重要。机器学习模型可以提供自动化的道路状况分类支持。

Method: 在约22,000张人工标注的摄像头图像上训练卷积神经网络和随机森林模型，将道路状况分为六类：严重积雪、积雪、潮湿、干燥、能见度差或被遮挡。

Result: 模型在完全未见过的摄像头上达到了81.5%的准确率，优先考虑了模型的泛化能力以满足实际运营需求。

Conclusion: 机器学习模型能够有效支持交通部门的道路状况评估，特别是在冬季天气事件中提供关键的运营决策依据。

Abstract: The New York State Department of Transportation (NYSDOT) has a network of
roadside traffic cameras that are used by both the NYSDOT and the public to
observe road conditions. The NYSDOT evaluates road conditions by driving on
roads and observing live cameras, tasks which are labor-intensive but necessary
for making critical operational decisions during winter weather events.
However, machine learning models can provide additional support for the NYSDOT
by automatically classifying current road conditions across the state. In this
study, convolutional neural networks and random forests are trained on camera
images and weather data to predict road surface conditions. Models are trained
on a hand-labeled dataset of ~22,000 camera images, each classified by human
labelers into one of six road surface conditions: severe snow, snow, wet, dry,
poor visibility, or obstructed. Model generalizability is prioritized to meet
the operational needs of the NYSDOT decision makers, and the weather-related
road surface condition model in this study achieves an accuracy of 81.5% on
completely unseen cameras.

</details>


### [205] [Cluster Paths: Navigating Interpretability in Neural Networks](https://arxiv.org/abs/2510.06541)
*Nicholas M. Kroeger,Vincent Bindschaedler*

Main category: cs.CV

TL;DR: 提出cluster paths方法，通过聚类神经网络激活值生成可解释的路径序列，并引入四个评估指标来验证其有效性。该方法能识别虚假线索、保持预测忠实度，并可作为异常检测器。


<details>
  <summary>Details</summary>
Motivation: 现代深度神经网络在视觉任务中表现出色但决策过程不透明，存在盲目信任、未检测偏见和意外失败的风险，需要可解释性方法来理解其决策过程。

Method: 在选定层对激活值进行聚类，将每个输入表示为聚类ID序列（cluster paths），并扩展到使用大语言模型生成概念路径。引入四个评估指标：路径复杂度、加权路径纯度、决策对齐忠实度和路径一致性。

Result: 在虚假线索CIFAR-10实验中识别了基于颜色的捷径；在CelebA头发颜色任务中达到90%忠实度和96%一致性；在ImageNet预训练的Vision Transformer上扩展为概念路径；可作为有效的异常检测器。

Conclusion: cluster paths能够揭示多个网络深度的视觉概念（如调色板、纹理、对象上下文），可扩展到大型视觉模型，同时生成简洁且人类可读的解释。

Abstract: While modern deep neural networks achieve impressive performance in vision
tasks, they remain opaque in their decision processes, risking unwarranted
trust, undetected biases and unexpected failures. We propose cluster paths, a
post-hoc interpretability method that clusters activations at selected layers
and represents each input as its sequence of cluster IDs. To assess these
cluster paths, we introduce four metrics: path complexity (cognitive load),
weighted-path purity (class alignment), decision-alignment faithfulness
(predictive fidelity), and path agreement (stability under perturbations). In a
spurious-cue CIFAR-10 experiment, cluster paths identify color-based shortcuts
and collapse when the cue is removed. On a five-class CelebA hair-color task,
they achieve 90% faithfulness and maintain 96% agreement under Gaussian noise
without sacrificing accuracy. Scaling to a Vision Transformer pretrained on
ImageNet, we extend cluster paths to concept paths derived from prompting a
large language model on minimal path divergences. Finally, we show that cluster
paths can serve as an effective out-of-distribution (OOD) detector, reliably
flagging anomalous samples before the model generates over-confident
predictions. Cluster paths uncover visual concepts, such as color palettes,
textures, or object contexts, at multiple network depths, demonstrating that
cluster paths scale to large vision models while generating concise and
human-readable explanations.

</details>


### [206] [SDQM: Synthetic Data Quality Metric for Object Detection Dataset Evaluation](https://arxiv.org/abs/2510.06596)
*Ayush Zenith,Arnold Zumbrun,Neel Raut,Jing Lin*

Main category: cs.CV

TL;DR: 本文提出了SDQM（合成数据集质量指标），用于评估目标检测任务中合成数据的质量，无需模型训练收敛即可评估，与YOLOv11的mAP分数强相关。


<details>
  <summary>Details</summary>
Motivation: 由于大规模、高质量标注数据集的稀缺性，合成数据成为提升模型性能的解决方案，但缺乏有效的质量评估指标。

Method: 开发SDQM指标来评估合成数据集质量，无需等待模型训练收敛，通过相关性分析验证其有效性。

Result: SDQM与YOLOv11的mAP分数表现出强相关性，而之前的指标仅显示中等或弱相关性，且能提供改进数据集质量的可操作见解。

Conclusion: SDQM为评估合成数据质量设定了新标准，是资源受限目标检测任务中高效生成和选择合成数据集的有效工具。

Abstract: The performance of machine learning models depends heavily on training data.
The scarcity of large-scale, well-annotated datasets poses significant
challenges in creating robust models. To address this, synthetic data generated
through simulations and generative models has emerged as a promising solution,
enhancing dataset diversity and improving the performance, reliability, and
resilience of models. However, evaluating the quality of this generated data
requires an effective metric. This paper introduces the Synthetic Dataset
Quality Metric (SDQM) to assess data quality for object detection tasks without
requiring model training to converge. This metric enables more efficient
generation and selection of synthetic datasets, addressing a key challenge in
resource-constrained object detection tasks. In our experiments, SDQM
demonstrated a strong correlation with the mean Average Precision (mAP) scores
of YOLOv11, a leading object detection model, while previous metrics only
exhibited moderate or weak correlations. Additionally, it provides actionable
insights for improving dataset quality, minimizing the need for costly
iterative training. This scalable and efficient metric sets a new standard for
evaluating synthetic data. The code for SDQM is available at
https://github.com/ayushzenith/SDQM

</details>


### [207] [Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking](https://arxiv.org/abs/2510.06820)
*Mitchell Keren Taraday,Shahaf Wagner,Chaim Baskin*

Main category: cs.CV

TL;DR: EDJE是一种高效的判别性联合编码器，通过预计算视觉token并压缩存储，大幅减少在线推理的计算和存储需求，在保持检索性能的同时实现高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言联合编码器（如BLIP）在视觉特征提取阶段存在瓶颈，计算成本高昂，难以在实际大规模部署中使用。

Method: EDJE预先离线计算视觉token，通过轻量级注意力适配器压缩存储，在线推理时仅需运行紧凑的联合编码器处理少量视觉token和文本。

Result: EDJE处理速度达50k图像-文本对/秒，每张图像仅需49kB磁盘存储，在Flickr（零样本）和COCO（微调）检索任务上达到与现有技术相当的性能。

Conclusion: EDJE通过优化视觉特征处理流程，在保持强检索性能的同时显著降低了存储和计算需求，使高吞吐量视觉-语言检索成为可能。

Abstract: Multimodal retrieval still leans on embedding-based models like CLIP for fast
vector search over pre-computed image embeddings. Yet, unlike text retrieval,
where joint-encoder rerankers are standard, comparable vision--language
rerankers are largely absent. We find that seminal joint encoders such as BLIP
are severely bottlenecked by an expensive visual feature-extraction stage,
preventing practical deployment at scale. Motivated by this bottleneck, we
introduce EDJE, an Efficient Discriminative Joint Encoder that precomputes
vision tokens offline and compresses them via a lightweight attention-based
adapter, so online inference runs only a compact joint encoder over a small set
of visual tokens plus the text. EDJE preserves strong retrieval performance
while drastically reducing storage and online compute, enabling high-throughput
inference. Specifically, EDJE processes 50k image--text pairs/second while
requiring 49kB of disk storage per image, matching prior art on Flickr
(zero-shot) and COCO (fine-tuned) retrieval. The implementation and checkpoints
will be made publicly available shortly.

</details>


### [208] [Resolution scaling governs DINOv3 transfer performance in chest radiograph classification](https://arxiv.org/abs/2510.07191)
*Soroosh Tayebi Arasteh,Mina Shaigan,Christiane Kuhl,Jakob Nikolas Kather,Sven Nebelung,Daniel Truhn*

Main category: cs.CV

TL;DR: DINOv3自监督学习模型在胸部X光图像分析中表现优于DINOv2和ImageNet初始化，特别是在512x512分辨率下。ConvNeXt-B架构优于ViT-B/16，而使用冻结的7B模型特征效果不如完全微调的小型模型。


<details>
  <summary>Details</summary>
Motivation: 评估自监督学习（特别是DINOv3）在胸部X光图像分析中的价值，这是一个具有细粒度发现的高容量成像模式。

Method: 在7个数据集（n>814,000）上对DINOv3、DINOv2和ImageNet初始化进行基准测试，使用ViT-B/16和ConvNeXt-B两种骨干网络，评估224x224、512x512和1024x1024三种分辨率。

Result: 在512x512分辨率下，DINOv3优于DINOv2和ImageNet；ConvNeXt-B优于ViT-B/16；冻结的7B模型特征表现不如完全微调的小型模型；1024x1024分辨率未带来进一步改进。

Conclusion: 在胸部X光图像分析中，512x512分辨率是实用上限，DINOv3初始化的ConvNeXt-B网络提供最强性能，对边界依赖和小病灶异常检测增益最明显。

Abstract: Self-supervised learning (SSL) has advanced visual representation learning,
but its value in chest radiography, a high-volume imaging modality with
fine-grained findings, remains unclear. Meta's DINOv3 extends earlier SSL
models through Gram-anchored self-distillation. Whether these design choices
improve transfer learning for chest radiography has not been systematically
tested. We benchmarked DINOv3 against DINOv2 and ImageNet initialization across
seven datasets (n>814,000). Two representative backbones were evaluated:
ViT-B/16 and ConvNeXt-B. Images were analyzed at 224x224, 512x512, and
1024x1024 pixels. We additionally assessed frozen features from a 7B model. The
primary outcome was mean AUROC across labels. At 224x224, DINOv3 and DINOv2
achieved comparable performance on adult datasets. Increasing resolution to
512x512 yielded consistent improvements for DINOv3 over both DINOv2 and
ImageNet. In contrast, results in pediatric cohort showed no differences across
initializations. Across all settings, ConvNeXt-B outperformed ViT-B/16. Models
using frozen DINOv3-7B features underperformed relative to fully finetuned
86-89M-parameter backbones, highlighting the importance of domain adaptation.
Scaling to 1024x1024 did not further improve accuracy. Resolution-related gains
were most evident for boundary-dependent and small focal abnormalities. In
chest radiography, higher input resolution is critical for leveraging the
benefits of modern self-supervised models. 512x512 pixels represent a practical
upper limit where DINOv3-initialized ConvNeXt-B networks provide the strongest
performance, while larger inputs offer minimal return on cost. Clinically,
these findings support use of finetuned, mid-sized backbones at 512x512 for
chest radiograph interpretation, with the greatest gains expected in detecting
subtle or boundary-centered lesions relevant to emergency and critical care
settings.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [209] [Latent Representation Learning in Heavy-Ion Collisions with MaskPoint Transformer](https://arxiv.org/abs/2510.06691)
*Jing-Zong Zhang,Shuang Guo,Li-Lin Zhu,Lingxiao Wang,Guo-Liang Ma*

Main category: hep-ph

TL;DR: 提出基于Transformer的自编码器，通过两阶段训练（自监督预训练+监督微调）从高能核碰撞数据中学习紧凑且信息丰富的特征表示，在区分大小碰撞系统任务中优于PointNet。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖选定观测值，可能忽略数据中细微但物理相关的结构，需要从高维末态数据中提取信息特征以支持可靠的下游分析。

Method: 使用Transformer自编码器，采用两阶段训练范式：自监督预训练学习未标记数据的潜在表示，然后监督微调适应具体物理任务。

Result: 在区分大小碰撞系统任务中分类准确率显著高于PointNet，主成分分析和SHAP解释表明自编码器捕获了超越单个观测值的复杂非线性相关性。

Conclusion: 该两阶段框架为高能核碰撞中的特征学习提供了通用且稳健的基础，为分析夸克-胶子等离子体性质和其他涌现现象开辟了道路。

Abstract: A central challenge in high-energy nuclear physics is to extract informative
features from the high-dimensional final-state data of heavy-ion collisions
(HIC) in order to enable reliable downstream analyses. Traditional approaches
often rely on selected observables, which may miss subtle but physically
relevant structures in the data. To address this, we introduce a
Transformer-based autoencoder trained with a two-stage paradigm:
self-supervised pre-training followed by supervised fine-tuning. The pretrained
encoder learns latent representations directly from unlabeled HIC data,
providing a compact and information-rich feature space that can be adapted to
diverse physics tasks. As a case study, we apply the method to distinguish
between large and small collision systems, where it achieves significantly
higher classification accuracy than PointNet. Principal component analysis and
SHAP interpretation further demonstrate that the autoencoder captures complex
nonlinear correlations beyond individual observables, yielding features with
strong discriminative and explanatory power. These results establish our
two-stage framework as a general and robust foundation for feature learning in
HIC, opening the door to more powerful analyses of quark--gluon plasma
properties and other emergent phenomena. The implementation is publicly
available at https://github.com/Giovanni-Sforza/MaskPoint-AMPT.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [210] [Memory-Augmented Generative AI for Real-time Wireless Prediction in Dynamic Industrial Environments](https://arxiv.org/abs/2510.06884)
*Rahul Gulia,Amlan Ganguly,Michael E. Kuhl,Ehsan Rashedi,Clark Hochgraf*

Main category: eess.SP

TL;DR: Evo-WISVA是一种用于工业4.0环境中无线信道预测的深度学习架构，通过结合VAE和ConvLSTM实现信号干扰噪声比的实时预测，在动态复杂环境中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统物理或统计模型无法应对智能仓库中移动障碍物和瞬时干扰带来的时空复杂性，而超可靠低延迟通信需要准确实时的无线信道条件预测。

Method: 提出Evo-WISVA架构，集成具有注意力驱动潜在记忆模块的变分自编码器进行空间特征提取，结合卷积长短期记忆网络进行时间预测和序列优化，通过联合损失函数进行端到端优化。

Result: 在高保真工业仓库数据集上的实验表明，Evo-WISVA显著超越现有基线方法，平均重建误差降低达47.6%，在动态复杂性大幅增加的环境中表现出优异的泛化能力。

Conclusion: Evo-WISVA为主动无线资源管理建立了基础技术，推动了工业通信网络中预测性数字孪生的实现。

Abstract: Accurate and real-time prediction of wireless channel conditions,
particularly the Signal-to-Interference-plus-Noise Ratio (SINR), is a
foundational requirement for enabling Ultra-Reliable Low-Latency Communication
(URLLC) in highly dynamic Industry 4.0 environments. Traditional physics-based
or statistical models fail to cope with the spatio-temporal complexities
introduced by mobile obstacles and transient interference inherent to smart
warehouses. To address this, we introduce Evo-WISVA (Evolutionary Wireless
Infrastructure for Smart Warehouse using VAE), a novel synergistic deep
learning architecture that functions as a lightweight 2D predictive digital
twin of the radio environment. Evo-WISVA integrates a memory-augmented
Variational Autoencoder (VAE) featuring an Attention-driven Latent Memory
Module (LMM) for robust, context-aware spatial feature extraction, with a
Convolutional Long Short-Term Memory (ConvLSTM) network for precise temporal
forecasting and sequential refinement. The entire pipeline is optimized
end-to-end via a joint loss function, ensuring optimal feature alignment
between the generative and predictive components. Rigorous experimental
evaluation conducted on a high-fidelity ns-3-generated industrial warehouse
dataset demonstrates that Evo-WISVA significantly surpasses state-of-the-art
baselines, achieving up to a 47.6\% reduction in average reconstruction error.
Crucially, the model exhibits exceptional generalization capacity to unseen
environments with vastly increased dynamic complexity (up to ten simultaneously
moving obstacles) while maintaining amortized computational efficiency
essential for real-time deployment. Evo-WISVA establishes a foundational
technology for proactive wireless resource management, enabling autonomous
optimization and advancing the realization of predictive digital twins in
industrial communication networks.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [211] [On-Package Memory with Universal Chiplet Interconnect Express (UCIe): A Low Power, High Bandwidth, Low Latency and Low Cost Approach](https://arxiv.org/abs/2510.06513)
*Debendra Das Sharma,Swadesh Choudhary,Peter Onufryk,Rob Pelt*

Main category: cs.AR

TL;DR: 提出通过增强UCIe接口支持内存语义，为AI等计算应用提供高能效带宽和成本效益的封装内内存解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有封装内内存解决方案无法满足AI等新兴计算应用对高能效带宽的需求，面临内存墙问题。

Method: 通过逻辑芯片复用LPDDR6和HBM内存连接到SoC的UCIe接口，以及让DRAM芯片原生支持UCIe而非LPDDR6总线接口。

Result: 相比现有HBM4和LPDDR封装内内存方案，带宽密度提升高达10倍，延迟降低3倍，功耗降低3倍，成本更低。

Conclusion: 增强UCIe支持内存语义可提供显著更优的封装内内存解决方案，满足AI等计算应用的带宽和能效需求。

Abstract: Emerging computing applications such as Artificial Intelligence (AI) are
facing a memory wall with existing on-package memory solutions that are unable
to meet the power-efficient bandwidth demands. We propose to enhance UCIe with
memory semantics to deliver power-efficient bandwidth and cost-effective
on-package memory solutions applicable across the entire computing continuum.
We propose approaches by reusing existing LPDDR6 and HBM memory through a logic
die that connects to the SoC using UCIe. We also propose an approach where the
DRAM die natively supports UCIe instead of the LPDDR6 bus interface. Our
approaches result in significantly higher bandwidth density (up to 10x), lower
latency (up to 3x), lower power (up to 3x), and lower cost compared to existing
HBM4 and LPDDR on-package memory solutions.

</details>


### [212] [Cocoon: A System Architecture for Differentially Private Training with Correlated Noises](https://arxiv.org/abs/2510.07304)
*Donghwan Kim,Xin Gu,Jinho Baek,Timothy Lo,Younghoon Min,Kwangsik Shin,Jongryool Kim,Jongse Park,Kiwan Maeng*

Main category: cs.AR

TL;DR: 本文分析了使用相关噪声的差分隐私训练机制的性能开销，并提出Cocoon硬件-软件协同设计框架来加速这类训练。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型会记忆和泄露训练数据，引发隐私问题。虽然DP-SGD等差分隐私训练算法能解决这一问题，但会降低模型准确性。使用相关噪声的新方法能提高准确性，但在大型模型或使用大嵌入表时会产生显著性能开销。

Method: 提出Cocoon框架：1) Cocoon-Emb通过预计算并以合并格式存储相关噪声来加速嵌入表模型；2) Cocoon-NMP使用定制近内存处理设备支持大型模型。

Result: 在基于FPGA的NMP设备原型上，Cocoon-Emb性能提升2.33-10.82倍，Cocoon-NMP性能提升1.55-3.06倍。

Conclusion: Cocoon框架有效解决了相关噪声差分隐私训练的性能瓶颈，为隐私保护机器学习提供了高效的硬件-软件协同解决方案。

Abstract: Machine learning (ML) models memorize and leak training data, causing serious
privacy issues to data owners. Training algorithms with differential privacy
(DP), such as DP-SGD, have been gaining attention as a solution. However,
DP-SGD adds a noise at each training iteration, which degrades the accuracy of
the trained model. To improve accuracy, a new family of approaches adds
carefully designed correlated noises, so that noises cancel out each other
across iterations. We performed an extensive characterization study of these
new mechanisms, for the first time to the best of our knowledge, and show they
incur non-negligible overheads when the model is large or uses large embedding
tables. Motivated by the analysis, we propose Cocoon, a hardware-software
co-designed framework for efficient training with correlated noises. Cocoon
accelerates models with embedding tables through pre-computing and storing
correlated noises in a coalesced format (Cocoon-Emb), and supports large models
through a custom near-memory processing device (Cocoon-NMP). On a real system
with an FPGA-based NMP device prototype, Cocoon improves the performance by
2.33-10.82x(Cocoon-Emb) and 1.55-3.06x (Cocoon-NMP).

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [213] [Multi-hop Deep Joint Source-Channel Coding with Deep Hash Distillation for Semantically Aligned Image Retrieval](https://arxiv.org/abs/2510.06868)
*Didrik Bergström,Deniz Gündüz,Onur Günlü*

Main category: cs.IT

TL;DR: 该论文提出了一种结合深度哈希蒸馏的深度联合源信道编码方法，用于多跳AWGN信道中的图像传输，通过语义聚类提升语义一致性和感知重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决传统DeepJSCC在多跳信道中可能出现的噪声积累问题，同时增强语义一致性以支持安全应用，并提升感知重建质量。

Method: 训练DeepJSCC编码器-解码器对，结合预训练的深度哈希蒸馏模块，同时最小化MSE和源图像与重建图像DHD哈希的余弦距离。

Result: 在不同多跳设置下，语义对齐显著改善了感知质量，通过LPIPS指标验证了性能提升。

Conclusion: 结合深度哈希蒸馏的DeepJSCC方法有效提升了多跳信道中图像传输的语义一致性和感知重建质量。

Abstract: We consider image transmission via deep joint source-channel coding
(DeepJSCC) over multi-hop additive white Gaussian noise (AWGN) channels by
training a DeepJSCC encoder-decoder pair with a pre-trained deep hash
distillation (DHD) module to semantically cluster images, facilitating
security-oriented applications through enhanced semantic consistency and
improving the perceptual reconstruction quality. We train the DeepJSCC module
to both reduce mean square error (MSE) and minimize cosine distance between DHD
hashes of source and reconstructed images. Significantly improved perceptual
quality as a result of semantic alignment is illustrated for different
multi-hop settings, for which classical DeepJSCC may suffer from noise
accumulation, measured by the learned perceptual image patch similarity (LPIPS)
metric.

</details>


### [214] [Spectral Graph Clustering under Differential Privacy: Balancing Privacy, Accuracy, and Efficiency](https://arxiv.org/abs/2510.07136)
*Mohamed Seif,Antti Koskela,H. Vincent Poor,Andrea J. Goldsmith*

Main category: cs.IT

TL;DR: 提出了三种在边差分隐私下的谱图聚类机制：边翻转加邻接矩阵混洗、低维空间投影加高斯噪声、以及带噪声的幂迭代方法，实现了严格的隐私保护和可控的误分类误差率。


<details>
  <summary>Details</summary>
Motivation: 研究在边差分隐私约束下的谱图聚类问题，需要在保护图结构隐私的同时保持聚类性能。

Method: 1) 边翻转加邻接矩阵混洗机制；2) 低维空间投影加高斯噪声；3) 带噪声的幂迭代方法。

Result: 理论分析提供了严格的隐私保证和误分类误差率的精确刻画，在合成和真实网络上的实验验证了理论分析。

Conclusion: 所提出的机制在边差分隐私下实现了有效的谱图聚类，展示了实用的隐私-效用权衡。

Abstract: We study the problem of spectral graph clustering under edge differential
privacy (DP). Specifically, we develop three mechanisms: (i) graph perturbation
via randomized edge flipping combined with adjacency matrix shuffling, which
enforces edge privacy while preserving key spectral properties of the graph.
Importantly, shuffling considerably amplifies the guarantees: whereas flipping
edges with a fixed probability alone provides only a constant epsilon edge DP
guarantee as the number of nodes grows, the shuffled mechanism achieves
(epsilon, delta) edge DP with parameters that tend to zero as the number of
nodes increase; (ii) private graph projection with additive Gaussian noise in a
lower-dimensional space to reduce dimensionality and computational complexity;
and (iii) a noisy power iteration method that distributes Gaussian noise across
iterations to ensure edge DP while maintaining convergence. Our analysis
provides rigorous privacy guarantees and a precise characterization of the
misclassification error rate. Experiments on synthetic and real-world networks
validate our theoretical analysis and illustrate the practical privacy-utility
trade-offs.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [215] [A General Constructive Upper Bound on Shallow Neural Nets Complexity](https://arxiv.org/abs/2510.06372)
*Frantisek Hakl,Vit Fojtik*

Main category: stat.ML

TL;DR: 本文提供了一个浅层神经网络逼近连续函数所需神经元数量的上界估计方法


<details>
  <summary>Details</summary>
Motivation: 为了量化神经网络逼近连续函数的能力，提供构造性的神经元数量估计

Method: 受Stone-Weierstrass定理特定证明启发，构建了一种构造性方法

Result: 得到了适用于任意紧集上任意连续函数的通用神经元数量上界

Conclusion: 该方法比先前类似边界更一般化，适用于更广泛的情形

Abstract: We provide an upper bound on the number of neurons required in a shallow
  neural network to approximate a continuous function on a compact set with a
  given accuracy. This method, inspired by a specific proof of the
  Stone-Weierstrass theorem, is constructive and more general than previous
  bounds of this character, as it applies to any continuous function on any
  compact set.

</details>


### [216] [Online Matching via Reinforcement Learning: An Expert Policy Orchestration Strategy](https://arxiv.org/abs/2510.06515)
*Chiara Mignacco,Matthieu Jonckheere,Gilles Stoltz*

Main category: stat.ML

TL;DR: 提出一种基于强化学习的方法，通过协调多个专家策略的优势来提升在线匹配问题的系统性能，该方法具有理论保证并在模拟中表现出优于单个专家和传统RL基准的性能。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法在在线匹配问题中虽然简单可解释，但通常针对特定操作环境设计，当条件变化时会导致效率低下。需要一种能够自适应协调多个专家策略的数据驱动方法。

Method: 基于Adv2框架，通过优势权重更新组合专家决策，扩展到仅能获得估计价值函数的情况。建立了期望和高概率后悔保证，推导了时间差分学习的有限时间偏差界限。引入神经演员-评论家架构以支持大规模状态空间的可扩展性。

Result: 在随机匹配模型（包括器官交换场景）的模拟中，协调策略比单个专家和传统RL基准收敛更快，系统级效率更高。

Conclusion: 结构化自适应学习能够改进复杂资源分配和决策过程的建模与管理，通过协调专家策略的优势实现更好的系统性能。

Abstract: Online matching problems arise in many complex systems, from cloud services
and online marketplaces to organ exchange networks, where timely, principled
decisions are critical for maintaining high system performance. Traditional
heuristics in these settings are simple and interpretable but typically
tailored to specific operating regimes, which can lead to inefficiencies when
conditions change. We propose a reinforcement learning (RL) approach that
learns to orchestrate a set of such expert policies, leveraging their
complementary strengths in a data-driven, adaptive manner. Building on the Adv2
framework (Jonckheere et al., 2024), our method combines expert decisions
through advantage-based weight updates and extends naturally to settings where
only estimated value functions are available. We establish both expectation and
high-probability regret guarantees and derive a novel finite-time bias bound
for temporal-difference learning, enabling reliable advantage estimation even
under constant step size and non-stationary dynamics. To support scalability,
we introduce a neural actor-critic architecture that generalizes across large
state spaces while preserving interpretability. Simulations on stochastic
matching models, including an organ exchange scenario, show that the
orchestrated policy converges faster and yields higher system level efficiency
than both individual experts and conventional RL baselines. Our results
highlight how structured, adaptive learning can improve the modeling and
management of complex resource allocation and decision-making processes.

</details>


### [217] [Q-Learning with Fine-Grained Gap-Dependent Regret](https://arxiv.org/abs/2510.06647)
*Haochen Zhang,Zhong Zheng,Lingzhou Xue*

Main category: stat.ML

TL;DR: 本文研究了表格马尔可夫决策过程中模型无关强化学习的细粒度间隙依赖遗憾界，改进了UCB和非UCB算法的遗憾分析框架。


<details>
  <summary>Details</summary>
Motivation: 现有模型无关算法虽然达到了极小极大最坏情况遗憾，但其间隙依赖界限仍然粗糙，未能充分捕捉次优性间隙的结构。

Method: 在UCB设置下开发了新的分析框架，明确分离最优和次优状态-动作对的分析；在非UCB设置下重新审视AMB算法，修正了其Q值更新中的截断问题和集中性论证中的鞅差条件违反问题。

Result: 为UCB-Hoeffding建立了首个细粒度遗憾上界；提出的ULCB-Hoeffding算法在经验上优于AMB；改进后的AMB版本实现了首个非UCB方法的严格细粒度间隙依赖遗憾保证。

Conclusion: 通过改进分析框架和算法设计，成功建立了模型无关强化学习的细粒度间隙依赖遗憾界限，为UCB和非UCB方法都提供了更精确的理论保证。

Abstract: We study fine-grained gap-dependent regret bounds for model-free
reinforcement learning in episodic tabular Markov Decision Processes. Existing
model-free algorithms achieve minimax worst-case regret, but their
gap-dependent bounds remain coarse and fail to fully capture the structure of
suboptimality gaps. We address this limitation by establishing fine-grained
gap-dependent regret bounds for both UCB-based and non-UCB-based algorithms. In
the UCB-based setting, we develop a novel analytical framework that explicitly
separates the analysis of optimal and suboptimal state-action pairs, yielding
the first fine-grained regret upper bound for UCB-Hoeffding (Jin et al., 2018).
To highlight the generality of this framework, we introduce ULCB-Hoeffding, a
new UCB-based algorithm inspired by AMB (Xu et al.,2021) but with a simplified
structure, which enjoys fine-grained regret guarantees and empirically
outperforms AMB. In the non-UCB-based setting, we revisit the only known
algorithm AMB, and identify two key issues in its algorithm design and
analysis: improper truncation in the $Q$-updates and violation of the
martingale difference condition in its concentration argument. We propose a
refined version of AMB that addresses these issues, establishing the first
rigorous fine-grained gap-dependent regret for a non-UCB-based method, with
experiments demonstrating improved performance over AMB.

</details>


### [218] [Gaussian Equivalence for Self-Attention: Asymptotic Spectral Analysis of Attention Matrix](https://arxiv.org/abs/2510.06685)
*Tomohiro Hayase,Benoît Collins,Ryo Karakida*

Main category: stat.ML

TL;DR: 本文首次建立了注意力矩阵的高斯等价性结果，证明了在逆温度保持常数阶的自然机制下，注意力矩阵的奇异值分布可由可处理的线性模型渐近表征，且与之前认为的Marchenko-Pastur定律存在偏差。


<details>
  <summary>Details</summary>
Motivation: 自注意力层已成为现代深度神经网络的基本构建模块，但其理论理解仍然有限，特别是从随机矩阵理论的角度来看。

Method: 通过精确控制归一化项的波动和利用指数函数有利泰勒展开的精细化线性化方法，分析注意力矩阵的奇异值谱。

Result: 证明了注意力矩阵奇异值分布偏离Marchenko-Pastur定律，并识别了线性化的阈值，阐明了注意力机制在此机制下为何能够获得严格的高斯等价性。

Conclusion: 该工作为注意力机制提供了首个高斯等价性结果，深化了对自注意力层随机矩阵性质的理论理解。

Abstract: Self-attention layers have become fundamental building blocks of modern deep
neural networks, yet their theoretical understanding remains limited,
particularly from the perspective of random matrix theory. In this work, we
provide a rigorous analysis of the singular value spectrum of the attention
matrix and establish the first Gaussian equivalence result for attention. In a
natural regime where the inverse temperature remains of constant order, we show
that the singular value distribution of the attention matrix is asymptotically
characterized by a tractable linear model. We further demonstrate that the
distribution of squared singular values deviates from the Marchenko-Pastur law,
which has been believed in previous work. Our proof relies on two key
ingredients: precise control of fluctuations in the normalization term and a
refined linearization that leverages favorable Taylor expansions of the
exponential. This analysis also identifies a threshold for linearization and
elucidates why attention, despite not being an entrywise operation, admits a
rigorous Gaussian equivalence in this regime.

</details>


### [219] [Bayesian Nonparametric Dynamical Clustering of Time Series](https://arxiv.org/abs/2510.06919)
*Adrián Pérez-Herrero,Paulo Félix,Jesús Presedo,Carl Henrik Ek*

Main category: stat.ML

TL;DR: 提出了一种基于贝叶斯非参数方法的时变时间序列聚类模型，能够自动发现和演化无界数量的时间序列聚类，通过切换线性动态系统和变分推理实现高效学习。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以处理时间序列模式随时间演化的问题，需要能够自动发现和跟踪无界数量聚类的方法，避免不必要的聚类扩散。

Method: 使用层次狄利克雷过程作为切换线性动态系统参数的先验，高斯过程先验建模每个聚类内的振幅和时间对齐统计变化，通过变分下界进行离线和在线推理。

Result: 在多个公开心电数据库的案例研究中展示了方法的有效性和通用性。

Conclusion: 该方法能够以原则性方式建模时间序列模式的演化，避免聚类过度扩散，为时间序列分析提供了灵活有效的框架。

Abstract: We present a method that models the evolution of an unbounded number of time
series clusters by switching among an unknown number of regimes with linear
dynamics. We develop a Bayesian non-parametric approach using a hierarchical
Dirichlet process as a prior on the parameters of a Switching Linear Dynamical
System and a Gaussian process prior to model the statistical variations in
amplitude and temporal alignment within each cluster. By modeling the evolution
of time series patterns, the method avoids unnecessary proliferation of
clusters in a principled manner. We perform inference by formulating a
variational lower bound for off-line and on-line scenarios, enabling efficient
learning through optimization. We illustrate the versatility and effectiveness
of the approach through several case studies of electrocardiogram analysis
using publicly available databases.

</details>


### [220] [PyCFRL: A Python library for counterfactually fair offline reinforcement learning via sequential data preprocessing](https://arxiv.org/abs/2510.06935)
*Jianhan Zhang,Jitao Wang,Chengchun Shi,John D. Piette,Donglin Zeng,Zhenke Wu*

Main category: stat.ML

TL;DR: PyCFRL是一个Python库，用于在离线强化学习中确保反事实公平性，通过数据预处理算法学习公平的RL策略并提供评估工具。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法虽然旨在最大化总体效益，但可能会对少数群体或社会经济弱势群体造成不利影响，因此需要确保策略的公平性。

Method: 开发了PyCFRL库，实现了一种新颖的数据预处理算法，用于从离线数据集中学习反事实公平的RL策略，并提供评估工具。

Result: PyCFRL库已公开发布在PyPI和GitHub上，提供了详细的使用教程和文档。

Conclusion: PyCFRL为解决强化学习中的公平性问题提供了一个实用的工具，能够帮助开发者在离线RL环境中实现反事实公平的策略学习。

Abstract: Reinforcement learning (RL) aims to learn and evaluate a sequential decision
rule, often referred to as a "policy", that maximizes the population-level
benefit in an environment across possibly infinitely many time steps. However,
the sequential decisions made by an RL algorithm, while optimized to maximize
overall population benefits, may disadvantage certain individuals who are in
minority or socioeconomically disadvantaged groups. To address this problem, we
introduce PyCFRL, a Python library for ensuring counterfactual fairness in
offline RL. PyCFRL implements a novel data preprocessing algorithm for learning
counterfactually fair RL policies from offline datasets and provides tools to
evaluate the values and counterfactual unfairness levels of RL policies. We
describe the high-level functionalities of PyCFRL and demonstrate one of its
major use cases through a data example. The library is publicly available on
PyPI and Github (https://github.com/JianhanZhang/PyCFRL), and detailed
tutorials can be found in the PyCFRL documentation
(https://pycfrl-documentation.netlify.app).

</details>


### [221] [Root Cause Analysis of Outliers in Unknown Cyclic Graphs](https://arxiv.org/abs/2510.06995)
*Daniela Schkoda,Dominik Janzing*

Main category: stat.ML

TL;DR: 研究线性结构方程循环因果图中的异常值传播，将其追溯到单个或多个"根本原因"节点。在扰动足够强且传播遵循正常模式相同结构方程的条件下，可以识别出包含真实根本原因及其在循环中父节点的短列表。


<details>
  <summary>Details</summary>
Motivation: 研究循环因果图中异常值的传播机制，旨在开发无需先验因果图知识的方法来识别异常的根本原因节点。

Method: 基于线性结构方程模型，分析异常值在循环因果图中的传播特性，通过扰动强度和传播一致性来识别潜在的根本原因节点。

Result: 证明可以识别出包含真实根本原因及其在循环中父节点的短列表，前提是扰动足够强且传播遵循正常模式的结构方程。

Conclusion: 该方法能够在无需因果图先验知识的情况下，有效识别循环因果图中异常传播的根本原因，为异常检测和根源分析提供了新思路。

Abstract: We study the propagation of outliers in cyclic causal graphs with linear
structural equations, tracing them back to one or several "root cause" nodes.
We show that it is possible to identify a short list of potential root causes
provided that the perturbation is sufficiently strong and propagates according
to the same structural equations as in the normal mode. This shortlist consists
of the true root causes together with those of its parents lying on a cycle
with the root cause. Notably, our method does not require prior knowledge of
the causal graph.

</details>


### [222] [Explaining Models under Multivariate Bernoulli Distribution via Hoeffding Decomposition](https://arxiv.org/abs/2510.07088)
*Baptiste Ferrere,Nicolas Bousquet,Fabrice Gamboa,Jean-Michel Loubes,Joseph Muré*

Main category: stat.ML

TL;DR: 本文针对伯努利分布输入变量的预测模型，提出了广义Hoeffding分解的完整描述，建立了可解释性框架，并推导出Sobol指数和Shapley效应等影响指标。


<details>
  <summary>Details</summary>
Motivation: 为随机输入预测模型的行为提供解释，通过子模型分解使特征更易解释，特别是在输入变量相关的情况下建立可解释性框架。

Method: 基于L2子空间斜投影概念的广义Hoeffding分解，针对伯努利分布输入变量，证明L2子空间是一维的，并获得显式函数分解。

Result: 建立了完整的可解释性框架，可显式推导Sobol指数和Shapley效应等输入影响指标，支持基于二元决策图、布尔网络或二元神经网络的决策支持问题。

Conclusion: 该方法为二元输入模型提供了有效的可解释性分析工具，并展望了在高维设置和有限可数输入模型中的扩展应用。

Abstract: Explaining the behavior of predictive models with random inputs can be
achieved through sub-models decomposition, where such sub-models have easier
interpretable features. Arising from the uncertainty quantification community,
recent results have demonstrated the existence and uniqueness of a generalized
Hoeffding decomposition for such predictive models when the stochastic input
variables are correlated, based on concepts of oblique projection onto L 2
subspaces. This article focuses on the case where the input variables have
Bernoulli distributions and provides a complete description of this
decomposition. We show that in this case the underlying L 2 subspaces are
one-dimensional and that the functional decomposition is explicit. This leads
to a complete interpretability framework and theoretically allows reverse
engineering. Explicit indicators of the influence of inputs on the output
prediction (exemplified by Sobol' indices and Shapley effects) can be
explicitly derived. Illustrated by numerical experiments, this type of analysis
proves useful for addressing decision-support problems, based on binary
decision diagrams, Boolean networks or binary neural networks. The article
outlines perspectives for exploring high-dimensional settings and, beyond the
case of binary inputs, extending these findings to models with finite countable
inputs.

</details>


### [223] [Diffusion-Augmented Reinforcement Learning for Robust Portfolio Optimization under Stress Scenarios](https://arxiv.org/abs/2510.07099)
*Himanshu Choudhary,Arishi Orra,Manoj Thakur*

Main category: stat.ML

TL;DR: 提出DARL框架，结合去噪扩散概率模型和深度强化学习进行投资组合管理，通过生成合成市场崩溃场景增强训练数据鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以捕捉金融市场复杂动态和满足多样化投资者偏好，需要更强大的投资组合优化方法。

Method: DARL框架将去噪扩散概率模型与深度强化学习相结合，利用DDPM生成不同压力强度下的合成市场崩溃场景来增强训练数据。

Result: 实证评估显示DARL优于传统基线方法，在风险调整后收益和对意外危机（如2025年关税危机）的韧性方面表现更优。

Conclusion: 该工作为增强DRL驱动金融应用的压力韧性提供了稳健实用的方法。

Abstract: In the ever-changing and intricate landscape of financial markets, portfolio
optimisation remains a formidable challenge for investors and asset managers.
Conventional methods often struggle to capture the complex dynamics of market
behaviour and align with diverse investor preferences. To address this, we
propose an innovative framework, termed Diffusion-Augmented Reinforcement
Learning (DARL), which synergistically integrates Denoising Diffusion
Probabilistic Models (DDPMs) with Deep Reinforcement Learning (DRL) for
portfolio management. By leveraging DDPMs to generate synthetic market crash
scenarios conditioned on varying stress intensities, our approach significantly
enhances the robustness of training data. Empirical evaluations demonstrate
that DARL outperforms traditional baselines, delivering superior risk-adjusted
returns and resilience against unforeseen crises, such as the 2025 Tariff
Crisis. This work offers a robust and practical methodology to bolster stress
resilience in DRL-driven financial applications.

</details>


### [224] [Split Conformal Classification with Unsupervised Calibration](https://arxiv.org/abs/2510.07185)
*Santiago Mazuelas*

Main category: stat.ML

TL;DR: 提出了一种用于分类任务的无监督校准分割共形预测方法，允许使用无监督校准样本和已有的监督训练样本来构建集合预测规则，避免了传统方法需要额外标记样本进行校准的限制。


<details>
  <summary>Details</summary>
Motivation: 传统分割共形预测方法需要使用与训练样本不同的标记样本进行校准，这既不方便又需要额外获取标签。该方法旨在解决这一限制，允许使用无监督样本进行校准。

Method: 使用无监督校准样本与已有的监督训练样本相结合，构建集合预测规则。通过理论分析和实验验证方法的有效性。

Result: 实验结果表明，该方法能够达到与监督校准相当的性能，仅在性能保证和计算效率方面有适度下降。

Conclusion: 提出的无监督校准方法为分割共形预测提供了一种有效的替代方案，在保持性能的同时减少了对外部标记数据的需求。

Abstract: Methods for split conformal prediction leverage calibration samples to
transform any prediction rule into a set-prediction rule that complies with a
target coverage probability. Existing methods provide remarkably strong
performance guarantees with minimal computational costs. However, they require
to use calibration samples composed by labeled examples different to those used
for training. This requirement can be highly inconvenient, as it prevents the
use of all labeled examples for training and may require acquiring additional
labels solely for calibration. This paper presents an effective methodology for
split conformal prediction with unsupervised calibration for classification
tasks. In the proposed approach, set-prediction rules are obtained using
unsupervised calibration samples together with supervised training samples
previously used to learn the classification rule. Theoretical and experimental
results show that the presented methods can achieve performance comparable to
that with supervised calibration, at the expenses of a moderate degradation in
performance guarantees and computational efficiency.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [225] [Developing a Sequential Deep Learning Pipeline to Model Alaskan Permafrost Thaw Under Climate Change](https://arxiv.org/abs/2510.06258)
*Addina Rahaman*

Main category: physics.ao-ph

TL;DR: 本研究提出了一个基于纬度的深度学习框架，用于预测阿拉斯加多年冻土活动层的土壤温度，使用了多种深度学习方法并发现GRU模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致永久冻土融化，释放温室气体，需要准确预测土壤温度以进行风险评估，但现有方法往往忽略影响土壤热动态的多种因素。

Method: 使用ERA5-Land再分析数据、静态地质特征、滑动窗口序列、长期气候强迫信号和纬度带嵌入，测试了TCN、Transformer、Conv1DLSTM、GRU和BiLSTM五种深度学习模型。

Result: 模型能有效识别纬度和深度方向的温度差异，GRU在序列温度模式检测中表现最佳，偏差校正的CMIP5数据能识别正弦温度趋势但情景间差异有限。

Conclusion: 本研究建立了一个端到端的深度学习框架，用于活动层温度建模，提供了季节、空间和垂直温度背景，且不受特征选择的固有限制。

Abstract: Changing climate conditions threaten the natural permafrost thaw-freeze
cycle, leading to year-round soil temperatures above 0{\deg}C. In Alaska, the
warming of the topmost permafrost layer, known as the active layer, signals
elevated greenhouse gas release due to high carbon storage. Accurate soil
temperature prediction is therefore essential for risk mitigation and stability
assessment; however, many existing approaches overlook the numerous factors
driving soil thermal dynamics. This study presents a proof-of-concept
latitude-based deep learning pipeline for modeling yearly soil temperatures
across multiple depths. The framework employs dynamic reanalysis feature data
from the ERA5-Land dataset, static geologic and lithological features,
sliding-window sequences for seasonal context, a derived scenario signal
feature for long-term climate forcing, and latitude band embeddings for spatial
sensitivity. Five deep learning models were tested: a Temporal Convolutional
Network (TCN), a Transformer, a 1-Dimensional Convolutional Long-Short Term
Memory (Conv1DLSTM), a Gated-Recurrent Unit (GRU), and a Bidirectional
Long-Short Term Memory (BiLSTM). Results showed solid recognition of
latitudinal and depth-wise temperature discrepancies, with the GRU performing
best in sequential temperature pattern detection. Bias-corrected CMIP5 RCP data
enabled recognition of sinusoidal temperature trends, though limited divergence
between scenarios were observed. This study establishes an end-to-end framework
for adopting deep learning in active layer temperature modeling, offering
seasonal, spatial, and vertical temperature context without intrinsic
restrictions on feature selection.

</details>


### [226] [Mass Conservation on Rails -- Rethinking Physics-Informed Learning of Ice Flow Vector Fields](https://arxiv.org/abs/2510.06286)
*Kim Bente,Roman Marchant,Fabio Ramos*

Main category: physics.ao-ph

TL;DR: 提出发散自由神经网络(dfNNs)，通过向量微积分技巧精确执行局部质量守恒，用于南极冰流矢量场的稀疏噪声测量插值，相比物理信息神经网络(PINNs)和无约束神经网络表现更可靠。


<details>
  <summary>Details</summary>
Motivation: 为可靠预测未来海平面上升，冰盖模型需要符合物理原理的输入。将质量守恒等物理原则嵌入到从稀疏噪声测量中插值南极冰流矢量场的模型中，不仅能促进物理一致性，还能提高准确性和鲁棒性。

Method: 提出发散自由神经网络(dfNNs)，通过向量微积分技巧精确执行局部质量守恒，而不是像PINNs那样将物理作为软惩罚。同时引入方向引导学习策略，利用全大陆卫星速度数据提升模型性能。

Result: 在伯德冰川冰通量插值任务中比较dfNNs、PINNs和无约束NNs，发现"轨道上的质量守恒"能产生更可靠的估计，方向引导策略在所有模型中都能提升性能。

Conclusion: dfNNs通过精确执行质量守恒提供了更可靠的冰流插值方法，方向引导学习策略是提升模型性能的有效手段，为冰盖建模提供了更物理一致的输入数据。

Abstract: To reliably project future sea level rise, ice sheet models require inputs
that respect physics. Embedding physical principles like mass conservation into
models that interpolate Antarctic ice flow vector fields from sparse & noisy
measurements not only promotes physical adherence but can also improve accuracy
and robustness. While physics-informed neural networks (PINNs) impose physics
as soft penalties, offering flexibility but no physical guarantees, we instead
propose divergence-free neural networks (dfNNs), which enforce local mass
conservation exactly via a vector calculus trick. Our comparison of dfNNs,
PINNs, and unconstrained NNs on ice flux interpolation over Byrd Glacier
suggests that "mass conservation on rails" yields more reliable estimates, and
that directional guidance, a learning strategy leveraging continent-wide
satellite velocity data, boosts performance across models.

</details>
